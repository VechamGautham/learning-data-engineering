{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e598ba85",
   "metadata": {},
   "source": [
    "## 📚 Table of Contents \n",
    "- [Data Ingestion](#data-ingestion)\n",
    "- [Stakeholder Conversation Summary](#stakeholder-conversation-summary-marketing-analyst)\n",
    "- [ETL VS ELT Batch Ingestion Patterns](#etl-vs-elt-batch-ingestion-patterns)\n",
    "- [API AND Rest API](#api--rest-api) \n",
    "- [streaming-ingestion-for-recommender-system](#-streaming-ingestion-for-recommender-system)\n",
    "- [apache kafka](#apache-kafka)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6aaa9a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Data Ingestion \n",
    "\n",
    "# 🔗 Resource\n",
    "\n",
    "To understand more about **Data Ingestion**, refer to this Coursera reading resource:\n",
    "\n",
    "[Batch and Streaming Tools (Coursera)](https://www.coursera.org/learn/source-systems-data-ingestion-and-pipelines/supplement/YD08f/batch-and-streaming-tools)\n",
    "\n",
    "---\n",
    "Nearly all data originates as a **continuous stream of events** (e.g., button clicks, stock price changes, IoT sensor readings).  \n",
    "To handle and process that data, we use ingestion techniques that fall along a **continuum** of frequency.\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 Ingestion Frequencies\n",
    "\n",
    "![Ingestion Frequencies](./images/ingestion_frequencies.png)\n",
    "\n",
    "| Frequency Type | Description       |\n",
    "|----------------|-------------------|\n",
    "| Batch          | Semi-Frequent     |\n",
    "| Micro-batch    | Frequent          |\n",
    "| Streaming      | Very Frequent     |\n",
    "\n",
    "> The **choice of ingestion frequency** depends on:\n",
    "- The **source systems**\n",
    "- The **end use case**\n",
    "\n",
    "---\n",
    "\n",
    "# 🔌 Ways to Ingest Data\n",
    "\n",
    "---\n",
    "\n",
    "## 🗄️ From Databases\n",
    "\n",
    "![Ingest from DB](./images/ways_to_ingest.png)\n",
    "\n",
    "### 🔗 Using Connectors (JDBC/ODBC APIs)\n",
    "- Pulls data using **standard drivers**.\n",
    "- Ingests:\n",
    "  - At regular intervals\n",
    "  - After a threshold of new records\n",
    "\n",
    "> JDBC (Java Database Connectivity) and ODBC (Open Database Connectivity) allow apps to query databases in a standard, language-independent way.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 Using Ingestion Tools\n",
    "\n",
    "- Example: **AWS Glue ETL**\n",
    "- Automates the pull from databases\n",
    "- Ingests data **on a regular basis**\n",
    "\n",
    "---\n",
    "\n",
    "## 📁 From Files\n",
    "\n",
    "![Ingest via Files](./images/ingest_via_files.png)\n",
    "\n",
    "### 🛠️ Manual File Download\n",
    "- Receive file from external source\n",
    "- Upload it manually to the system\n",
    "\n",
    "### 🔐 Secure File Transfer (e.g., AWS Transfer Family)\n",
    "- Protocols used:\n",
    "  - **SFTP**: Secure File Transfer Protocol\n",
    "  - **SCP**: Secure Copy Protocol\n",
    "\n",
    "---\n",
    "\n",
    "## 📡 From Streaming Systems\n",
    "\n",
    "![Streaming Ingestion](./images/ingest_via_streaming_systems.png)\n",
    "\n",
    "- For **real-time or near-real-time** event ingestion\n",
    "- Source: Event Producers like **IoT devices**, apps, etc.\n",
    "- Sent to: Message Queues or Streaming Platforms (e.g., **Amazon Kinesis**, **Apache Kafka**)\n",
    "- Consumed by: Downstream **event consumers**\n",
    "\n",
    "---\n",
    "\n",
    "# 🧠 Batching vs Streaming: Conceptual Continuum\n",
    "\n",
    "Every event can be ingested either:\n",
    "- **One-by-one** (→ **Streaming**)\n",
    "- **Grouped together** (→ **Batch**)\n",
    "\n",
    "### You can impose batch boundaries using:\n",
    "- **Size** (e.g., 10GB chunks)\n",
    "- **Count** (e.g., every 1,000 events)\n",
    "- **Time** (e.g., every 24 hours, every hour)\n",
    "\n",
    "> 🌀 High-frequency batch ingestion eventually approaches real-time streaming.\n",
    "\n",
    "---\n",
    "\n",
    "# ⚖️ Choosing the Right Ingestion Pattern\n",
    "\n",
    "Your choice depends on:\n",
    "- 🔹 What kind of **source system** you're working with (API, DB, Stream)\n",
    "- 🔹 What **latency** the business case demands\n",
    "- 🔹 What the **API or system constraints** are (rate limits, payload size)\n",
    "\n",
    "---\n",
    "\n",
    "# 🧪 Practical Use Cases Coming Up\n",
    "\n",
    "This module covers **two hands-on case studies**:\n",
    "- **Batch ingestion from an API**\n",
    "- **Streaming ingestion from Amazon Kinesis**\n",
    "\n",
    "You'll work with real-world tools like:\n",
    "- **AWS Glue**\n",
    "- **Streaming platforms**\n",
    "- **Secure file transfers**\n",
    "- **Custom connectors**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2dad30",
   "metadata": {},
   "source": [
    "# Stakeholder Conversation Summary: Marketing Analyst\n",
    "\n",
    "## 🎯 Stakeholder\n",
    "**Colleen** — Marketing Analyst\n",
    "\n",
    "## 👋 Data Engineer\n",
    "**Joe** — New Data Engineer at the e-commerce company\n",
    "\n",
    "---\n",
    "\n",
    "## 🗣️ Conversation Overview\n",
    "\n",
    "In this discussion, Colleen (Marketing Analyst) and Joe (Data Engineer) explore how the marketing team can gain insights into **external factors** that may influence **customer purchasing behavior**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Business Goal\n",
    "To analyze **external signals** — such as **music listening trends** — that could correlate with **online shopping behavior**, helping to uncover new **sales insights and patterns**.\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Key Idea from Marketing\n",
    "- Customer **emotions or moods** (e.g., happy, sad, excited, relaxed) may influence shopping habits.\n",
    "- Direct emotional data is unavailable, but **music listening patterns** may act as a **proxy** for mood.\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Proposed Data Source\n",
    "- **Spotify Public API**:\n",
    "  - Provides access to **trending artists**, **listening trends** over time.\n",
    "  - Data available across **different regions**.\n",
    "  \n",
    "---\n",
    "\n",
    "## 📥 Data Ingestion Needs\n",
    "- Pull data from the **Spotify API** (public third-party REST API).\n",
    "- Compare **regional music trends** with **product sales data**.\n",
    "- Use this for **marketing analysis and insight generation**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧰 Next Steps for the Data Engineer\n",
    "- Review the **Spotify API documentation**.\n",
    "- Identify what **data can be accessed** (e.g., top artists by region, listening time series).\n",
    "- Clarify what **specific data** the marketing team wants and **how to serve it** (e.g., dashboards, reports).\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Note from the Course\n",
    "While using music trends may not seem like a strong marketing strategy, it's common for stakeholders to request **unusual data sources**. The goal here is to learn **requirement gathering** and **API data ingestion** techniques — not to judge the validity of the idea.\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Key Concept Introduced\n",
    "- The pipeline will require **batch data ingestion** from a **third-party REST API**.\n",
    "- Next steps involve exploring **ETL vs ELT** strategies before implementing the ingestion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f234054d",
   "metadata": {},
   "source": [
    "# ETL vs ELT: Batch Ingestion Patterns\n",
    "\n",
    "##   Resource  \n",
    "\n",
    "For an official summary of the differences between ETL and ELT, refer to this Coursera resource:  \n",
    "[Summary of the Differences: ETL vs ELT](https://www.coursera.org/learn/source-systems-data-ingestion-and-pipelines/supplement/FN6ny/summary-of-the-differences-etl-vs-elt)\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Goals of the Marketing Analyst\n",
    "\n",
    "The marketing analyst is interested in **analyzing historical trends** by ingesting **external data** (e.g., from Spotify API) in **batch**. Since there is:\n",
    "\n",
    "- **No need for real-time analysis**\n",
    "- **Limited frequency of API requests**\n",
    "\n",
    "A batch ingestion pipeline is most suitable.\n",
    "\n",
    "![Goals of the Marketing Analyst](./images/ma_goals.png)\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Batch Ingestion Patterns: ETL vs ELT\n",
    "\n",
    "![ETL vs ELT](./images/etl-vs_elt.png)\n",
    "\n",
    "### 🧪 ETL (Extract → Transform → Load)\n",
    "\n",
    "- **Extract** raw data from source\n",
    "- **Transform** it in a staging area\n",
    "- **Load** the transformed data into a data warehouse\n",
    "\n",
    "📉 *Potential information loss* during early transformation.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 ELT (Extract → Load → Transform)\n",
    "\n",
    "- **Extract** raw data\n",
    "- **Load** raw data directly into the target destination\n",
    "- **Transform** inside the target system (e.g., data warehouse)\n",
    "\n",
    "📦 *All data is captured*, providing more flexibility.\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 Advantages of ELT\n",
    "\n",
    "![Advantages of ELT](./images/adv_elt.png)\n",
    "\n",
    "1. ✅ **Faster to implement**\n",
    "2. ⚡ **Data available quickly to end users**\n",
    "3. 🔄 **Transformations can still be done efficiently**\n",
    "4. 🔧 **Flexible — transformations can be decided later**\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Downsides of ELT\n",
    "\n",
    "![Disadvantages of ELT](./images/dis_elt.png)\n",
    "\n",
    "- 💥 If not carefully planned, it becomes just an **EL pipeline**\n",
    "- 🐊 May result in a **Data Swamp**: unorganized, unmanageable, and unusable data\n",
    "- ❓ *You must ask*: \"How will you use the data?\"\n",
    "\n",
    "---\n",
    "\n",
    "## 👥 Use Case: Conversation with the Marketing Analyst\n",
    "\n",
    "The marketing analyst wants to explore new patterns from external data (e.g., music listening trends) via exploratory analysis.\n",
    "\n",
    "Since the transformations required aren't clearly defined upfront, **ELT is the better choice** here.\n",
    "\n",
    "![ELT for Marketing Analyst](./images/elt_ma.png)\n",
    "\n",
    "---\n",
    "\n",
    "# 🧮 Detailed Comparison Table\n",
    "\n",
    "| Feature | **ETL** | **ELT** |\n",
    "|--------|--------|--------|\n",
    "| **History** | Popular in the 80s–90s when storage was expensive and data was smaller | Became popular with cloud storage & explosion of big data |\n",
    "| **Transformation Timing** | Before loading into the warehouse (predefined schema needed) | After loading into the warehouse (can delay schema decisions) |\n",
    "| **Processing Power Used** | External staging tools or ETL platforms | Modern data warehouse (e.g., BigQuery, Redshift, Snowflake) |\n",
    "| **Flexibility** | Low — schema must be known early | High — raw data allows flexible querying and transformations |\n",
    "| **Data Types Supported** | Mainly structured data | Structured, semi-structured (JSON), unstructured (images/text) |\n",
    "| **Maintenance** | Costly — must re-ingest if transformation was wrong | Easier — raw data already loaded, can re-transform anytime |\n",
    "| **Load Time** | Longer — needs staging and transformation first | Faster — directly load raw data |\n",
    "| **Transformation Time** | Depends on tool and complexity | Faster — utilizes scalable DW compute |\n",
    "| **Scalability** | Scalable but harder to manage multiple sources/targets | Highly scalable with cloud warehouses |\n",
    "| **Cost** | Depends on ETL tools and compute | Lower due to modern cloud infra but still depends on volume |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0a7116",
   "metadata": {},
   "source": [
    "# API & Rest API\n",
    "\n",
    "An **API (Application Programming Interface)** is a set of **rules and specifications** that allows applications to **programmatically communicate and exchange data**.\n",
    "\n",
    "> Think of an API as a waiter in a restaurant — it takes your request to the kitchen and brings back the food you asked for.\n",
    "\n",
    "![What is an API](./images/api_.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 🧰 Why APIs Matter\n",
    "\n",
    "APIs allow:\n",
    "\n",
    "- 💬 Communication between different services and applications\n",
    "- 🤝 Stable interfaces across teams\n",
    "- 🌍 Integration with third-party services (e.g., Spotify, Twitter, AWS)\n",
    "\n",
    "> For example:\n",
    "> - Social media apps use APIs to load posts and reactions\n",
    "> - E-commerce platforms use APIs to interact with payment systems\n",
    "> - Data engineers use APIs to fetch external datasets\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ Key Features of APIs\n",
    "\n",
    "![API Features](./images/api_feature.png)\n",
    "\n",
    "- **Metadata** – Provides context about the data (like column names, units)\n",
    "- **Documentation** – Helps developers understand how to use the API\n",
    "- **Authentication** – Ensures that only authorized users can access the data\n",
    "- **Error Handling** – Makes debugging easier when requests fail\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 What is a REST API?\n",
    "\n",
    "A **REST API (Representational State Transfer API)** is the most common type of API that uses **HTTP protocols** to facilitate communication.\n",
    "\n",
    "![REST API](./images/rest_api.png)\n",
    "\n",
    "### How it works:\n",
    "\n",
    "- The client (e.g., data engineer or browser) sends an **HTTP request** to a server.\n",
    "- The server **responds with the requested resource**, such as JSON data or a webpage.\n",
    "\n",
    "### REST API is stateless:\n",
    "Every request from the client must contain all the information the server needs to fulfill the request. It doesn’t store client context between requests.\n",
    "\n",
    "---\n",
    "\n",
    "## 📦 Real-World Use Case\n",
    "\n",
    "In our project, the **marketing analyst** wants to retrieve data from **Spotify**, which exposes data via a **public REST API**.\n",
    "\n",
    "This is a common situation where a **data engineer** connects to an **external source system** through an API to extract data for further analysis — such as identifying trends in regional music consumption.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c1f7bd",
   "metadata": {},
   "source": [
    "# 📡 Streaming Ingestion for Recommender System \n",
    "\n",
    "## 🎯 Goal\n",
    "Build a **real-time data ingestion pipeline** to feed a **product recommender system** using **website user activity data**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🗣 Stakeholder Conversation Highlights\n",
    "\n",
    "### 1. Context\n",
    "- Current website logs capture **all events**:\n",
    "  - Internal system metrics (performance, errors, anomalies)\n",
    "  - User activity (clicks, product views, add-to-cart, purchases)\n",
    "- As a data engineer, you only need **user activity events** for the recommender system.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Separation of Data\n",
    "- Request made to **separate user activity logs** from system metrics.\n",
    "- Upstream software engineer agreed:\n",
    "  - Will push **only user activity messages** into a dedicated **stream**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Streaming Platform Choice\n",
    "- **Amazon Kinesis Data Streams** selected for ingestion.\n",
    "- Benefits:\n",
    "  - Scales via **shards**\n",
    "  - Maintains **ordering per partition key**\n",
    "  - Supports **parallel consumption**\n",
    "  - Provides **retention window** for replay\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Message Format & Volume\n",
    "- **Format:** JSON payload\n",
    "- **Fields:** session ID, customer info (location), browsing actions\n",
    "- **Size:** Few hundred bytes per event\n",
    "- **Rate:** \n",
    "  - Peak users: ~10,000\n",
    "  - Each generates several events/minute\n",
    "  - Approx. **1,000 events/sec** at peak\n",
    "- **Throughput:** <1 MB/sec (well within Kinesis capacity)\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Retention Planning\n",
    "- Retain messages for **1 day**:\n",
    "  - Allows replay if downstream pipeline fails\n",
    "  - At 1 MB/sec for ~100,000 seconds/day → ~100 GB/day storage worst case\n",
    "- Stream acts as an **append-only log**; old data purged after retention expires.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 Pipeline Flow\n",
    "1. Website captures events.\n",
    "2. Internal metrics discarded for this pipeline.\n",
    "3. **User activity events → Kinesis Data Stream**\n",
    "4. Consumer application:\n",
    "   - **Real-time**: Feed recommender engine\n",
    "   - **Archival**: Store in S3/data lake for offline training\n",
    "5. Retention in Kinesis ensures replay within 24 hours if needed.\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Key Takeaways\n",
    "- Separate **user events** from **system logs** upstream.\n",
    "- Use **Kinesis Data Streams** for scalable, ordered, real-time ingestion.\n",
    "- Plan **shards** for throughput and **retention** for recovery.\n",
    "- Always archive for offline analysis in addition to real-time processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46691e2c",
   "metadata": {},
   "source": [
    "# Apache Kafka\n",
    "\n",
    "Apache Kafka is an **open-source event streaming platform** used for building **real-time data pipelines** and **streaming applications**.  \n",
    "It enables **publish–subscribe messaging** with high throughput, scalability, and fault tolerance.\n",
    "\n",
    "---\n",
    "\n",
    "## 1️⃣ Streaming Systems Overview\n",
    "In streaming systems, data flows continuously from a **source system** (event producer) to a **destination** (event consumer).\n",
    "\n",
    "![Streaming Systems](./images/streaming_systems.png)\n",
    "\n",
    "- **Event Producer**: Generates the data/events.\n",
    "- **Event Streaming Platform**: Stores and transports the data.\n",
    "- **Event Consumer**: Reads and processes the data.\n",
    "- **Data Engineer**: Designs and maintains the ingestion pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## 2️⃣ Message Queues vs Event Streaming Platforms\n",
    "\n",
    "![Message Queue vs Event Streaming](./images/message_queue_event_streaming.png)\n",
    "\n",
    "### **Message Queue**\n",
    "- Acts as a buffer between producer and consumer.\n",
    "- Operates in **FIFO** (First In, First Out) mode.\n",
    "- Once consumed, the message is **removed** from the queue.\n",
    "\n",
    "### **Event Streaming Platform**\n",
    "- Uses an **append-only persistent log**.\n",
    "- Can store messages for a **configurable retention period**.\n",
    "- Allows **multiple consumers** to read the same data independently.\n",
    "- Supports **replaying** past events.\n",
    "\n",
    "---\n",
    "\n",
    "## 3️⃣ Kafka Architecture\n",
    "\n",
    "![Kafka Architecture](./images/kafka_.png)\n",
    "\n",
    "- **Event Producers**: Applications or services that push messages into Kafka.\n",
    "- **Kafka Cluster**: Made up of multiple servers called **brokers**.\n",
    "- **Topics**: Categories that store related messages.\n",
    "- **Event Consumers**: Applications that pull messages from Kafka topics.\n",
    "\n",
    "**Push Messages** → Producers send data to Kafka topics.  \n",
    "**Pull Messages** → Consumers read data from Kafka topics.\n",
    "\n",
    "---\n",
    "\n",
    "## 4️⃣ Core Kafka Concepts in Detail\n",
    "\n",
    "### 🖥 Cluster\n",
    "- A **cluster** is a group of Kafka **brokers** working together.\n",
    "- **Why a cluster?** Scalability & fault tolerance.\n",
    "- If one broker fails, others keep the system running.\n",
    "- **Example:** A cluster with 3 brokers:  \n",
    "  `broker-1`, `broker-2`, `broker-3`.\n",
    "\n",
    "---\n",
    "\n",
    "### 🗄 Brokers\n",
    "- A **broker** is a single Kafka server that stores data and serves clients.\n",
    "- Brokers handle:\n",
    "  - Storing topic partitions.\n",
    "  - Serving read/write requests from producers/consumers.\n",
    "- Multiple brokers form a cluster.\n",
    "- **Example:**  \n",
    "  - Broker-1 stores `TopicA-Partition0` and `TopicB-Partition1`.  \n",
    "  - Broker-2 stores `TopicA-Partition1` and `TopicB-Partition0`.\n",
    "\n",
    "---\n",
    "\n",
    "### 🗂 Topics\n",
    "- A **topic** is a **category or feed name** to which records are sent by producers.\n",
    "- Topics organize data streams.\n",
    "- Consumers subscribe to one or more topics to read messages.\n",
    "- **Example Topics:**\n",
    "  - `fraud-alerts`\n",
    "  - `customer-orders`\n",
    "  - `temperature-readings`\n",
    "\n",
    "**Analogy:** A topic is like a **mailbox** — producers put messages in it, consumers pick them up.\n",
    "\n",
    "---\n",
    "\n",
    "### 🛣 Partitions\n",
    "![Kafka Topics and Partitions](./images/kafka_topic.png)\n",
    "\n",
    "- Each topic is split into **partitions**.\n",
    "- A **partition** is an **append-only, ordered log**.\n",
    "- Kafka guarantees **message order within a partition**.\n",
    "- Partitions enable **parallel processing**.\n",
    "- More partitions = higher throughput.\n",
    "\n",
    "**Analogy:**  \n",
    "- Topic = highway  \n",
    "- Partition = lane on the highway  \n",
    "- Messages = cars moving in order within a lane.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔑 Keys & Routing\n",
    "- Producers can send a **key** with each message.\n",
    "- Kafka hashes the key to decide **which partition** to send the message to.\n",
    "- **Guarantee:** Messages with the same key always go to the **same partition** (order preserved for that key).\n",
    "- **Example:**  \n",
    "  - Key = `user_42` → always lands in Partition 3.  \n",
    "  - Ensures all actions from `user_42` stay ordered.\n",
    "\n",
    "---\n",
    "\n",
    "### 👥 Consumer Groups\n",
    "- Consumers are organized into **groups**.\n",
    "- Each partition is read by only **one consumer in a group**.\n",
    "- Multiple consumers in a group → parallel reading.\n",
    "- Different groups get independent copies of the data.\n",
    "- **Example:**  \n",
    "  - `recs-service` group processes real-time recommendations.  \n",
    "  - `analytics` group writes data to S3 for offline analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### 📍 Offsets\n",
    "- Each message in a partition has an **offset** — a unique position number.\n",
    "- Consumers keep track of the last offset they processed.\n",
    "- Allows **restarts** and **replays** by resetting the offset.\n",
    "\n",
    "---\n",
    "\n",
    "### ♻ Retention\n",
    "- Kafka keeps data for a **configured time** (default: 7 days).\n",
    "- Messages are not removed immediately after being read.\n",
    "- Allows consumers to **reprocess past events**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 Replication\n",
    "- Each partition has a **leader** replica and **follower** replicas on different brokers.\n",
    "- If a broker with the leader fails, a follower takes over.\n",
    "- Provides **high availability**.\n",
    "\n",
    "---\n",
    "\n",
    "## 5️⃣ How It All Works Together\n",
    "1. **Producer** sends a message with a key.\n",
    "2. Kafka hashes the key → decides partition.\n",
    "3. The message is appended to the partition log.\n",
    "4. The **leader broker** for that partition stores the message and replicates it to followers.\n",
    "5. **Consumers** in a group read from different partitions in parallel.\n",
    "6. Offsets track read positions; retention ensures replayability.\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 More Reference\n",
    "🎥 [YouTube: Apache Kafka Explained](https://www.youtube.com/watch?v=QkdkLdMBuL0&t=56s)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
