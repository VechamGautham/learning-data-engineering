{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e598ba85",
   "metadata": {},
   "source": [
    "## 📚 Table of Contents \n",
    "- [Data Ingestion](#data-ingestion)\n",
    "- [Stakeholder Conversation Summary](#stakeholder-conversation-summary-marketing-analyst)\n",
    "- [ETL VS ELT Batch Ingestion Patterns](#etl-vs-elt-batch-ingestion-patterns)\n",
    "- [API AND Rest API](#api--rest-api) \n",
    "- [streaming-ingestion-for-recommender-system](#-streaming-ingestion-for-recommender-system)\n",
    "- [apache kafka](#apache-kafka)\n",
    "- [Change Data Capture (CDC)](#change-data-capture-cdc)\n",
    "- [general-considerations-for-choosing-ingestion-tools](#general-considerations-for-choosing-ingestion-tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6aaa9a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Data Ingestion \n",
    "\n",
    "# 🔗 Resource\n",
    "\n",
    "To understand more about **Data Ingestion**, refer to this Coursera reading resource:\n",
    "\n",
    "[Batch and Streaming Tools (Coursera)](https://www.coursera.org/learn/source-systems-data-ingestion-and-pipelines/supplement/YD08f/batch-and-streaming-tools)\n",
    "\n",
    "---\n",
    "Nearly all data originates as a **continuous stream of events** (e.g., button clicks, stock price changes, IoT sensor readings).  \n",
    "To handle and process that data, we use ingestion techniques that fall along a **continuum** of frequency.\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 Ingestion Frequencies\n",
    "\n",
    "![Ingestion Frequencies](./images/ingestion_frequencies.png)\n",
    "\n",
    "| Frequency Type | Description       |\n",
    "|----------------|-------------------|\n",
    "| Batch          | Semi-Frequent     |\n",
    "| Micro-batch    | Frequent          |\n",
    "| Streaming      | Very Frequent     |\n",
    "\n",
    "> The **choice of ingestion frequency** depends on:\n",
    "- The **source systems**\n",
    "- The **end use case**\n",
    "\n",
    "---\n",
    "\n",
    "# 🔌 Ways to Ingest Data\n",
    "\n",
    "---\n",
    "\n",
    "## 🗄️ From Databases\n",
    "\n",
    "![Ingest from DB](./images/ways_to_ingest.png)\n",
    "\n",
    "### 🔗 Using Connectors (JDBC/ODBC APIs)\n",
    "- Pulls data using **standard drivers**.\n",
    "- Ingests:\n",
    "  - At regular intervals\n",
    "  - After a threshold of new records\n",
    "\n",
    "> JDBC (Java Database Connectivity) and ODBC (Open Database Connectivity) allow apps to query databases in a standard, language-independent way.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 Using Ingestion Tools\n",
    "\n",
    "- Example: **AWS Glue ETL**\n",
    "- Automates the pull from databases\n",
    "- Ingests data **on a regular basis**\n",
    "\n",
    "---\n",
    "\n",
    "## 📁 From Files\n",
    "\n",
    "![Ingest via Files](./images/ingest_via_files.png)\n",
    "\n",
    "### 🛠️ Manual File Download\n",
    "- Receive file from external source\n",
    "- Upload it manually to the system\n",
    "\n",
    "### 🔐 Secure File Transfer (e.g., AWS Transfer Family)\n",
    "- Protocols used:\n",
    "  - **SFTP**: Secure File Transfer Protocol\n",
    "  - **SCP**: Secure Copy Protocol\n",
    "\n",
    "---\n",
    "\n",
    "## 📡 From Streaming Systems\n",
    "\n",
    "![Streaming Ingestion](./images/ingest_via_streaming_systems.png)\n",
    "\n",
    "- For **real-time or near-real-time** event ingestion\n",
    "- Source: Event Producers like **IoT devices**, apps, etc.\n",
    "- Sent to: Message Queues or Streaming Platforms (e.g., **Amazon Kinesis**, **Apache Kafka**)\n",
    "- Consumed by: Downstream **event consumers**\n",
    "\n",
    "---\n",
    "\n",
    "# 🧠 Batching vs Streaming: Conceptual Continuum\n",
    "\n",
    "Every event can be ingested either:\n",
    "- **One-by-one** (→ **Streaming**)\n",
    "- **Grouped together** (→ **Batch**)\n",
    "\n",
    "### You can impose batch boundaries using:\n",
    "- **Size** (e.g., 10GB chunks)\n",
    "- **Count** (e.g., every 1,000 events)\n",
    "- **Time** (e.g., every 24 hours, every hour)\n",
    "\n",
    "> 🌀 High-frequency batch ingestion eventually approaches real-time streaming.\n",
    "\n",
    "---\n",
    "\n",
    "# ⚖️ Choosing the Right Ingestion Pattern\n",
    "\n",
    "Your choice depends on:\n",
    "- 🔹 What kind of **source system** you're working with (API, DB, Stream)\n",
    "- 🔹 What **latency** the business case demands\n",
    "- 🔹 What the **API or system constraints** are (rate limits, payload size)\n",
    "\n",
    "---\n",
    "\n",
    "# 🧪 Practical Use Cases Coming Up\n",
    "\n",
    "This module covers **two hands-on case studies**:\n",
    "- **Batch ingestion from an API**\n",
    "- **Streaming ingestion from Amazon Kinesis**\n",
    "\n",
    "You'll work with real-world tools like:\n",
    "- **AWS Glue**\n",
    "- **Streaming platforms**\n",
    "- **Secure file transfers**\n",
    "- **Custom connectors**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2dad30",
   "metadata": {},
   "source": [
    "# Stakeholder Conversation Summary: Marketing Analyst\n",
    "\n",
    "## 🎯 Stakeholder\n",
    "**Colleen** — Marketing Analyst\n",
    "\n",
    "## 👋 Data Engineer\n",
    "**Joe** — New Data Engineer at the e-commerce company\n",
    "\n",
    "---\n",
    "\n",
    "## 🗣️ Conversation Overview\n",
    "\n",
    "In this discussion, Colleen (Marketing Analyst) and Joe (Data Engineer) explore how the marketing team can gain insights into **external factors** that may influence **customer purchasing behavior**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Business Goal\n",
    "To analyze **external signals** — such as **music listening trends** — that could correlate with **online shopping behavior**, helping to uncover new **sales insights and patterns**.\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Key Idea from Marketing\n",
    "- Customer **emotions or moods** (e.g., happy, sad, excited, relaxed) may influence shopping habits.\n",
    "- Direct emotional data is unavailable, but **music listening patterns** may act as a **proxy** for mood.\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Proposed Data Source\n",
    "- **Spotify Public API**:\n",
    "  - Provides access to **trending artists**, **listening trends** over time.\n",
    "  - Data available across **different regions**.\n",
    "  \n",
    "---\n",
    "\n",
    "## 📥 Data Ingestion Needs\n",
    "- Pull data from the **Spotify API** (public third-party REST API).\n",
    "- Compare **regional music trends** with **product sales data**.\n",
    "- Use this for **marketing analysis and insight generation**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧰 Next Steps for the Data Engineer\n",
    "- Review the **Spotify API documentation**.\n",
    "- Identify what **data can be accessed** (e.g., top artists by region, listening time series).\n",
    "- Clarify what **specific data** the marketing team wants and **how to serve it** (e.g., dashboards, reports).\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Note from the Course\n",
    "While using music trends may not seem like a strong marketing strategy, it's common for stakeholders to request **unusual data sources**. The goal here is to learn **requirement gathering** and **API data ingestion** techniques — not to judge the validity of the idea.\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Key Concept Introduced\n",
    "- The pipeline will require **batch data ingestion** from a **third-party REST API**.\n",
    "- Next steps involve exploring **ETL vs ELT** strategies before implementing the ingestion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f234054d",
   "metadata": {},
   "source": [
    "# ETL vs ELT: Batch Ingestion Patterns\n",
    "\n",
    "##   Resource  \n",
    "\n",
    "For an official summary of the differences between ETL and ELT, refer to this Coursera resource:  \n",
    "[Summary of the Differences: ETL vs ELT](https://www.coursera.org/learn/source-systems-data-ingestion-and-pipelines/supplement/FN6ny/summary-of-the-differences-etl-vs-elt)\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Goals of the Marketing Analyst\n",
    "\n",
    "The marketing analyst is interested in **analyzing historical trends** by ingesting **external data** (e.g., from Spotify API) in **batch**. Since there is:\n",
    "\n",
    "- **No need for real-time analysis**\n",
    "- **Limited frequency of API requests**\n",
    "\n",
    "A batch ingestion pipeline is most suitable.\n",
    "\n",
    "![Goals of the Marketing Analyst](./images/ma_goals.png)\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Batch Ingestion Patterns: ETL vs ELT\n",
    "\n",
    "![ETL vs ELT](./images/etl-vs_elt.png)\n",
    "\n",
    "### 🧪 ETL (Extract → Transform → Load)\n",
    "\n",
    "- **Extract** raw data from source\n",
    "- **Transform** it in a staging area\n",
    "- **Load** the transformed data into a data warehouse\n",
    "\n",
    "📉 *Potential information loss* during early transformation.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 ELT (Extract → Load → Transform)\n",
    "\n",
    "- **Extract** raw data\n",
    "- **Load** raw data directly into the target destination\n",
    "- **Transform** inside the target system (e.g., data warehouse)\n",
    "\n",
    "📦 *All data is captured*, providing more flexibility.\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 Advantages of ELT\n",
    "\n",
    "![Advantages of ELT](./images/adv_elt.png)\n",
    "\n",
    "1. ✅ **Faster to implement**\n",
    "2. ⚡ **Data available quickly to end users**\n",
    "3. 🔄 **Transformations can still be done efficiently**\n",
    "4. 🔧 **Flexible — transformations can be decided later**\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Downsides of ELT\n",
    "\n",
    "![Disadvantages of ELT](./images/dis_elt.png)\n",
    "\n",
    "- 💥 If not carefully planned, it becomes just an **EL pipeline**\n",
    "- 🐊 May result in a **Data Swamp**: unorganized, unmanageable, and unusable data\n",
    "- ❓ *You must ask*: \"How will you use the data?\"\n",
    "\n",
    "---\n",
    "\n",
    "## 👥 Use Case: Conversation with the Marketing Analyst\n",
    "\n",
    "The marketing analyst wants to explore new patterns from external data (e.g., music listening trends) via exploratory analysis.\n",
    "\n",
    "Since the transformations required aren't clearly defined upfront, **ELT is the better choice** here.\n",
    "\n",
    "![ELT for Marketing Analyst](./images/elt_ma.png)\n",
    "\n",
    "---\n",
    "\n",
    "# 🧮 Detailed Comparison Table\n",
    "\n",
    "| Feature | **ETL** | **ELT** |\n",
    "|--------|--------|--------|\n",
    "| **History** | Popular in the 80s–90s when storage was expensive and data was smaller | Became popular with cloud storage & explosion of big data |\n",
    "| **Transformation Timing** | Before loading into the warehouse (predefined schema needed) | After loading into the warehouse (can delay schema decisions) |\n",
    "| **Processing Power Used** | External staging tools or ETL platforms | Modern data warehouse (e.g., BigQuery, Redshift, Snowflake) |\n",
    "| **Flexibility** | Low — schema must be known early | High — raw data allows flexible querying and transformations |\n",
    "| **Data Types Supported** | Mainly structured data | Structured, semi-structured (JSON), unstructured (images/text) |\n",
    "| **Maintenance** | Costly — must re-ingest if transformation was wrong | Easier — raw data already loaded, can re-transform anytime |\n",
    "| **Load Time** | Longer — needs staging and transformation first | Faster — directly load raw data |\n",
    "| **Transformation Time** | Depends on tool and complexity | Faster — utilizes scalable DW compute |\n",
    "| **Scalability** | Scalable but harder to manage multiple sources/targets | Highly scalable with cloud warehouses |\n",
    "| **Cost** | Depends on ETL tools and compute | Lower due to modern cloud infra but still depends on volume |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0a7116",
   "metadata": {},
   "source": [
    "# API & Rest API\n",
    "\n",
    "An **API (Application Programming Interface)** is a set of **rules and specifications** that allows applications to **programmatically communicate and exchange data**.\n",
    "\n",
    "> Think of an API as a waiter in a restaurant — it takes your request to the kitchen and brings back the food you asked for.\n",
    "\n",
    "![What is an API](./images/api_.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 🧰 Why APIs Matter\n",
    "\n",
    "APIs allow:\n",
    "\n",
    "- 💬 Communication between different services and applications\n",
    "- 🤝 Stable interfaces across teams\n",
    "- 🌍 Integration with third-party services (e.g., Spotify, Twitter, AWS)\n",
    "\n",
    "> For example:\n",
    "> - Social media apps use APIs to load posts and reactions\n",
    "> - E-commerce platforms use APIs to interact with payment systems\n",
    "> - Data engineers use APIs to fetch external datasets\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ Key Features of APIs\n",
    "\n",
    "![API Features](./images/api_feature.png)\n",
    "\n",
    "- **Metadata** – Provides context about the data (like column names, units)\n",
    "- **Documentation** – Helps developers understand how to use the API\n",
    "- **Authentication** – Ensures that only authorized users can access the data\n",
    "- **Error Handling** – Makes debugging easier when requests fail\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 What is a REST API?\n",
    "\n",
    "A **REST API (Representational State Transfer API)** is the most common type of API that uses **HTTP protocols** to facilitate communication.\n",
    "\n",
    "![REST API](./images/rest_api.png)\n",
    "\n",
    "### How it works:\n",
    "\n",
    "- The client (e.g., data engineer or browser) sends an **HTTP request** to a server.\n",
    "- The server **responds with the requested resource**, such as JSON data or a webpage.\n",
    "\n",
    "### REST API is stateless:\n",
    "Every request from the client must contain all the information the server needs to fulfill the request. It doesn’t store client context between requests.\n",
    "\n",
    "---\n",
    "\n",
    "## 📦 Real-World Use Case\n",
    "\n",
    "In our project, the **marketing analyst** wants to retrieve data from **Spotify**, which exposes data via a **public REST API**.\n",
    "\n",
    "This is a common situation where a **data engineer** connects to an **external source system** through an API to extract data for further analysis — such as identifying trends in regional music consumption.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c1f7bd",
   "metadata": {},
   "source": [
    "# 📡 Streaming Ingestion for Recommender System \n",
    "\n",
    "## 🎯 Goal\n",
    "Build a **real-time data ingestion pipeline** to feed a **product recommender system** using **website user activity data**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🗣 Stakeholder Conversation Highlights\n",
    "\n",
    "### 1. Context\n",
    "- Current website logs capture **all events**:\n",
    "  - Internal system metrics (performance, errors, anomalies)\n",
    "  - User activity (clicks, product views, add-to-cart, purchases)\n",
    "- As a data engineer, you only need **user activity events** for the recommender system.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Separation of Data\n",
    "- Request made to **separate user activity logs** from system metrics.\n",
    "- Upstream software engineer agreed:\n",
    "  - Will push **only user activity messages** into a dedicated **stream**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Streaming Platform Choice\n",
    "- **Amazon Kinesis Data Streams** selected for ingestion.\n",
    "- Benefits:\n",
    "  - Scales via **shards**\n",
    "  - Maintains **ordering per partition key**\n",
    "  - Supports **parallel consumption**\n",
    "  - Provides **retention window** for replay\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Message Format & Volume\n",
    "- **Format:** JSON payload\n",
    "- **Fields:** session ID, customer info (location), browsing actions\n",
    "- **Size:** Few hundred bytes per event\n",
    "- **Rate:** \n",
    "  - Peak users: ~10,000\n",
    "  - Each generates several events/minute\n",
    "  - Approx. **1,000 events/sec** at peak\n",
    "- **Throughput:** <1 MB/sec (well within Kinesis capacity)\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Retention Planning\n",
    "- Retain messages for **1 day**:\n",
    "  - Allows replay if downstream pipeline fails\n",
    "  - At 1 MB/sec for ~100,000 seconds/day → ~100 GB/day storage worst case\n",
    "- Stream acts as an **append-only log**; old data purged after retention expires.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 Pipeline Flow\n",
    "1. Website captures events.\n",
    "2. Internal metrics discarded for this pipeline.\n",
    "3. **User activity events → Kinesis Data Stream**\n",
    "4. Consumer application:\n",
    "   - **Real-time**: Feed recommender engine\n",
    "   - **Archival**: Store in S3/data lake for offline training\n",
    "5. Retention in Kinesis ensures replay within 24 hours if needed.\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Key Takeaways\n",
    "- Separate **user events** from **system logs** upstream.\n",
    "- Use **Kinesis Data Streams** for scalable, ordered, real-time ingestion.\n",
    "- Plan **shards** for throughput and **retention** for recovery.\n",
    "- Always archive for offline analysis in addition to real-time processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46691e2c",
   "metadata": {},
   "source": [
    "# Apache Kafka\n",
    "\n",
    "Apache Kafka is an **open-source event streaming platform** used for building **real-time data pipelines** and **streaming applications**.  \n",
    "It enables **publish–subscribe messaging** with high throughput, scalability, and fault tolerance.\n",
    "\n",
    "---\n",
    "\n",
    "## 1️⃣ Streaming Systems Overview\n",
    "In streaming systems, data flows continuously from a **source system** (event producer) to a **destination** (event consumer).\n",
    "\n",
    "![Streaming Systems](./images/streaming_systems.png)\n",
    "\n",
    "- **Event Producer**: Generates the data/events.\n",
    "- **Event Streaming Platform**: Stores and transports the data.\n",
    "- **Event Consumer**: Reads and processes the data.\n",
    "- **Data Engineer**: Designs and maintains the ingestion pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## 2️⃣ Message Queues vs Event Streaming Platforms\n",
    "\n",
    "![Message Queue vs Event Streaming](./images/message_queue_event_streaming.png)\n",
    "\n",
    "### **Message Queue**\n",
    "- Acts as a buffer between producer and consumer.\n",
    "- Operates in **FIFO** (First In, First Out) mode.\n",
    "- Once consumed, the message is **removed** from the queue.\n",
    "\n",
    "### **Event Streaming Platform**\n",
    "- Uses an **append-only persistent log**.\n",
    "- Can store messages for a **configurable retention period**.\n",
    "- Allows **multiple consumers** to read the same data independently.\n",
    "- Supports **replaying** past events.\n",
    "\n",
    "---\n",
    "\n",
    "## 3️⃣ Kafka Architecture\n",
    "\n",
    "![Kafka Architecture](./images/kafka_.png)\n",
    "\n",
    "- **Event Producers**: Applications or services that push messages into Kafka.\n",
    "- **Kafka Cluster**: Made up of multiple servers called **brokers**.\n",
    "- **Topics**: Categories that store related messages.\n",
    "- **Event Consumers**: Applications that pull messages from Kafka topics.\n",
    "\n",
    "**Push Messages** → Producers send data to Kafka topics.  \n",
    "**Pull Messages** → Consumers read data from Kafka topics.\n",
    "\n",
    "---\n",
    "\n",
    "## 4️⃣ Core Kafka Concepts in Detail\n",
    "\n",
    "### 🖥 Cluster\n",
    "- A **cluster** is a group of Kafka **brokers** working together.\n",
    "- **Why a cluster?** Scalability & fault tolerance.\n",
    "- If one broker fails, others keep the system running.\n",
    "- **Example:** A cluster with 3 brokers:  \n",
    "  `broker-1`, `broker-2`, `broker-3`.\n",
    "\n",
    "---\n",
    "\n",
    "### 🗄 Brokers\n",
    "- A **broker** is a single Kafka server that stores data and serves clients.\n",
    "- Brokers handle:\n",
    "  - Storing topic partitions.\n",
    "  - Serving read/write requests from producers/consumers.\n",
    "- Multiple brokers form a cluster.\n",
    "- **Example:**  \n",
    "  - Broker-1 stores `TopicA-Partition0` and `TopicB-Partition1`.  \n",
    "  - Broker-2 stores `TopicA-Partition1` and `TopicB-Partition0`.\n",
    "\n",
    "---\n",
    "\n",
    "### 🗂 Topics\n",
    "- A **topic** is a **category or feed name** to which records are sent by producers.\n",
    "- Topics organize data streams.\n",
    "- Consumers subscribe to one or more topics to read messages.\n",
    "- **Example Topics:**\n",
    "  - `fraud-alerts`\n",
    "  - `customer-orders`\n",
    "  - `temperature-readings`\n",
    "\n",
    "**Analogy:** A topic is like a **mailbox** — producers put messages in it, consumers pick them up.\n",
    "\n",
    "---\n",
    "\n",
    "### 🛣 Partitions\n",
    "![Kafka Topics and Partitions](./images/kafka_topic.png)\n",
    "\n",
    "- Each topic is split into **partitions**.\n",
    "- A **partition** is an **append-only, ordered log**.\n",
    "- Kafka guarantees **message order within a partition**.\n",
    "- Partitions enable **parallel processing**.\n",
    "- More partitions = higher throughput.\n",
    "\n",
    "**Analogy:**  \n",
    "- Topic = highway  \n",
    "- Partition = lane on the highway  \n",
    "- Messages = cars moving in order within a lane.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔑 Keys & Routing\n",
    "- Producers can send a **key** with each message.\n",
    "- Kafka hashes the key to decide **which partition** to send the message to.\n",
    "- **Guarantee:** Messages with the same key always go to the **same partition** (order preserved for that key).\n",
    "- **Example:**  \n",
    "  - Key = `user_42` → always lands in Partition 3.  \n",
    "  - Ensures all actions from `user_42` stay ordered.\n",
    "\n",
    "---\n",
    "\n",
    "### 👥 Consumer Groups\n",
    "- Consumers are organized into **groups**.\n",
    "- Each partition is read by only **one consumer in a group**.\n",
    "- Multiple consumers in a group → parallel reading.\n",
    "- Different groups get independent copies of the data.\n",
    "- **Example:**  \n",
    "  - `recs-service` group processes real-time recommendations.  \n",
    "  - `analytics` group writes data to S3 for offline analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### 📍 Offsets\n",
    "- Each message in a partition has an **offset** — a unique position number.\n",
    "- Consumers keep track of the last offset they processed.\n",
    "- Allows **restarts** and **replays** by resetting the offset.\n",
    "\n",
    "---\n",
    "\n",
    "### ♻ Retention\n",
    "- Kafka keeps data for a **configured time** (default: 7 days).\n",
    "- Messages are not removed immediately after being read.\n",
    "- Allows consumers to **reprocess past events**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 Replication\n",
    "- Each partition has a **leader** replica and **follower** replicas on different brokers.\n",
    "- If a broker with the leader fails, a follower takes over.\n",
    "- Provides **high availability**.\n",
    "\n",
    "---\n",
    "\n",
    "## 5️⃣ How It All Works Together\n",
    "1. **Producer** sends a message with a key.\n",
    "2. Kafka hashes the key → decides partition.\n",
    "3. The message is appended to the partition log.\n",
    "4. The **leader broker** for that partition stores the message and replicates it to followers.\n",
    "5. **Consumers** in a group read from different partitions in parallel.\n",
    "6. Offsets track read positions; retention ensures replayability.\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 More Reference\n",
    "🎥 [YouTube: Apache Kafka Explained](https://www.youtube.com/watch?v=QkdkLdMBuL0&t=56s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24a20a1",
   "metadata": {},
   "source": [
    "# Amazon Kinesis Data Streams\n",
    "\n",
    "Amazon Kinesis Data Streams (KDS) is a **real-time event streaming service** that lets you collect, process, and analyze streaming data at scale.  \n",
    "It works similarly to Apache Kafka but is fully managed by AWS.\n",
    "\n",
    "---\n",
    "\n",
    "## 1️⃣ Overview of Kinesis Data Streams\n",
    "\n",
    "![Kinesis Overview](./images/kinesis.png)\n",
    "\n",
    "- **Event Producers** push messages into Kinesis streams.\n",
    "- **Kinesis Streams** are divided into **shards** (units of capacity).\n",
    "- **Event Consumers** pull messages from the streams for processing.\n",
    "- Each shard has defined **read/write capacity limits**.\n",
    "\n",
    "**Write Operation Capacity per Shard:**\n",
    "- Up to **1,000 records/sec** or **1 MB/sec** (whichever limit is reached first).\n",
    "\n",
    "**Read Operation Capacity per Shard:**\n",
    "- Up to **5 read operations/sec**\n",
    "- Maximum total read rate: **2 MB/sec**\n",
    "\n",
    "---\n",
    "\n",
    "## 2️⃣ On-Demand vs Provisioned Mode\n",
    "\n",
    "![On-Demand vs Provisioned](./images/kinesis_on_demand_provisioned.png)\n",
    "\n",
    "### **On-Demand Mode**\n",
    "- AWS automatically scales shards **up/down** as needed.\n",
    "- You only pay for what you use.\n",
    "- Ideal for unpredictable or highly variable workloads.\n",
    "\n",
    "### **Provisioned Mode**\n",
    "- You set the number of shards manually based on expected read/write traffic.\n",
    "- You must **reshard** (add/remove shards) yourself when capacity changes.\n",
    "- Better for predictable workloads and tighter cost control.\n",
    "\n",
    "---\n",
    "\n",
    "## 3️⃣ Data Record Structure\n",
    "\n",
    "![Data Record & Shared Fan-Out](./images/shared_fan_out.png)\n",
    "\n",
    "A **data record** in Kinesis has three components:\n",
    "1. **Partition Key** — chosen by the producer; determines which shard the record goes to.\n",
    "2. **Sequence Number** — assigned by Kinesis to preserve order **within a shard**.\n",
    "3. **Data Blob** — the actual payload (binary, JSON, text, etc.).\n",
    "\n",
    "**Partition Key Function:**\n",
    "- Acts like an “address” to group related records.\n",
    "- Example: If you choose `customer_id` as the partition key, all transactions for a customer will land in the **same shard**.\n",
    "\n",
    "---\n",
    "\n",
    "## 4️⃣ Shards — The Core Unit of Capacity\n",
    "\n",
    "![Enhanced Fan-Out](./images/enhanced.png)\n",
    "\n",
    "- **Shards** are ordered sequences of data records.\n",
    "- Ordering is guaranteed **within a shard**.\n",
    "- More shards → higher parallelism and throughput.\n",
    "- **Write limits per shard:** 1 MB/sec or 1,000 records/sec.\n",
    "- **Read limits per shard:** 2 MB/sec, up to 5 reads/sec.\n",
    "\n",
    "**Fan-Out Types:**\n",
    "- **Shared Fan-Out**: Consumers share the shard’s read capacity.\n",
    "- **Enhanced Fan-Out**: Each consumer gets **dedicated 2 MB/sec** read throughput.\n",
    "\n",
    "---\n",
    "\n",
    "## 5️⃣ Processing Data from Kinesis\n",
    "\n",
    "![Kinesis Tools](./images/tools_.png)\n",
    "\n",
    "You can process Kinesis data using:\n",
    "- **AWS Lambda** — event-driven processing.\n",
    "- **Amazon Managed Service for Apache Flink** — advanced stream processing.\n",
    "- **AWS Glue** — ETL jobs.\n",
    "- **Amazon Kinesis Client Library (KCL)** — custom consumer applications.\n",
    "- **Amazon Data Firehose** — send data directly to storage like S3.\n",
    "\n",
    "---\n",
    "\n",
    "## 6️⃣ How It Works Together\n",
    "1. **Producers** send records with a chosen partition key.\n",
    "2. Kinesis hashes the key → assigns record to a shard.\n",
    "3. The shard appends the record and assigns a **sequence number**.\n",
    "4. **Consumers** read records from shards in order.\n",
    "5. Data can be processed, stored, or sent to downstream systems.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Key Takeaways:**\n",
    "- **Partition Key** controls ordering and grouping.\n",
    "- **Shards** are the main scaling unit.\n",
    "- **On-Demand** mode is easier for unpredictable workloads.\n",
    "- **Provisioned** mode is better for cost control and predictable patterns.\n",
    "- Use **Enhanced Fan-Out** if multiple consumers need full read speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852cf4da",
   "metadata": {},
   "source": [
    "# Change Data Capture (CDC)\n",
    "---\n",
    "\n",
    "# 🔗 Reference  \n",
    "[What is Change Data Capture (CDC)? – Coursera](https://www.coursera.org/learn/source-systems-data-ingestion-and-pipelines/supplement/w4wRP/what-is-change-data-capture-cdc)\n",
    "\n",
    "---\n",
    "\n",
    "## **1. What is CDC?**\n",
    "Change Data Capture (CDC) is a technique for **identifying and capturing only the changes** (inserts, updates, deletes) made in a source database and delivering them to downstream systems such as a data warehouse or data lake.\n",
    "\n",
    "> According to *Fundamentals of Data Engineering*:  \n",
    "> “Change data capture (CDC) is a method for extracting each change event (insert, update, delete) that occurs in a database and making it available for downstream systems.”\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Why CDC?**\n",
    "When we have both:\n",
    "- **Production Database (OLTP)** → Runs the day-to-day application.\n",
    "- **Data Warehouse (OLAP)** → Used for analytics, reporting, machine learning.\n",
    "\n",
    "We need the warehouse to stay **in sync** with the production database without:\n",
    "- Replacing the entire dataset (full snapshot reloads).\n",
    "- Writing directly to both systems from the application (dual-write).\n",
    "\n",
    "### **Example: E-commerce Address Update**\n",
    "1. A user updates their shipping address in the app.\n",
    "2. Production DB is updated immediately.\n",
    "3. CDC detects this change from the DB’s **transaction log** (binlog/WAL).\n",
    "4. CDC sends the change event to a **streaming platform** like **Amazon Kinesis** or Kafka.\n",
    "5. A warehouse consumer reads the event asynchronously and **upserts** the new address into the warehouse.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Why not Dual-Writes?**\n",
    "A “dual-write” means your application writes to:\n",
    "1. Production DB  \n",
    "2. Data Warehouse (or queue feeding it)  \n",
    "…in the same request.\n",
    "\n",
    "**Problems with dual-writes:**\n",
    "- **Atomicity risk**: If one write succeeds and the other fails (network crash, process crash), systems drift out of sync.\n",
    "- **Performance impact**: If the warehouse is slow or unavailable, the application will slow down or fail.\n",
    "- **Complexity**: Requires retry logic, error handling, and schema transformation logic in the application.\n",
    "- **Tight coupling**: Changes in the warehouse schema or availability can impact the production app.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Why CDC is the Better Approach**\n",
    "CDC avoids these problems by:\n",
    "- **Single Source of Truth**: App writes only to production DB.\n",
    "- **Asynchronous**: Changes flow to the warehouse without slowing down the app.\n",
    "- **Reliable**: Log-based CDC reads committed changes directly from the DB log, ensuring no missed updates.\n",
    "- **Replayable**: If the target system is down, CDC can replay from the last processed log position.\n",
    "- **Ordered per key**: Maintains correct sequence of changes for each record.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. How Log-Based CDC Works with a Streaming Platform**\n",
    "1. **Transaction in Prod DB**: User updates data → change is written to transaction log.\n",
    "2. **CDC Connector**: Tails the log, extracts change events.\n",
    "3. **Publish to Stream**: Events are sent to Amazon Kinesis (or Kafka).\n",
    "4. **Consumers**: Warehouse consumers read events in real-time or micro-batches.\n",
    "5. **Upsert to Warehouse**: Apply changes using `MERGE`/`UPSERT` statements.\n",
    "\n",
    "**Flow Diagram:**\n",
    "[App] → [Prod DB] → [CDC Connector] → [Kinesis Stream] → [Warehouse Consumer] → [Warehouse] \n",
    "\n",
    "---\n",
    "\n",
    "## **6. Streaming vs Batch Updates in the Warehouse**\n",
    "- **Streaming**: Apply each change immediately — great for near real-time analytics.\n",
    "- **Micro-batch**: Process events every few minutes/hours to reduce load and cost.\n",
    "- **Large batch**: Apply once a day — useful when real-time freshness isn’t needed.\n",
    "\n",
    "**Best practice**:  \n",
    "Capture CDC in real-time but write to the warehouse in micro-batches for efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Summary\n",
    "CDC enables **fast, reliable, and decoupled** data synchronization between operational and analytical systems.  \n",
    "It eliminates the risks of dual-writes, avoids full table reloads, and supports near real-time analytics — making it a foundational pattern in modern data engineering pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b9d82d",
   "metadata": {},
   "source": [
    "#  General Considerations for Choosing Ingestion Tools\n",
    "---\n",
    "# 🔗 Reference  \n",
    "[Summary: General Considerations for Choosing Ingestion Tools – Coursera](https://www.coursera.org/learn/source-systems-data-ingestion-and-pipelines/supplement/PlabN/summary-general-considerations-for-choosing-ingestion-tools)\n",
    "\n",
    "---\n",
    "When selecting an ingestion tool, you need to consider:\n",
    "1. **Characteristics of the Data** (data payload).\n",
    "2. **Reliability & Durability** of the ingestion process.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Characteristics of the Data**\n",
    "\n",
    "> In *Fundamentals of Data Engineering*, Joe and Matt refer to data characteristics as the **data payload**, which includes:\n",
    "> - Data kind (type & format)\n",
    "> - Shape\n",
    "> - Size\n",
    "> - Schema & data types\n",
    "> - Metadata\n",
    "\n",
    "---\n",
    "\n",
    "### **a) Data Type & Structure**\n",
    "Data can be **structured**, **unstructured**, or **semi-structured**.  \n",
    "Understanding type & format helps pick the right tool and plan transformations.\n",
    "\n",
    "**Example:**\n",
    "- **Structured**: A CSV file with sales records (use batch ingestion with ETL tools like AWS Glue).\n",
    "- **Unstructured**: PNG product images (use object storage like Amazon S3).\n",
    "- **Semi-structured**: JSON order events (use streaming tools like Kafka or Kinesis).\n",
    "\n",
    "---\n",
    "\n",
    "### **b) Data Volume**\n",
    "Two aspects to consider:\n",
    "\n",
    "1. **Existing Data Size**  \n",
    "   - **Batch ingestion**: Can you transfer the entire historical dataset in one go?  \n",
    "     Example: Migrating a 500 GB database over limited bandwidth may require chunking into smaller batches.  \n",
    "   - **Streaming ingestion**: Check max message size.  \n",
    "     Example: Amazon Kinesis max = **1 MB** per record; Kafka default = **1 MB**, configurable up to **20 MB**.\n",
    "\n",
    "2. **Future Data Growth**  \n",
    "   - Estimate daily, monthly, yearly growth.\n",
    "   - Helps configure scaling & anticipate storage costs.  \n",
    "     Example: IoT sensors generating 5 GB/day today, expected to grow to 50 GB/day in 2 years.\n",
    "\n",
    "---\n",
    "\n",
    "### **c) Latency Requirements**\n",
    "- **Batch**: Data ingested periodically (daily, weekly, monthly).  \n",
    "- **Streaming**: Data ingested continuously for near real-time insights.\n",
    "\n",
    "**Example:**\n",
    "- Daily sales report → Batch ingestion every night.\n",
    "- Fraud detection → Streaming ingestion with millisecond delay.\n",
    "\n",
    "Consider:\n",
    "- How quickly stakeholders need the data.\n",
    "- How quickly source data is generated.\n",
    "\n",
    "---\n",
    "\n",
    "### **d) Data Quality**\n",
    "Assess whether source data is ready for use or requires cleaning.\n",
    "\n",
    "**Example:**\n",
    "- If customer table has missing emails or inconsistent phone numbers, ingestion might include validation & cleansing steps.\n",
    "- Tools like AWS Glue DataBrew can detect/fix inconsistencies during ingestion.\n",
    "\n",
    "---\n",
    "\n",
    "### **e) Changes in Schema**\n",
    "Schema changes are common in source systems:\n",
    "- Adding/removing columns\n",
    "- Renaming columns\n",
    "- Changing data types\n",
    "\n",
    "**Example:**\n",
    "- A \"customer_phone\" column changes from `INT` to `VARCHAR` — ingestion tools must adapt without breaking pipelines.\n",
    "\n",
    "If frequent:\n",
    "- Choose ingestion tools that detect schema changes automatically (e.g., Debezium, Fivetran).\n",
    "- Maintain communication with upstream teams.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Reliability & Durability**\n",
    "\n",
    "- **Reliability**: Ingestion tool consistently performs as intended.\n",
    "- **Durability**: Data is not lost or corrupted.\n",
    "\n",
    "**Example:**\n",
    "- Streaming from IoT devices: If ingestion fails and devices don’t retain events, data is lost forever.\n",
    "- Kafka replication ensures data durability even if one broker fails.\n",
    "\n",
    "---\n",
    "\n",
    "### **Design Advice**\n",
    "- Understand source systems & ingestion tool limits.\n",
    "- Decide tradeoffs: cost of losing data vs. cost of redundancy.\n",
    "\n",
    "**Example:**\n",
    "- For low-value data (test logs), accept occasional loss.\n",
    "- For critical data (financial transactions), build high redundancy:\n",
    "  - Replication\n",
    "  - Failover clusters\n",
    "  - Persistent storage\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Summary**:  \n",
    "Choosing the right ingestion tool means aligning **data characteristics** with the **tool’s capabilities**, while ensuring reliability and durability to prevent data loss and meet performance needs."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
