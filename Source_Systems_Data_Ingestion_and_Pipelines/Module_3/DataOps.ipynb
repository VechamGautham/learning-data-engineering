{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fc57958",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [Introduction to Dataops](#-introduction-to-dataops)\n",
    "- [DataOps Automation](#dataops-automation)\n",
    "- [IAC](#-infrastructure-as-code-iac)\n",
    "- [terraform-setting-up-ec2-instance](#terraform-setting-up-ec2-instance)\n",
    "- [Terraform variables and outputs](#terraform---variables-and-outputs)\n",
    "- [Terraform data sources and modules](#terraform--data-sources--modules)\n",
    "- [Data observability and monitoring](#data-observability--monitoring--why-its-needed)\n",
    "- [AWS Cloudwatch](#aws-cloudwatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6131db6c",
   "metadata": {},
   "source": [
    "# Introduction to DataOps\n",
    "\n",
    "**DataOps** is a collaborative data management practice focused on improving the communication, integration, and automation of data flows between data managers and data consumers across an organization.  \n",
    "It aims to deliver **faster, more reliable, and higher-quality** data analytics through the adoption of agile development, DevOps practices, and lean manufacturing principles.\n",
    "\n",
    "---\n",
    "\n",
    "## üèõ The Three Pillars of DataOps\n",
    "\n",
    "1. **Automation**  \n",
    "   - Streamlines repetitive processes using tools and scripts.\n",
    "   - Enables rapid, reliable, and scalable data pipeline deployments.\n",
    "   \n",
    "2. **Observability & Monitoring**  \n",
    "   - Ensures full visibility into the performance, health, and data quality across pipelines.\n",
    "   - Uses monitoring tools and alerts to proactively detect issues.\n",
    "   \n",
    "3. **Incident Response**  \n",
    "   - Defines structured processes for identifying, triaging, and resolving data issues quickly.\n",
    "   - Reduces downtime and minimizes business impact.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Pillars Diagram\n",
    "\n",
    "![DataOps Pillars](./images/dataops_pillers.png)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Automation in DataOps\n",
    "\n",
    "Automation is a core pillar of DataOps, enabling teams to reduce manual intervention, minimize human errors, and accelerate delivery.\n",
    "\n",
    "### **Key Automation Practices**\n",
    "1. **Continuous Integration and Continuous Delivery (CI/CD)**  \n",
    "   - Automates the process of building, testing, integrating, and deploying data pipelines.\n",
    "   \n",
    "2. **Infrastructure as Code (IaC)**  \n",
    "   - Uses code to define, provision, and manage infrastructure resources.\n",
    "   - Ensures reproducibility and scalability.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ† Tools for Automation\n",
    "\n",
    "Some popular tools used for automation in DataOps include:\n",
    "\n",
    "- **Terraform** ‚Äì For provisioning and managing infrastructure as code.  \n",
    "- **Ansible** ‚Äì For configuration management and deployment automation.  \n",
    "- **Jenkins / GitHub Actions / GitLab CI** ‚Äì For CI/CD pipeline automation.  \n",
    "- **Apache Airflow / Prefect** ‚Äì For orchestrating data workflows.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Automation Diagram\n",
    "\n",
    "![Automation](./images/automation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eca541",
   "metadata": {},
   "source": [
    "# DataOps Automation\n",
    "\n",
    "## üìå What is DataOps Automation?\n",
    "**DataOps Automation** is the practice of streamlining and automating every stage of a **data pipeline** ‚Äî from ingestion to transformation to delivery ‚Äî in order to:\n",
    "- Reduce manual intervention.\n",
    "- Improve reliability and consistency.\n",
    "- Enable faster and safer deployments.\n",
    "- Integrate best practices from **DevOps** into data engineering workflows.\n",
    "\n",
    "It borrows concepts from **DevOps automation** like:\n",
    "- **CI/CD (Continuous Integration / Continuous Delivery)**\n",
    "- **Version Control**\n",
    "- **Infrastructure as Code (IaC)**\n",
    "- **Orchestration with DAGs** (Directed Acyclic Graphs)\n",
    "\n",
    "---\n",
    "\n",
    "## üõ† Levels of DataOps Automation\n",
    "\n",
    "### 1Ô∏è‚É£ No Automation\n",
    "- All processes are run **manually** by engineers.\n",
    "- Time-consuming, prone to human error, and difficult to scale.\n",
    "\n",
    "![Manual Automation](./images/manul.png)\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ Pure Scheduling (Semi-Automation)\n",
    "- Each stage of the pipeline runs on a **fixed schedule**.\n",
    "- Improves consistency, but lacks dynamic triggers and dependency management.\n",
    "\n",
    "![Semi Automation](./images/semi_automatio.png)\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ Fully Automated with Orchestration (e.g., Apache Airflow)\n",
    "- Pipelines are defined as a **Directed Acyclic Graph (DAG)**.\n",
    "- Orchestration tools like **Apache Airflow** ensure tasks run in the right order, only when dependencies are met.\n",
    "- Enables retries, error handling, and monitoring.\n",
    "\n",
    "![Fully Automated](./images/fully_autmated_via_apache_airflow.png)\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ CI/CD in DataOps\n",
    "**Continuous Integration / Continuous Delivery** automates:\n",
    "1. **Build** ‚Äì Prepare code and configurations.\n",
    "2. **Test** ‚Äì Automatic review and testing of new code or data transformations.\n",
    "3. **Integrate** ‚Äì Merge tested changes into the main pipeline.\n",
    "4. **Deploy** ‚Äì Automatic delivery into production.\n",
    "\n",
    "This approach ensures rapid, reliable updates to both **code and data**.\n",
    "\n",
    "![CI/CD](./images/ci-cd.png)\n",
    "\n",
    "---\n",
    "\n",
    "##  Innfrastructure as Code (IaC)\n",
    "- Maintain infrastructure configurations as **code**.\n",
    "- Example: Provisioning cloud storage, compute resources, and databases through code files.\n",
    "- Benefits:\n",
    "  - Version control for infrastructure.\n",
    "  - Reproducibility.\n",
    "  - Easy rollback to previous setups.\n",
    "\n",
    "![Infrastructure as Code](./images/iac.png)\n",
    "\n",
    "---\n",
    "\n",
    "## üìÇ Version Control for Code & Data\n",
    "- Tracks **changes** in both:\n",
    "  - **Pipeline code** (SQL, Python, configs).\n",
    "  - **Data versions** moving through the pipeline.\n",
    "- Enables rollback to **previous versions** in case of errors.\n",
    "\n",
    "![Version Control](./images/version_control.png)\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Why DataOps Automation Matters\n",
    "- **Consistency** ‚Äì Fewer errors and more predictable results.\n",
    "- **Speed** ‚Äì Faster deployments and updates.\n",
    "- **Scalability** ‚Äì Handle large, complex pipelines without bottlenecks.\n",
    "- **Resilience** ‚Äì Automatic error handling, monitoring, and quick rollbacks.\n",
    "\n",
    "In short, DataOps Automation ensures that **data pipelines run like a well-oiled factory line** ‚Äî continuously delivering trusted data products at high speed and with minimal manual effort."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8c117e",
   "metadata": {},
   "source": [
    "# Infrastructure as Code (IaC)\n",
    "\n",
    "## üìå What is Infrastructure as Code?\n",
    "**Infrastructure as Code (IaC)** is the practice of defining, deploying, and maintaining infrastructure using **code**, rather than manual processes.  \n",
    "With IaC, you can automate the creation of **networking, security, computing, storage**, and other resources required for your cloud-based data pipelines.\n",
    "\n",
    "Benefits include:\n",
    "- **Automation** ‚Äì Reduce manual effort and human error.\n",
    "- **Scalability** ‚Äì Deploy infrastructure for large, complex systems quickly.\n",
    "- **Consistency** ‚Äì Ensure all environments match the desired configuration.\n",
    "- **Version Control** ‚Äì Track changes and roll back when necessary.\n",
    "\n",
    "---\n",
    "\n",
    "## üï∞ History of IaC\n",
    "\n",
    "![History of IaC](./images/iac_history.png)\n",
    "\n",
    "1. **1970s ‚Äì Configuration Management**\n",
    "   - Engineers used scripts (like early **BASH**) to automate configuration of physical machines.\n",
    "   - Primitive automation for repetitive setup tasks.\n",
    "\n",
    "2. **2006 ‚Äì AWS EC2 Launch**\n",
    "   - Cloud computing became widely accessible.\n",
    "   - Developers could **spin up virtual servers on demand**.\n",
    "\n",
    "3. **2010s ‚Äì Modern IaC Tools**\n",
    "   - Tools like **Terraform**, **AWS CloudFormation**, and **Ansible** emerged.\n",
    "   - Enabled full infrastructure provisioning via code files instead of manual configuration.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è How Terraform Works\n",
    "\n",
    "Terraform is a **cloud-agnostic** IaC tool created by **HashiCorp**.  \n",
    "It uses **HCL (HashiCorp Configuration Language)**, a **declarative** language, to define the desired end state of infrastructure.\n",
    "\n",
    "### Example: S3 Bucket Setup in Terraform\n",
    "\n",
    "![Terraform S3 Config](./images/terraform_s3_config.png)\n",
    "\n",
    "**Key points in Terraform syntax:**\n",
    "- `resource` ‚Üí Keyword indicating the type of entity you want to create.\n",
    "- `\"aws_s3_bucket\"` ‚Üí **Resource type** (provider + service).\n",
    "- `\"data_lake\"` ‚Üí **Resource name** (your internal reference).\n",
    "- `{ ... }` ‚Üí **Configuration block** with key-value pairs.\n",
    "\n",
    "---\n",
    "\n",
    "## üñã Example: Creating a VPC and EC2 Instance\n",
    "\n",
    "![VPC & Instance](./images/vpc_instance_iac.png)\n",
    "```hcl\n",
    "#VPC Creation\n",
    "resource \"aws_vpc\" \"main\" {\n",
    "  cidr_block       = \"10.0.0.0/16\"\n",
    "  instance_tenancy = \"default\"\n",
    "\n",
    "  tags = {\n",
    "    Name = \"main\"\n",
    "  }\n",
    "}\n",
    "\n",
    "#EC2 Instance Creation\n",
    "resource \"aws_instance\" \"web\" {\n",
    "  ami           = data.aws_ami.ubuntu.id\n",
    "  instance_type = \"t3.micro\"\n",
    "\n",
    "  tags = {\n",
    "    Name = \"HelloWorld\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "---\n",
    "\n",
    "## üîÑ Terraform Idempotency\n",
    "\n",
    "![Idempotency](./images/idempotent.png)\n",
    "\n",
    "Terraform ensures **idempotency** ‚Äî running the same configuration multiple times **won‚Äôt recreate resources unnecessarily**:\n",
    "- If the resource **does not exist**, Terraform creates it.\n",
    "- If it **exists but differs** from the desired state, Terraform updates it.\n",
    "- If it **matches exactly**, Terraform does nothing.\n",
    "\n",
    "---\n",
    "\n",
    "## üÜö Bash vs Terraform\n",
    "- **Bash scripts** are **imperative** ‚Üí they execute commands in a specific order without checking the existing state.\n",
    "- Running the same Bash provisioning script twice will create **duplicate resources**.\n",
    "- **Terraform** is **declarative** ‚Üí it checks the current state and only makes necessary changes.\n",
    "- This makes Terraform **safe, repeatable, and idempotent**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842647a6",
   "metadata": {},
   "source": [
    "# Terraform setting up EC2 Instance\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ What We‚Äôll Build (Big Picture)\n",
    "\n",
    "![Region + Default VPC](./images/setting_up_ec2.png)\n",
    "\n",
    "We‚Äôll create a tiny EC2 instance in **`us-east-1`** (West Virginia) inside the **default VPC**. This is great for learning; in production you‚Äôd use a custom VPC.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Providers & Plugins ‚Äî What They Are (and Why You Need Them)\n",
    "\n",
    "![Provider anatomy](./images/provider.png)\n",
    "\n",
    "**Terraform Core** understands `.tf` files and figures out a plan, but it **doesn‚Äôt know vendor APIs** by itself.  \n",
    "That‚Äôs the job of a **provider plugin**:\n",
    "\n",
    "- A **provider** (in Terraform terms) is a **binary plugin** that knows how to talk to an external system‚Äôs API (AWS, GCP, GitHub, Datadog, etc.).\n",
    "- The provider implements CRUD operations for **resources** (e.g., `aws_instance`, `aws_s3_bucket`).\n",
    "- When you run `terraform init`, Terraform **downloads the provider plugin** you declared in your config (from the Terraform Registry) and stores it under `.terraform/`.\n",
    "\n",
    "**Two steps you always do:**\n",
    "1. **Declare** that you need a provider (so Terraform downloads the plugin).\n",
    "2. **Configure** the provider (region, credentials/profile, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "## üß∞ Install & Prepare Terraform\n",
    "\n",
    "![Install Terraform](./images/terraform_intall.png)\n",
    "\n",
    "1. **Install Terraform CLI**\n",
    "   - macOS (Homebrew): `brew tap hashicorp/tap && brew install hashicorp/tap/terraform`\n",
    "   - Windows: Install from the official MSI or use `choco install terraform`.\n",
    "   - Linux: Use your distro‚Äôs package manager or download the binary from HashiCorp.\n",
    "\n",
    "2. **Set up AWS credentials** (pick one)\n",
    "   - **AWS CLI profile**:  \n",
    "     `aws configure --profile myprofile`\n",
    "   - **Environment variables**:  \n",
    "     `export AWS_ACCESS_KEY_ID=...`  \n",
    "     `export AWS_SECRET_ACCESS_KEY=...`  \n",
    "     (and optionally `AWS_SESSION_TOKEN` if you use SSO/STS)\n",
    "   - **EC2 role** (if running Terraform **on** an EC2 instance)\n",
    "\n",
    "3. **Create a new project folder**\n",
    "   ```\n",
    "   mkdir tf-ec2-lab && cd tf-ec2-lab\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Terraform Workflow You‚Äôll Use\n",
    "\n",
    "![Terraform Steps](./images/terraform_steps.png)\n",
    "\n",
    "1. **Write** `.tf` files  \n",
    "2. **Init** ‚Üí downloads provider plugin(s)  \n",
    "3. **Plan** ‚Üí shows the execution plan  \n",
    "4. **Apply** ‚Üí creates/updates/destroys resources\n",
    "\n",
    "---\n",
    "\n",
    "## üß± Code Anatomy: Blocks, Labels, Arguments\n",
    "\n",
    "![Code blocks in editor](./images/terraform_code.png)\n",
    "![Labels & types](./images/resourse__.png)\n",
    "\n",
    "- **Block**: `keyword \"label1\" \"label2\" { ... }`\n",
    "- **Examples**: `terraform {}`, `provider \"aws\" {}`, `resource \"aws_instance\" \"web\" {}`  \n",
    "- **Arguments** inside `{}` are key‚Äìvalue pairs or nested blocks.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) **Declare** the AWS Provider (so Terraform can download the plugin)\n",
    "\n",
    "This lives in `main.tf`. It tells Terraform **which provider plugin** to get and which Terraform CLI versions are allowed.\n",
    "\n",
    "```hcl\n",
    "terraform {\n",
    "  required_version = \">= 1.5.0\"\n",
    "\n",
    "  required_providers {\n",
    "    aws = {\n",
    "      source  = \"hashicorp/aws\"   # Download the AWS provider plugin from the Registry\n",
    "      version = \"~> 5.0\"          # Stay within major version 5 for compatibility\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**What each part does**\n",
    "- `required_providers.aws.source` ‚Üí Registry address for the **plugin**.\n",
    "- `required_providers.aws.version` ‚Üí Version range for stability.\n",
    "- `required_version` ‚Üí Guardrail for your local Terraform CLI version.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) **Configure** the AWS Provider (region & auth settings)\n",
    "\n",
    "This block configures the **runtime** for the provider plugin (e.g., which **region** to call).\n",
    "\n",
    "```hcl\n",
    "provider \"aws\" {\n",
    "  region  = \"us-east-1\"   # Target AWS region for API calls\n",
    "  # profile = \"myprofile\" # Optional: use a specific AWS CLI profile\n",
    "}\n",
    "```\n",
    "\n",
    "- If you omit `profile`, Terraform uses the default credential chain:\n",
    "  environment variables ‚Üí shared credentials files ‚Üí SSO/role ‚Üí EC2 role, etc.\n",
    "- You can keep `region` as a variable if you prefer (shown below).\n",
    "\n",
    "![Provider block callouts](./images/terraform_provider.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 3) **Create** an EC2 Instance (Simple Version)\n",
    "\n",
    "![Resource type](./images/resourse.png)\n",
    "\n",
    "At minimum you need **AMI** and **instance_type**.\n",
    "\n",
    "```hcl\n",
    "resource \"aws_instance\" \"webserver\" {\n",
    "  ami           = \"ami-0453ec754f44f9a4a\"  # Replace with a valid AMI ID in your region\n",
    "  instance_type = \"t2.micro\"\n",
    "\n",
    "  tags = {\n",
    "    Name = \"ExampleServer\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "- `resource \"aws_instance\" \"webserver\"`: resource **type** and local **name**.\n",
    "- `ami`: Operating system image.\n",
    "- `instance_type`: Size (CPU/RAM).\n",
    "- `tags`: helpful labels.\n",
    "\n",
    "---\n",
    "\n",
    "## 3b) **(Safer)** Lookup the Latest Amazon Linux 2 AMI Dynamically\n",
    "\n",
    "Avoids hard‚Äëcoded AMI IDs.\n",
    "\n",
    "```hcl\n",
    "data \"aws_ami\" \"al2\" {\n",
    "  most_recent = true\n",
    "\n",
    "  filter {\n",
    "    name   = \"name\"\n",
    "    values = [\"amzn2-ami-hvm-*-x86_64-gp2\"]\n",
    "  }\n",
    "\n",
    "  owners = [\"137112412989\"] # Amazon\n",
    "}\n",
    "\n",
    "resource \"aws_instance\" \"webserver\" {\n",
    "  ami           = data.aws_ami.al2.id\n",
    "  instance_type = \"t2.micro\"\n",
    "\n",
    "  tags = {\n",
    "    Name = \"ExampleServer\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Why this is better**\n",
    "- The **data source** only **reads** info (does not create resources).\n",
    "- You always get a recent, supported AMI in your region.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## üß™ Run It\n",
    "\n",
    "1) Initialize (downloads the **AWS provider plugin**)\n",
    "```bash\n",
    "terraform init\n",
    "```\n",
    "\n",
    "2) Preview what will happen\n",
    "```bash\n",
    "terraform plan\n",
    "```\n",
    "\n",
    "3) Create resources (you‚Äôll be prompted to type `yes`)\n",
    "```bash\n",
    "terraform apply\n",
    "```\n",
    "\n",
    "4) Clean up later\n",
    "```bash\n",
    "terraform destroy\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Extra Notes & Troubleshooting\n",
    "\n",
    "- **Credentials**: If `plan/apply` fails with ‚Äúno valid credentials,‚Äù set env vars or specify `profile` in the provider block.\n",
    "- **Region mismatch**: AMI IDs are region‚Äëspecific. Use the **data source** approach to avoid mismatches.\n",
    "- **Idempotency**: Terraform is **declarative** and **idempotent**‚Äîre‚Äërunning `apply` won‚Äôt duplicate resources if the real world already matches your config.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8de1ba",
   "metadata": {},
   "source": [
    "# Terraform - Variables and Outputs \n",
    "\n",
    "![Hard-coded values example](./images/hard_code.png)\n",
    "\n",
    "In the earlier config, values like **AWS region** and **EC2 instance name** were **hard-coded**. Replacing them with **variables** makes the code reusable, and exposing key attributes as **outputs** lets you print or pass data to other stacks.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Declare input variables\n",
    "\n",
    "Create `variables.tf` and describe the inputs your config needs.\n",
    "\n",
    "```hcl\n",
    "variable \"region\" {\n",
    "  description = \"AWS region to deploy resources\"\n",
    "  type        = string\n",
    "  default     = \"us-east-1\"\n",
    "}\n",
    "\n",
    "variable \"serverName\" {\n",
    "  description = \"Name tag for the EC2 instance\"\n",
    "  type        = string\n",
    "}\n",
    "```\n",
    "\n",
    "**What this does**\n",
    "\n",
    "- `variable \"<name>\"` defines an input.\n",
    "- `description` documents it.\n",
    "- `type` constrains values.\n",
    "- `default` (optional). If omitted, Terraform will prompt (ask) for a value.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Use variables in your config\n",
    "\n",
    "Wire those inputs into your provider and resources by referencing `var.<name>`.\n",
    "\n",
    "```hcl\n",
    "# providers.tf\n",
    "provider \"aws\" {\n",
    "  region = var.region\n",
    "}\n",
    "\n",
    "# main.tf\n",
    "resource \"aws_instance\" \"webserver\" {\n",
    "  ami           = \"ami-0453ec754f44f9a4a\"\n",
    "  instance_type = \"t2.micro\"\n",
    "\n",
    "  tags = {\n",
    "    Name = var.serverName\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**What this does**\n",
    "\n",
    "- `var.region` replaces the hard-coded region.\n",
    "- `var.serverName` sets the `Name` tag dynamically.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Set variable values\n",
    "\n",
    "You can pass values at apply time or via a `.tfvars` file.\n",
    "\n",
    "```bash\n",
    "# Option A: CLI flag\n",
    "terraform apply -var=\"serverName=ExampleServer\"\n",
    "\n",
    "# Option B: terraform.tfvars (auto-loaded)\n",
    "```\n",
    "\n",
    "```hcl\n",
    "# terraform.tfvars\n",
    "serverName = \"ExampleServer\"\n",
    "```\n",
    "\n",
    "**What this does**\n",
    "\n",
    "- CLI `-var` sets a one-off value.\n",
    "- `terraform.tfvars` (or any `*.tfvars`) is auto-read, great for teams.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Declare outputs\n",
    "\n",
    "Expose useful attributes (like the instance **ID** and **ARN**) so you can print or reuse them.\n",
    "\n",
    "```hcl\n",
    "# outputs.tf\n",
    "output \"server_id\" {\n",
    "  description = \"The ID of the EC2 instance\"\n",
    "  value       = aws_instance.webserver.id\n",
    "}\n",
    "\n",
    "output \"server_arn\" {\n",
    "  description = \"The ARN of the EC2 instance\"\n",
    "  value       = aws_instance.webserver.arn\n",
    "}\n",
    "```\n",
    "\n",
    "**What this does**\n",
    "\n",
    "- `output \"<name>\"` defines a value to return after `apply`.\n",
    "- `value` reads an attribute from a resource using\n",
    "  `resource_type.resource_name.attribute`.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Apply and read outputs\n",
    "\n",
    "```bash\n",
    "# Create or update infra\n",
    "terraform apply\n",
    "\n",
    "# Show all outputs\n",
    "terraform output\n",
    "\n",
    "# Show a single output\n",
    "terraform output server_id\n",
    "```\n",
    "\n",
    "**What this does**\n",
    "\n",
    "- `apply` evaluates variables, creates/updates resources, then prints outputs.\n",
    "- `terraform output` queries stored output values anytime after an apply.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) File layout (recommended)\n",
    "since terraform assume all tf files as one big folder it is safe to use differnt tf file for usage for better understading \n",
    "Keep things tidy by splitting concerns:\n",
    "\n",
    "```\n",
    ".\n",
    "‚îú‚îÄ main.tf         # resources\n",
    "‚îú‚îÄ providers.tf    # provider + terraform blocks\n",
    "‚îú‚îÄ variables.tf    # inputs\n",
    "‚îú‚îÄ outputs.tf      # outputs\n",
    "‚îî‚îÄ terraform.tfvars# values for inputs (excluded from VCS if sensitive)\n",
    "```\n",
    "\n",
    "This organization scales as your workspace grows. Variables make configs **portable**, and outputs make results **discoverable** across modules/workspaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883a0931",
   "metadata": {},
   "source": [
    "# Terraform ‚Äì Data Sources & Modules \n",
    "\n",
    "## 1) Data Sources\n",
    "\n",
    "**What they are**  \n",
    "A **data source** lets Terraform **read information about an existing resource** (created outside your current config or in another workspace). It‚Äôs **read-only** and evaluated **during plan/apply** so other resources can use the fetched values.\n",
    "\n",
    "**Why use them (vs hardcoding IDs)**  \n",
    "- Avoid stale IDs across dev/stage/prod  \n",
    "- Fewer copy-paste mistakes  \n",
    "- Can **query** and always pick the **latest**/correct match\n",
    "\n",
    "**Pattern**\n",
    "```\n",
    "data \"<provider_type>\" \"<name>\" {\n",
    "  # arguments or filters that identify the existing thing\n",
    "}\n",
    "\n",
    "# later, use it:\n",
    "something = data.<provider_type>.<name>.<attribute>\n",
    "```\n",
    "\n",
    "**Example ‚Äì Get the latest Amazon Linux 2 AMI and use it**\n",
    "```\n",
    "data \"aws_ami\" \"latest_linux\" {\n",
    "  most_recent = true\n",
    "  owners      = [\"amazon\"]\n",
    "\n",
    "  filter {\n",
    "    name   = \"name\"\n",
    "    values = [\"amzn2-ami-hvm-*-x86_64-gp2\"]\n",
    "  }\n",
    "}\n",
    "\n",
    "resource \"aws_instance\" \"my_server\" {\n",
    "  ami           = data.aws_ami.latest_linux.id   # ‚Üê fetched dynamically\n",
    "  instance_type = \"t2.micro\"\n",
    "}\n",
    "```\n",
    "\n",
    "> ‚úÖ Takeaway: **Data sources bring info *into* Terraform** at plan/apply time so your resources can use accurate, up-to-date values.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Modules\n",
    "\n",
    "**What they are**  \n",
    "A **module** is a **reusable package** (a folder) of Terraform files. Think of it like a function:\n",
    "- **Inputs** ‚Üí module variables you pass from the root  \n",
    "- **Outputs** ‚Üí values the module returns back to the root\n",
    "\n",
    "**Typical structure**\n",
    "```\n",
    "project/\n",
    "‚îú‚îÄ main.tf                 # root (calls modules)\n",
    "‚îú‚îÄ outputs.tf              # root outputs\n",
    "‚îú‚îÄ variables.tf            # root vars (optional)\n",
    "‚îî‚îÄ website/                # ‚Üê module folder\n",
    "   ‚îú‚îÄ main.tf              # module resources\n",
    "   ‚îú‚îÄ variables.tf         # module input variables\n",
    "   ‚îî‚îÄ outputs.tf           # module outputs\n",
    "```\n",
    "\n",
    "**Module code (inside `website/`)**\n",
    "\n",
    "`website/variables.tf`\n",
    "```\n",
    "variable \"ami_id\" {}\n",
    "variable \"instance_type\" {}\n",
    "```\n",
    "\n",
    "`website/main.tf`\n",
    "```\n",
    "resource \"aws_instance\" \"web\" {\n",
    "  ami           = var.ami_id\n",
    "  instance_type = var.instance_type\n",
    "}\n",
    "```\n",
    "\n",
    "`website/outputs.tf`\n",
    "```\n",
    "output \"instance_id\" {\n",
    "  value = aws_instance.web.id\n",
    "}\n",
    "```\n",
    "\n",
    "**Root calls the module (`main.tf`)**\n",
    "```\n",
    "module \"website\" {\n",
    "  source        = \"./website\"                   # path to module folder\n",
    "  ami_id        = data.aws_ami.latest_linux.id  # pass data-source value in\n",
    "  instance_type = \"t2.micro\"                    # pass a fixed value in\n",
    "}\n",
    "```\n",
    "\n",
    "**Root exposes module output (`outputs.tf`)**\n",
    "```\n",
    "output \"website_instance_id\" {\n",
    "  value = module.website.instance_id\n",
    "}\n",
    "```\n",
    "\n",
    "**Execution flow (mental model)**\n",
    "```\n",
    "Root main.tf ‚Üí calls module + passes inputs\n",
    "        ‚Üì\n",
    "Module variables.tf ‚Üí receives inputs (var.*)\n",
    "        ‚Üì\n",
    "Module main.tf ‚Üí creates resources using those inputs\n",
    "        ‚Üì\n",
    "Module outputs.tf ‚Üí returns values (output *)\n",
    "        ‚Üì\n",
    "Root outputs.tf ‚Üí prints them after apply\n",
    "```\n",
    "\n",
    "> ‚úÖ Takeaway: **Modules let you reuse and organize code**. Root decides *what to build* and *with which inputs*; the module is the reusable blueprint.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Data Source vs Output ‚Äì Quick Comparison\n",
    "\n",
    "| Concept        | What it does                                                                 | When it runs                     | Direction of data        | Typical usage example                                                 |\n",
    "|----------------|-------------------------------------------------------------------------------|----------------------------------|--------------------------|-----------------------------------------------------------------------|\n",
    "| **Data Source**| **Reads info about an existing resource** (no creation/modification)          | **During plan/apply**            | **Into Terraform**       | `ami = data.aws_ami.latest_linux.id` to get the latest Linux AMI      |\n",
    "| **Output**     | **Exposes info about a created resource** (so humans/other configs can see it)| **After apply finishes**         | **Out of Terraform**     | `output \"instance_id\" { value = aws_instance.web.id }` to print EC2 ID |\n",
    "\n",
    "> Mnemonic: **Data source = input finder**; **Output = result sharer**.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Mini Checklist\n",
    "\n",
    "- Use **data sources** whenever you‚Äôd otherwise paste hardcoded IDs.  \n",
    "- Put reusable resource logic in a **module**; call it from root with inputs.  \n",
    "- Run `terraform init` after adding/modifying modules.  \n",
    "- Use **root outputs** to expose what you‚Äôll need post-apply (IDs, URLs, ARNs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cd32ee",
   "metadata": {},
   "source": [
    "# Data Observability & Monitoring ‚Äì Why It‚Äôs Needed\n",
    "![Data Observability](./images/data_observability.png)\n",
    "\n",
    "Data observability and monitoring are key pillars of **DataOps**, adapted from **DevOps** practices.  \n",
    "They ensure that data systems are **healthy**, **reliable**, and **delivering high-quality data** to stakeholders.\n",
    "\n",
    "## üîç What is Data Observability?\n",
    "Data observability tools help data engineers **gain visibility** into the health of their data systems.  \n",
    "They monitor both:\n",
    "- **System metrics** ‚Äì CPU usage, RAM usage, response times (like in DevOps)\n",
    "- **Data quality metrics** ‚Äì Accuracy, completeness, timeliness, and discoverability\n",
    "\n",
    "**Why it‚Äôs needed:**  \n",
    "Even if your pipelines run without errors, **low-quality data** can still flow downstream, misleading stakeholders and causing costly business decisions.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ High Quality vs ‚ùå Low Quality Data\n",
    "![High vs Low Quality Data](./images/high_low.png)  \n",
    "| High Quality Data | Low Quality Data |\n",
    "|-------------------|------------------|\n",
    "| Accurate | Inaccurate |\n",
    "| Complete | Incomplete |\n",
    "| Discoverable | Hard to find |\n",
    "| Available in a timely manner | Late |\n",
    "\n",
    "High-quality data **matches stakeholder expectations**, with a **well-defined schema** and **clear data definitions**.  \n",
    "Low-quality data, on the other hand, can be **worse than no data at all** because it can drive **wrong business decisions**.\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Example: The Currency Conversion Mishap\n",
    "Imagine a U.S. company selling products in Europe.  \n",
    "The source database changes the sales price from **USD** to **EUR** without notice.  \n",
    "- Pipelines still run  \n",
    "- Dashboards still update  \n",
    "- Numbers look ‚Äúplausible‚Äù  \n",
    "But revenue suddenly appears **10‚Äì20% lower**, triggering panic in leadership.  \n",
    "**Root cause:** The data was no longer in the expected format (accuracy issue).  \n",
    "\n",
    "**Lesson:**  \n",
    "Without observability, this silent failure could go unnoticed until it causes **major business impact**.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Conclusion\n",
    "Data observability ensures that:\n",
    "- You **quickly detect anomalies**\n",
    "- Identify and fix problems early\n",
    "- Prevent downtime in data pipelines\n",
    "- Maintain stakeholder trust by delivering **reliable, high-quality data**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ebd8a5",
   "metadata": {},
   "source": [
    "## Data Quality & Monitoring ‚Äì Project Specific\n",
    "\n",
    "Data quality metrics and monitoring priorities **change depending on the project** and the **stakeholder needs**.  \n",
    "Rather than tracking everything, focus on what matters most to avoid *confusion* and *alert fatigue*.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Common Metrics\n",
    "- **Volume** ‚Äì Total records ingested per batch/time interval  \n",
    "- **Null Values** ‚Äì Count of missing values in important columns  \n",
    "- **Distribution** ‚Äì Value ranges staying within thresholds  \n",
    "- **Freshness** ‚Äì Time difference between now and the latest record  \n",
    "\n",
    "![Data Quality Metrics](./images/data_quality_metrics.png)\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Stakeholder-Driven Monitoring\n",
    "- Focus on what impacts **key business outcomes** (e.g., sales revenue accuracy, no nulls in purchase amounts).  \n",
    "- Less important checks (e.g., SKU‚Äìproduct description match) can be deprioritized.\n",
    "\n",
    "![Stakeholder Needs](./images/stake_holder_needs.png)\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Tip:** Align monitoring rules with stakeholder priorities to ensure accuracy, completeness, and relevance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2784ef93",
   "metadata": {},
   "source": [
    "# Great Expectations ‚Äì Step-by-Step Workflow\n",
    "\n",
    "Great Expectations is a **data quality framework** that allows you to test, document, and profile your data.  \n",
    "Its workflow revolves around **three main steps**:  \n",
    "1. **Specify the data**  \n",
    "2. **Define your expectations**  \n",
    "3. **Validate your data against the expectations**  \n",
    "\n",
    "---\n",
    "\n",
    "## üìå 1. Data Context\n",
    "- **What it is:**  \n",
    "  The **Data Context** is the **entry point** for the Great Expectations API.  \n",
    "  It‚Äôs like the **brain** of your project ‚Äî it stores metadata, manages configurations, and connects all other components.\n",
    "  \n",
    "- **What it does:**  \n",
    "  - Stores settings for data sources, expectations, and checkpoints.\n",
    "  - Saves metadata in backend stores such as:\n",
    "    - **Expectation Store** ‚Äì stores all your expectation suites.\n",
    "    - **Validation Store** ‚Äì stores validation results.\n",
    "    - **Checkpoint Store** ‚Äì stores checkpoint configurations.\n",
    "    - **Data Docs Store** ‚Äì stores rendered HTML reports of validations.\n",
    "\n",
    "- **Analogy:**  \n",
    "  Think of it like a **control center** for your data quality project ‚Äî everything starts here.\n",
    "\n",
    "üì∑ *Reference:*  \n",
    "![Data Context Diagram](./images/great_expectations.png)\n",
    "\n",
    "---\n",
    "\n",
    "## üìå 2. Data Sources\n",
    "- **What it is:**  \n",
    "  Data Sources tell Great Expectations **where** your data lives (SQL database, CSV, S3 bucket, Pandas DataFrame, etc.).\n",
    "\n",
    "- **Steps inside Data Sources:**\n",
    "  1. **Declare the Data Source** ‚Äì connect to the location where your data is stored.\n",
    "  2. **Declare Data Assets** ‚Äì select the specific tables/files/queries you care about.\n",
    "  3. **Partition into Batches** ‚Äì optionally split your data into smaller chunks (e.g., by month, store ID, or time period) to test them separately.\n",
    "  4. **Create a Batch Request** ‚Äì the formal request to retrieve the specific batch(es) you want to validate.\n",
    "\n",
    "- **Why it‚Äôs important:**  \n",
    "  Batching helps you **pinpoint issues** in specific parts of the data and **prevent future issues** by validating each batch as it comes in.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå 3. Expectations\n",
    "- **What it is:**  \n",
    "  Expectations are **rules/tests** you define to check if your data meets certain conditions.\n",
    "\n",
    "- **Examples:**\n",
    "  - `expect_column_values_to_be_unique` ‚Äì no duplicates allowed.\n",
    "  - `expect_column_values_to_not_be_null` ‚Äì no missing values.\n",
    "  - `expect_column_min_to_be_between` ‚Äì minimum value should fall within a range.\n",
    "\n",
    "- **How it works:**\n",
    "  - You create one or more expectations and **store them together** in an **Expectation Suite**.\n",
    "  - These can be reused for future data validations.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå 4. Checkpoints\n",
    "- **What it is:**  \n",
    "  Checkpoints **bundle everything together** for execution.\n",
    "\n",
    "- **How it works:**\n",
    "  - A Checkpoint takes:\n",
    "    - A **Batch Request** (which data to validate)\n",
    "    - An **Expectation Suite** (the rules to check)\n",
    "  - Runs the validation automatically using the **Validator**.\n",
    "  - Produces **Validation Results** and stores them in your backend stores.\n",
    "\n",
    "- **Benefit:**  \n",
    "  Automates and schedules validations without you having to manually run tests.\n",
    "\n",
    "üì∑ *Workflow Diagram:*  \n",
    "![Great Expectations Workflow](./images/great_expectation_workflow.png)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary Workflow\n",
    "1. **Instantiate Data Context** ‚Üí create and configure your project.  \n",
    "2. **Connect Data Source** ‚Üí specify where data comes from.  \n",
    "3. **Create Data Assets & Batches** ‚Üí define what data to test.  \n",
    "4. **Define Expectations** ‚Üí set data quality rules.  \n",
    "5. **Bundle into Expectation Suite** ‚Üí group the rules together.  \n",
    "6. **Create Checkpoint** ‚Üí run validations automatically.  \n",
    "7. **Review Validation Results** ‚Üí check Data Docs for results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8fd60e",
   "metadata": {},
   "source": [
    "# AWS CloudWatch\n",
    "\n",
    "## üåê What is CloudWatch?\n",
    "AWS **CloudWatch** is a monitoring and observability service for AWS resources and applications.  \n",
    "It collects and tracks metrics, monitors log files, and sets alarms to help you keep your systems healthy.\n",
    "\n",
    "**Why CloudWatch?**\n",
    "- Detect and respond to performance issues quickly.\n",
    "- Understand resource utilization.\n",
    "- Anticipate problems before they impact end-users.\n",
    "\n",
    "---\n",
    "\n",
    "## üñ• System Level Metrics\n",
    "Provide a **general understanding** of how your AWS resources are performing.\n",
    "- **CPU Utilization**\n",
    "- **Disk I/O**\n",
    "- **Network Traffic**\n",
    "- **Memory Usage**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öô Custom Metrics\n",
    "Track **application-specific** metrics not covered by default.\n",
    "- Number of transactions processed\n",
    "- API endpoint response time\n",
    "- Number of active users\n",
    "\n",
    "---\n",
    "\n",
    "## üö® CloudWatch Alarms\n",
    "- Set **thresholds** for any metric.\n",
    "- Trigger alerts or automated actions when thresholds are breached.\n",
    "- Establish a **baseline** by monitoring performance under different loads.\n",
    "- Retains metric data for **up to 15 months**.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Common Metrics for RDS\n",
    "![CloudWatch System Level Metrics](./images/cloudwatch.png)\n",
    "\n",
    "![Common Metrics for RDS](./images/common_metrics.png)\n",
    "\n",
    "**1. CPU Utilization**\n",
    "- High value = heavy load; over 80‚Äì90% can cause bottlenecks.\n",
    "\n",
    "**2. RAM Consumption**\n",
    "- High usage can slow down performance; may require larger instance type.\n",
    "\n",
    "**3. Disk Space**\n",
    "- Over 85% consistently ‚Üí delete/archive data to free space.\n",
    "\n",
    "**4. Database Connections**\n",
    "- Approaching max limit ‚Üí connection errors and application failures.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **In short:** CloudWatch helps you **monitor**, **visualize**, and **alert** on key metrics so you can keep AWS resources healthy and optimized."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
