{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fc57958",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [Introduction to Dataops](#-introduction-to-dataops)\n",
    "- [DataOps Automation](#dataops-automation)\n",
    "- [IAC](#infrastructure-as-code-iac)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6131db6c",
   "metadata": {},
   "source": [
    "# Introduction to DataOps\n",
    "\n",
    "**DataOps** is a collaborative data management practice focused on improving the communication, integration, and automation of data flows between data managers and data consumers across an organization.  \n",
    "It aims to deliver **faster, more reliable, and higher-quality** data analytics through the adoption of agile development, DevOps practices, and lean manufacturing principles.\n",
    "\n",
    "---\n",
    "\n",
    "## üèõ The Three Pillars of DataOps\n",
    "\n",
    "1. **Automation**  \n",
    "   - Streamlines repetitive processes using tools and scripts.\n",
    "   - Enables rapid, reliable, and scalable data pipeline deployments.\n",
    "   \n",
    "2. **Observability & Monitoring**  \n",
    "   - Ensures full visibility into the performance, health, and data quality across pipelines.\n",
    "   - Uses monitoring tools and alerts to proactively detect issues.\n",
    "   \n",
    "3. **Incident Response**  \n",
    "   - Defines structured processes for identifying, triaging, and resolving data issues quickly.\n",
    "   - Reduces downtime and minimizes business impact.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Pillars Diagram\n",
    "\n",
    "![DataOps Pillars](./images/dataops_pillers.png)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Automation in DataOps\n",
    "\n",
    "Automation is a core pillar of DataOps, enabling teams to reduce manual intervention, minimize human errors, and accelerate delivery.\n",
    "\n",
    "### **Key Automation Practices**\n",
    "1. **Continuous Integration and Continuous Delivery (CI/CD)**  \n",
    "   - Automates the process of building, testing, integrating, and deploying data pipelines.\n",
    "   \n",
    "2. **Infrastructure as Code (IaC)**  \n",
    "   - Uses code to define, provision, and manage infrastructure resources.\n",
    "   - Ensures reproducibility and scalability.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ† Tools for Automation\n",
    "\n",
    "Some popular tools used for automation in DataOps include:\n",
    "\n",
    "- **Terraform** ‚Äì For provisioning and managing infrastructure as code.  \n",
    "- **Ansible** ‚Äì For configuration management and deployment automation.  \n",
    "- **Jenkins / GitHub Actions / GitLab CI** ‚Äì For CI/CD pipeline automation.  \n",
    "- **Apache Airflow / Prefect** ‚Äì For orchestrating data workflows.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Automation Diagram\n",
    "\n",
    "![Automation](./images/automation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eca541",
   "metadata": {},
   "source": [
    "# DataOps Automation\n",
    "\n",
    "## üìå What is DataOps Automation?\n",
    "**DataOps Automation** is the practice of streamlining and automating every stage of a **data pipeline** ‚Äî from ingestion to transformation to delivery ‚Äî in order to:\n",
    "- Reduce manual intervention.\n",
    "- Improve reliability and consistency.\n",
    "- Enable faster and safer deployments.\n",
    "- Integrate best practices from **DevOps** into data engineering workflows.\n",
    "\n",
    "It borrows concepts from **DevOps automation** like:\n",
    "- **CI/CD (Continuous Integration / Continuous Delivery)**\n",
    "- **Version Control**\n",
    "- **Infrastructure as Code (IaC)**\n",
    "- **Orchestration with DAGs** (Directed Acyclic Graphs)\n",
    "\n",
    "---\n",
    "\n",
    "## üõ† Levels of DataOps Automation\n",
    "\n",
    "### 1Ô∏è‚É£ No Automation\n",
    "- All processes are run **manually** by engineers.\n",
    "- Time-consuming, prone to human error, and difficult to scale.\n",
    "\n",
    "![Manual Automation](./images/manul.png)\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ Pure Scheduling (Semi-Automation)\n",
    "- Each stage of the pipeline runs on a **fixed schedule**.\n",
    "- Improves consistency, but lacks dynamic triggers and dependency management.\n",
    "\n",
    "![Semi Automation](./images/semi_automatio.png)\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ Fully Automated with Orchestration (e.g., Apache Airflow)\n",
    "- Pipelines are defined as a **Directed Acyclic Graph (DAG)**.\n",
    "- Orchestration tools like **Apache Airflow** ensure tasks run in the right order, only when dependencies are met.\n",
    "- Enables retries, error handling, and monitoring.\n",
    "\n",
    "![Fully Automated](./images/fully_autmated_via_apache_airflow.png)\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ CI/CD in DataOps\n",
    "**Continuous Integration / Continuous Delivery** automates:\n",
    "1. **Build** ‚Äì Prepare code and configurations.\n",
    "2. **Test** ‚Äì Automatic review and testing of new code or data transformations.\n",
    "3. **Integrate** ‚Äì Merge tested changes into the main pipeline.\n",
    "4. **Deploy** ‚Äì Automatic delivery into production.\n",
    "\n",
    "This approach ensures rapid, reliable updates to both **code and data**.\n",
    "\n",
    "![CI/CD](./images/ci-cd.png)\n",
    "\n",
    "---\n",
    "\n",
    "## üíª Infrastructure as Code (IaC)\n",
    "- Maintain infrastructure configurations as **code**.\n",
    "- Example: Provisioning cloud storage, compute resources, and databases through code files.\n",
    "- Benefits:\n",
    "  - Version control for infrastructure.\n",
    "  - Reproducibility.\n",
    "  - Easy rollback to previous setups.\n",
    "\n",
    "![Infrastructure as Code](./images/iac.png)\n",
    "\n",
    "---\n",
    "\n",
    "## üìÇ Version Control for Code & Data\n",
    "- Tracks **changes** in both:\n",
    "  - **Pipeline code** (SQL, Python, configs).\n",
    "  - **Data versions** moving through the pipeline.\n",
    "- Enables rollback to **previous versions** in case of errors.\n",
    "\n",
    "![Version Control](./images/version_control.png)\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Why DataOps Automation Matters\n",
    "- **Consistency** ‚Äì Fewer errors and more predictable results.\n",
    "- **Speed** ‚Äì Faster deployments and updates.\n",
    "- **Scalability** ‚Äì Handle large, complex pipelines without bottlenecks.\n",
    "- **Resilience** ‚Äì Automatic error handling, monitoring, and quick rollbacks.\n",
    "\n",
    "In short, DataOps Automation ensures that **data pipelines run like a well-oiled factory line** ‚Äî continuously delivering trusted data products at high speed and with minimal manual effort."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8c117e",
   "metadata": {},
   "source": [
    "# Infrastructure as Code (IaC)\n",
    "\n",
    "## üìå What is Infrastructure as Code?\n",
    "**Infrastructure as Code (IaC)** is the practice of defining, deploying, and maintaining infrastructure using **code**, rather than manual processes.  \n",
    "With IaC, you can automate the creation of **networking, security, computing, storage**, and other resources required for your cloud-based data pipelines.\n",
    "\n",
    "Benefits include:\n",
    "- **Automation** ‚Äì Reduce manual effort and human error.\n",
    "- **Scalability** ‚Äì Deploy infrastructure for large, complex systems quickly.\n",
    "- **Consistency** ‚Äì Ensure all environments match the desired configuration.\n",
    "- **Version Control** ‚Äì Track changes and roll back when necessary.\n",
    "\n",
    "---\n",
    "\n",
    "## üï∞ History of IaC\n",
    "\n",
    "![History of IaC](./images/iac_history.png)\n",
    "\n",
    "1. **1970s ‚Äì Configuration Management**\n",
    "   - Engineers used scripts (like early **BASH**) to automate configuration of physical machines.\n",
    "   - Primitive automation for repetitive setup tasks.\n",
    "\n",
    "2. **2006 ‚Äì AWS EC2 Launch**\n",
    "   - Cloud computing became widely accessible.\n",
    "   - Developers could **spin up virtual servers on demand**.\n",
    "\n",
    "3. **2010s ‚Äì Modern IaC Tools**\n",
    "   - Tools like **Terraform**, **AWS CloudFormation**, and **Ansible** emerged.\n",
    "   - Enabled full infrastructure provisioning via code files instead of manual configuration.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è How Terraform Works\n",
    "\n",
    "Terraform is a **cloud-agnostic** IaC tool created by **HashiCorp**.  \n",
    "It uses **HCL (HashiCorp Configuration Language)**, a **declarative** language, to define the desired end state of infrastructure.\n",
    "\n",
    "### Example: S3 Bucket Setup in Terraform\n",
    "\n",
    "![Terraform S3 Config](./images/terraform_s3_config.png)\n",
    "\n",
    "**Key points in Terraform syntax:**\n",
    "- `resource` ‚Üí Keyword indicating the type of entity you want to create.\n",
    "- `\"aws_s3_bucket\"` ‚Üí **Resource type** (provider + service).\n",
    "- `\"data_lake\"` ‚Üí **Resource name** (your internal reference).\n",
    "- `{ ... }` ‚Üí **Configuration block** with key-value pairs.\n",
    "\n",
    "---\n",
    "\n",
    "## üñã Example: Creating a VPC and EC2 Instance\n",
    "\n",
    "![VPC & Instance](./images/vpc_instance_iac.png)\n",
    "```hcl\n",
    "#VPC Creation\n",
    "resource \"aws_vpc\" \"main\" {\n",
    "  cidr_block       = \"10.0.0.0/16\"\n",
    "  instance_tenancy = \"default\"\n",
    "\n",
    "  tags = {\n",
    "    Name = \"main\"\n",
    "  }\n",
    "}\n",
    "\n",
    "#EC2 Instance Creation\n",
    "resource \"aws_instance\" \"web\" {\n",
    "  ami           = data.aws_ami.ubuntu.id\n",
    "  instance_type = \"t3.micro\"\n",
    "\n",
    "  tags = {\n",
    "    Name = \"HelloWorld\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "---\n",
    "\n",
    "## üîÑ Terraform Idempotency\n",
    "\n",
    "![Idempotency](./images/idempotent.png)\n",
    "\n",
    "Terraform ensures **idempotency** ‚Äî running the same configuration multiple times **won‚Äôt recreate resources unnecessarily**:\n",
    "- If the resource **does not exist**, Terraform creates it.\n",
    "- If it **exists but differs** from the desired state, Terraform updates it.\n",
    "- If it **matches exactly**, Terraform does nothing.\n",
    "\n",
    "---\n",
    "\n",
    "## üÜö Bash vs Terraform\n",
    "- **Bash scripts** are **imperative** ‚Üí they execute commands in a specific order without checking the existing state.\n",
    "- Running the same Bash provisioning script twice will create **duplicate resources**.\n",
    "- **Terraform** is **declarative** ‚Üí it checks the current state and only makes necessary changes.\n",
    "- This makes Terraform **safe, repeatable, and idempotent**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842647a6",
   "metadata": {},
   "source": [
    "# Terraform setting up EC2 Instance\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ What We‚Äôll Build (Big Picture)\n",
    "\n",
    "![Region + Default VPC](./images/setting_up_ec2.png)\n",
    "\n",
    "We‚Äôll create a tiny EC2 instance in **`us-east-1`** (West Virginia) inside the **default VPC**. This is great for learning; in production you‚Äôd use a custom VPC.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Providers & Plugins ‚Äî What They Are (and Why You Need Them)\n",
    "\n",
    "![Provider anatomy](./images/provider.png)\n",
    "\n",
    "**Terraform Core** understands `.tf` files and figures out a plan, but it **doesn‚Äôt know vendor APIs** by itself.  \n",
    "That‚Äôs the job of a **provider plugin**:\n",
    "\n",
    "- A **provider** (in Terraform terms) is a **binary plugin** that knows how to talk to an external system‚Äôs API (AWS, GCP, GitHub, Datadog, etc.).\n",
    "- The provider implements CRUD operations for **resources** (e.g., `aws_instance`, `aws_s3_bucket`).\n",
    "- When you run `terraform init`, Terraform **downloads the provider plugin** you declared in your config (from the Terraform Registry) and stores it under `.terraform/`.\n",
    "\n",
    "**Two steps you always do:**\n",
    "1. **Declare** that you need a provider (so Terraform downloads the plugin).\n",
    "2. **Configure** the provider (region, credentials/profile, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "## üß∞ Install & Prepare Terraform\n",
    "\n",
    "![Install Terraform](./images/terraform_intall.png)\n",
    "\n",
    "1. **Install Terraform CLI**\n",
    "   - macOS (Homebrew): `brew tap hashicorp/tap && brew install hashicorp/tap/terraform`\n",
    "   - Windows: Install from the official MSI or use `choco install terraform`.\n",
    "   - Linux: Use your distro‚Äôs package manager or download the binary from HashiCorp.\n",
    "\n",
    "2. **Set up AWS credentials** (pick one)\n",
    "   - **AWS CLI profile**:  \n",
    "     `aws configure --profile myprofile`\n",
    "   - **Environment variables**:  \n",
    "     `export AWS_ACCESS_KEY_ID=...`  \n",
    "     `export AWS_SECRET_ACCESS_KEY=...`  \n",
    "     (and optionally `AWS_SESSION_TOKEN` if you use SSO/STS)\n",
    "   - **EC2 role** (if running Terraform **on** an EC2 instance)\n",
    "\n",
    "3. **Create a new project folder**\n",
    "   ```\n",
    "   mkdir tf-ec2-lab && cd tf-ec2-lab\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Terraform Workflow You‚Äôll Use\n",
    "\n",
    "![Terraform Steps](./images/terraform_steps.png)\n",
    "\n",
    "1. **Write** `.tf` files  \n",
    "2. **Init** ‚Üí downloads provider plugin(s)  \n",
    "3. **Plan** ‚Üí shows the execution plan  \n",
    "4. **Apply** ‚Üí creates/updates/destroys resources\n",
    "\n",
    "---\n",
    "\n",
    "## üß± Code Anatomy: Blocks, Labels, Arguments\n",
    "\n",
    "![Code blocks in editor](./images/terraform_code.png)\n",
    "![Labels & types](./images/resourse__.png)\n",
    "\n",
    "- **Block**: `keyword \"label1\" \"label2\" { ... }`\n",
    "- **Examples**: `terraform {}`, `provider \"aws\" {}`, `resource \"aws_instance\" \"web\" {}`  \n",
    "- **Arguments** inside `{}` are key‚Äìvalue pairs or nested blocks.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) **Declare** the AWS Provider (so Terraform can download the plugin)\n",
    "\n",
    "This lives in `main.tf`. It tells Terraform **which provider plugin** to get and which Terraform CLI versions are allowed.\n",
    "\n",
    "```hcl\n",
    "terraform {\n",
    "  required_version = \">= 1.5.0\"\n",
    "\n",
    "  required_providers {\n",
    "    aws = {\n",
    "      source  = \"hashicorp/aws\"   # Download the AWS provider plugin from the Registry\n",
    "      version = \"~> 5.0\"          # Stay within major version 5 for compatibility\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**What each part does**\n",
    "- `required_providers.aws.source` ‚Üí Registry address for the **plugin**.\n",
    "- `required_providers.aws.version` ‚Üí Version range for stability.\n",
    "- `required_version` ‚Üí Guardrail for your local Terraform CLI version.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) **Configure** the AWS Provider (region & auth settings)\n",
    "\n",
    "This block configures the **runtime** for the provider plugin (e.g., which **region** to call).\n",
    "\n",
    "```hcl\n",
    "provider \"aws\" {\n",
    "  region  = \"us-east-1\"   # Target AWS region for API calls\n",
    "  # profile = \"myprofile\" # Optional: use a specific AWS CLI profile\n",
    "}\n",
    "```\n",
    "\n",
    "- If you omit `profile`, Terraform uses the default credential chain:\n",
    "  environment variables ‚Üí shared credentials files ‚Üí SSO/role ‚Üí EC2 role, etc.\n",
    "- You can keep `region` as a variable if you prefer (shown below).\n",
    "\n",
    "![Provider block callouts](./images/terraform_provider.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 3) **Create** an EC2 Instance (Simple Version)\n",
    "\n",
    "![Resource type](./images/resourse.png)\n",
    "\n",
    "At minimum you need **AMI** and **instance_type**.\n",
    "\n",
    "```hcl\n",
    "resource \"aws_instance\" \"webserver\" {\n",
    "  ami           = \"ami-0453ec754f44f9a4a\"  # Replace with a valid AMI ID in your region\n",
    "  instance_type = \"t2.micro\"\n",
    "\n",
    "  tags = {\n",
    "    Name = \"ExampleServer\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "- `resource \"aws_instance\" \"webserver\"`: resource **type** and local **name**.\n",
    "- `ami`: Operating system image.\n",
    "- `instance_type`: Size (CPU/RAM).\n",
    "- `tags`: helpful labels.\n",
    "\n",
    "---\n",
    "\n",
    "## 3b) **(Safer)** Lookup the Latest Amazon Linux 2 AMI Dynamically\n",
    "\n",
    "Avoids hard‚Äëcoded AMI IDs.\n",
    "\n",
    "```hcl\n",
    "data \"aws_ami\" \"al2\" {\n",
    "  most_recent = true\n",
    "\n",
    "  filter {\n",
    "    name   = \"name\"\n",
    "    values = [\"amzn2-ami-hvm-*-x86_64-gp2\"]\n",
    "  }\n",
    "\n",
    "  owners = [\"137112412989\"] # Amazon\n",
    "}\n",
    "\n",
    "resource \"aws_instance\" \"webserver\" {\n",
    "  ami           = data.aws_ami.al2.id\n",
    "  instance_type = \"t2.micro\"\n",
    "\n",
    "  tags = {\n",
    "    Name = \"ExampleServer\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Why this is better**\n",
    "- The **data source** only **reads** info (does not create resources).\n",
    "- You always get a recent, supported AMI in your region.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## üß™ Run It\n",
    "\n",
    "1) Initialize (downloads the **AWS provider plugin**)\n",
    "```bash\n",
    "terraform init\n",
    "```\n",
    "\n",
    "2) Preview what will happen\n",
    "```bash\n",
    "terraform plan\n",
    "```\n",
    "\n",
    "3) Create resources (you‚Äôll be prompted to type `yes`)\n",
    "```bash\n",
    "terraform apply\n",
    "```\n",
    "\n",
    "4) Clean up later\n",
    "```bash\n",
    "terraform destroy\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Extra Notes & Troubleshooting\n",
    "\n",
    "- **Credentials**: If `plan/apply` fails with ‚Äúno valid credentials,‚Äù set env vars or specify `profile` in the provider block.\n",
    "- **Region mismatch**: AMI IDs are region‚Äëspecific. Use the **data source** approach to avoid mismatches.\n",
    "- **Idempotency**: Terraform is **declarative** and **idempotent**‚Äîre‚Äërunning `apply` won‚Äôt duplicate resources if the real world already matches your config.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8de1ba",
   "metadata": {},
   "source": [
    "# Using Variables and Outputs in Terraform\n",
    "\n",
    "![Hard-coded values example](./images/hard_code.png)\n",
    "\n",
    "In the earlier config, values like **AWS region** and **EC2 instance name** were **hard-coded**. Replacing them with **variables** makes the code reusable, and exposing key attributes as **outputs** lets you print or pass data to other stacks.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Declare input variables\n",
    "\n",
    "Create `variables.tf` and describe the inputs your config needs.\n",
    "\n",
    "```hcl\n",
    "variable \"region\" {\n",
    "  description = \"AWS region to deploy resources\"\n",
    "  type        = string\n",
    "  default     = \"us-east-1\"\n",
    "}\n",
    "\n",
    "variable \"serverName\" {\n",
    "  description = \"Name tag for the EC2 instance\"\n",
    "  type        = string\n",
    "}\n",
    "```\n",
    "\n",
    "**What this does**\n",
    "\n",
    "- `variable \"<name>\"` defines an input.\n",
    "- `description` documents it.\n",
    "- `type` constrains values.\n",
    "- `default` (optional). If omitted, Terraform will prompt (ask) for a value.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Use variables in your config\n",
    "\n",
    "Wire those inputs into your provider and resources by referencing `var.<name>`.\n",
    "\n",
    "```hcl\n",
    "# providers.tf\n",
    "provider \"aws\" {\n",
    "  region = var.region\n",
    "}\n",
    "\n",
    "# main.tf\n",
    "resource \"aws_instance\" \"webserver\" {\n",
    "  ami           = \"ami-0453ec754f44f9a4a\"\n",
    "  instance_type = \"t2.micro\"\n",
    "\n",
    "  tags = {\n",
    "    Name = var.serverName\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**What this does**\n",
    "\n",
    "- `var.region` replaces the hard-coded region.\n",
    "- `var.serverName` sets the `Name` tag dynamically.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Set variable values\n",
    "\n",
    "You can pass values at apply time or via a `.tfvars` file.\n",
    "\n",
    "```bash\n",
    "# Option A: CLI flag\n",
    "terraform apply -var=\"serverName=ExampleServer\"\n",
    "\n",
    "# Option B: terraform.tfvars (auto-loaded)\n",
    "```\n",
    "\n",
    "```hcl\n",
    "# terraform.tfvars\n",
    "serverName = \"ExampleServer\"\n",
    "```\n",
    "\n",
    "**What this does**\n",
    "\n",
    "- CLI `-var` sets a one-off value.\n",
    "- `terraform.tfvars` (or any `*.tfvars`) is auto-read, great for teams.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Declare outputs\n",
    "\n",
    "Expose useful attributes (like the instance **ID** and **ARN**) so you can print or reuse them.\n",
    "\n",
    "```hcl\n",
    "# outputs.tf\n",
    "output \"server_id\" {\n",
    "  description = \"The ID of the EC2 instance\"\n",
    "  value       = aws_instance.webserver.id\n",
    "}\n",
    "\n",
    "output \"server_arn\" {\n",
    "  description = \"The ARN of the EC2 instance\"\n",
    "  value       = aws_instance.webserver.arn\n",
    "}\n",
    "```\n",
    "\n",
    "**What this does**\n",
    "\n",
    "- `output \"<name>\"` defines a value to return after `apply`.\n",
    "- `value` reads an attribute from a resource using\n",
    "  `resource_type.resource_name.attribute`.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Apply and read outputs\n",
    "\n",
    "```bash\n",
    "# Create or update infra\n",
    "terraform apply\n",
    "\n",
    "# Show all outputs\n",
    "terraform output\n",
    "\n",
    "# Show a single output\n",
    "terraform output server_id\n",
    "```\n",
    "\n",
    "**What this does**\n",
    "\n",
    "- `apply` evaluates variables, creates/updates resources, then prints outputs.\n",
    "- `terraform output` queries stored output values anytime after an apply.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) File layout (recommended)\n",
    "since terraform assume all tf files as one big folder it is safe to use differnt tf file for usage for better understading \n",
    "Keep things tidy by splitting concerns:\n",
    "\n",
    "```\n",
    ".\n",
    "‚îú‚îÄ main.tf         # resources\n",
    "‚îú‚îÄ providers.tf    # provider + terraform blocks\n",
    "‚îú‚îÄ variables.tf    # inputs\n",
    "‚îú‚îÄ outputs.tf      # outputs\n",
    "‚îî‚îÄ terraform.tfvars# values for inputs (excluded from VCS if sensitive)\n",
    "```\n",
    "\n",
    "This organization scales as your workspace grows. Variables make configs **portable**, and outputs make results **discoverable** across modules/workspaces."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
