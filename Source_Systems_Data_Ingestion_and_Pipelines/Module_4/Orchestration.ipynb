{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90efe29a",
   "metadata": {},
   "source": [
    "# Table of Contents \n",
    "\n",
    "- [Cron Jobs (Before Orchestration)](#cron-jobs-before-orchestration)\n",
    "- [Evolution of Orchestration tools](#evolution-of-orchestration-tools)\n",
    "- [Orchestration Basics](#orchestration-basics)\n",
    "- [Airflow Core Components](#airflow-core-components)\n",
    "- [Airflow UI](#video-recommendation-learn-the-airflow-ui)\n",
    "- [Airflow Xcom & variables](#airflow-xcom--variables)\n",
    "- [Best practices for writing airflow dags](#best-practices-for-writing-airflow-dags)\n",
    "- [Taskflow_api in Airflow](#taskflow-api-in-airflow)\n",
    "- [Orchestration on AWS](#orchestration-on-aws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7763c68",
   "metadata": {},
   "source": [
    "# Cron Jobs (Before Orchestration)\n",
    "\n",
    "![Cron Logo](./images/cron.png)\n",
    "\n",
    "## What is Cron?\n",
    "**Cron** is a Unix/Linux utility (from the 1970s) that automatically runs commands or scripts on a schedule you define.\n",
    "\n",
    "---\n",
    "\n",
    "## How a Cron Job Works\n",
    "A cron job line has **five timing fields** followed by the command:\n",
    "\n",
    "    MINUTE(0‚Äì59) HOUR(0‚Äì23) DAY(1‚Äì31) MONTH(1‚Äì12) WEEKDAY(0‚Äì6)  command\n",
    "\n",
    "You can use `*` (asterisk) to mean ‚Äúany value‚Äù.\n",
    "\n",
    "**Examples**\n",
    "- Run at midnight on Jan 1 every year:\n",
    "\n",
    "      0 0 1 1 * echo \"Happy New Year\"\n",
    "\n",
    "- Run every night at midnight:\n",
    "\n",
    "      0 0 * * * python ingest_from_rest_api.py\n",
    "\n",
    "![Cron Fields](./images/cron_work.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Before Orchestration: Pure Scheduling with Cron\n",
    "Teams used to chain data pipeline steps by scheduling **multiple cron jobs** a little apart in time so they‚Äôd (hopefully) run in order:\n",
    "\n",
    "- 12:00 AM ‚Üí ingest API  \n",
    "- 01:00 AM ‚Üí transform  \n",
    "- 02:00 AM ‚Üí combine with DB  \n",
    "- 03:00 AM ‚Üí load to warehouse\n",
    "\n",
    "This is a **pure scheduling approach**‚Äîno dependency awareness, just timed starts.\n",
    "\n",
    "---\n",
    "\n",
    "## Problems with Pure Cron Scheduling\n",
    "- ‚ùå No dependency checks (a 1 AM job runs even if the midnight job failed or ran long)  \n",
    "- ‚ùå Minimal monitoring/alerting; failures often discovered late  \n",
    "- ‚ùå Debugging & observability are DIY (logs, alerts, retries)  \n",
    "- ‚ùå Fragile when task durations vary\n",
    "\n",
    "![Pure Scheduling Drawbacks](./images/pure_scheduiling.png)\n",
    "\n",
    "---\n",
    "\n",
    "## When Cron Is Still a Good Fit\n",
    "- ‚úÖ Simple, independent, recurring tasks (backups, log cleanup, small data fetches)  \n",
    "- ‚úÖ Quick prototypes where a full orchestrator is overkill  \n",
    "- ‚úÖ Environments with very light automation needs\n",
    "\n",
    "**Rule of thumb:** If tasks depend on other tasks finishing, or you need retries, backfills, SLAs, or rich monitoring‚Äîuse an **orchestration tool** (e.g., Airflow, Prefect, Dagster). Otherwise, Cron is perfectly fine for small periodic jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45bb4a9",
   "metadata": {},
   "source": [
    "# Evolution of Orchestration Tools\n",
    "\n",
    "Orchestration is the backbone of modern data engineering, ensuring that complex workflows run in sequence, on time, and with reliability. Let‚Äôs walk through its evolution and see how the tooling landscape has shifted over the years.\n",
    "\n",
    "---\n",
    "\n",
    "## üìú Early Days ‚Äì In-House Solutions\n",
    "Before the past decade, orchestration was largely **limited to big tech companies** because:\n",
    "- Open-source or managed orchestration tools didn‚Äôt exist.\n",
    "- Building in-house systems was expensive and complicated.\n",
    "\n",
    "---\n",
    "\n",
    "## üï∞ Timeline of Orchestration Tools\n",
    "\n",
    "![Evolution Timeline](./images/evolution.png)\n",
    "\n",
    "- **Late 2000s**:  \n",
    "  - **Facebook‚Äôs DataSwarm** ‚Üí built internally to manage their growing data workflows.\n",
    "  \n",
    "- **2010s**:  \n",
    "  - **Apache Oozie** ‚Üí became popular, but it was tied to Hadoop clusters, making it less flexible in heterogeneous environments.\n",
    "\n",
    "- **2014**:  \n",
    "  - **Airbnb released Airflow** ‚Üí inspired by earlier tools like DataSwarm but designed to be **open-source, flexible, and Python-based**.  \n",
    "  - Quickly became the *industry standard*.\n",
    "\n",
    "- **2019**:  \n",
    "  - **Apache Airflow** graduated to a full Apache Software Foundation project, solidifying its role as the most widely adopted orchestration framework.\n",
    "\n",
    "---\n",
    "\n",
    "## üåü Airflow: Advantages & Challenges\n",
    "\n",
    "![Airflow Advantages](./images/adv_airflow.png)\n",
    "\n",
    "**Advantages**\n",
    "- Written in **Python**, making it flexible and widely accessible.  \n",
    "- Very **active open-source community** with frequent commits and bug fixes.  \n",
    "- Available as a **managed service** through providers like AWS, GCP, and Astronomer.  \n",
    "\n",
    "**Challenges**\n",
    "- Struggles with **scalability** for very large workflows.  \n",
    "- Limited built-in support for **data integrity**.  \n",
    "- Lacks **native support for streaming pipelines**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÆ Other Open-Source Orchestration Tools\n",
    "\n",
    "![Other Tools](./images/other_orche_tools.png)\n",
    "\n",
    "As the ecosystem evolved, newer tools emerged, aiming to improve on Airflow‚Äôs design while addressing its shortcomings:\n",
    "\n",
    "- **Luigi**: Early workflow management tool.  \n",
    "- **Conductor**: Focused on microservice orchestration.  \n",
    "- **Prefect**: More scalable and developer-friendly orchestration system.  \n",
    "- **Dagster**: Adds built-in data quality testing and transformation features.  \n",
    "- **Mage**: Provides integrated data transformation and monitoring.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö° Example Improvements\n",
    "\n",
    "![Examples of Other Tools](./images/eg_other_orche.png)\n",
    "\n",
    "- **Prefect** ‚Üí More scalable than Airflow, making it a good fit for heavy workloads.  \n",
    "- **Dagster & Mage** ‚Üí Bring built-in **data quality testing** and transformation features, helping ensure correctness beyond just scheduling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104ef677",
   "metadata": {},
   "source": [
    "# Orchestration Basics\n",
    "\n",
    "## Why Orchestration?\n",
    "Orchestration is about managing complex **data pipelines** more reliably than simple Cron jobs. While Cron can schedule tasks, orchestration tools give you advanced control over dependencies, monitoring, alerts, and fallback plans.\n",
    "\n",
    "**Pros:**\n",
    "- Set up dependencies between tasks  \n",
    "- Monitor task execution  \n",
    "- Get alerts on failures  \n",
    "- Create fallback plans  \n",
    "\n",
    "**Cons:**\n",
    "- Adds more operational overhead compared to simple Cron scheduling  \n",
    "\n",
    "![Pros and Cons](./images/pro_con.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Directed Acyclic Graphs (DAGs)\n",
    "At the heart of orchestration is the **Directed Acyclic Graph (DAG)** ‚Äî a structure where:\n",
    "- Each **task** is a node  \n",
    "- Each **arrow** (edge) shows data flow  \n",
    "- Data flows **only in one direction**  \n",
    "- No loops or cycles are allowed  \n",
    "\n",
    "This ensures predictable execution order.\n",
    "\n",
    "![DAG](./images/dag.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Dependencies Between Tasks\n",
    "With Cron, tasks could overlap or break if one runs late.  \n",
    "With orchestration, you can **define dependencies**: a task won‚Äôt start until its upstream tasks are complete.\n",
    "\n",
    "![Task Dependencies](./images/task_based.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Orchestration in Airflow\n",
    "In Airflow, DAGs are defined in Python. You programmatically specify tasks and how they depend on each other.\n",
    "\n",
    "![Code Example](./images/code_for_orchestration_.png)\n",
    "\n",
    "Airflow then lets you:\n",
    "- Visualize DAGs  \n",
    "- Trigger runs manually or on schedule  \n",
    "- Monitor progress & debug issues  \n",
    "\n",
    "---\n",
    "\n",
    "## Time-based vs Event-based Triggers\n",
    "Airflow DAGs can run on **time-based schedules** (like Cron) or be triggered by **events** (like new data arriving).\n",
    "\n",
    "![Time or Event Based](./images/time_or_event_based.png)\n",
    "\n",
    "### Example: Time-based\n",
    "Run daily at midnight:  \n",
    "![Example Time-based](./images/eg_time_based.png)\n",
    "\n",
    "### Example: Event-based\n",
    "Run when a dataset updates:  \n",
    "![Example Event-based](./images/eg_event_based.png)\n",
    "\n",
    "You can even make **part of a DAG** wait for an external event, e.g., a file landing in S3.  \n",
    "\n",
    "![External Flow](./images/external_flow.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Data Quality Checks\n",
    "Another orchestration benefit: embedding **data quality checks** into the DAG.  \n",
    "For example:\n",
    "- Count of null values  \n",
    "- Validating ranges of values  \n",
    "- Schema verification  \n",
    "\n",
    "![Quality Checks](./images/quality_checks.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "Orchestration tools (like Airflow) provide the structure to:\n",
    "- Define pipelines as **DAGs**  \n",
    "- Manage **dependencies** between tasks  \n",
    "- Run on **time or event conditions**  \n",
    "- Monitor and alert on failures  \n",
    "- Enforce **data quality**  \n",
    "\n",
    "Though more complex than Cron, orchestration is essential for reliable and scalable data engineering workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d85b88",
   "metadata": {},
   "source": [
    "# Airflow Core Components\n",
    "\n",
    "Airflow is built around a set of core components that work together to run your DAGs (Directed Acyclic Graphs), monitor dependencies, execute tasks, and display status updates to users.\n",
    "\n",
    "---\n",
    "\n",
    "## üîë Core Components of Airflow\n",
    "\n",
    "The main components of Airflow are:\n",
    "\n",
    "- **Web Server** ‚Üí Hosts the Airflow **User Interface (UI)**.  \n",
    "- **Scheduler** ‚Üí Monitors DAGs and determines when tasks should run.  \n",
    "- **Workers** ‚Üí Execute the tasks that are scheduled.  \n",
    "- **Metadata Database** ‚Üí Stores the state of DAGs and tasks (success, failure, etc.).  \n",
    "- **DAG Directory** ‚Üí Stores Python scripts that define your DAGs.\n",
    "\n",
    "All these components are essential parts of an Airflow environment, whether you install it directly or use a managed service like MWAA.\n",
    "\n",
    "![Airflow Components](./images/airflow_components.png)\n",
    "\n",
    "---\n",
    "\n",
    "## üñ•Ô∏è User Interaction with Airflow\n",
    "\n",
    "- You (the user) write **Python scripts** to define DAGs and place them in the **DAG Directory**.  \n",
    "- These DAGs automatically appear in the **Web Server UI**, where you can:\n",
    "  - Visualize DAGs  \n",
    "  - Monitor tasks  \n",
    "  - Trigger DAGs manually  \n",
    "  - Troubleshoot issues  \n",
    "\n",
    "Thus, the DAG directory + user interface are the **main interaction points** for users, while the other components work in the background.\n",
    "\n",
    "![User Interaction](./images/user_interaction.png)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚è∞ Scheduling with the Scheduler\n",
    "\n",
    "- The **Scheduler** runs every minute by default.  \n",
    "- It checks all DAGs in the DAG Directory and determines:\n",
    "  - If a task should be triggered by time (schedule-based)  \n",
    "  - If a task‚Äôs dependencies are complete (dependency-based)  \n",
    "- Once ready, the **Scheduler**:\n",
    "  1. Pushes tasks into a queue  \n",
    "  2. Uses an **Executor** to extract tasks from the queue  \n",
    "  3. Sends the tasks to **Workers**, which run them  \n",
    "\n",
    "As tasks move through this process, their status transitions from:  \n",
    "`schedule ‚Üí queued ‚Üí running ‚Üí success/failed`.\n",
    "\n",
    "![Scheduler Workflow](./images/scheduler_workflow.png)\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Task Status and Metadata Database\n",
    "\n",
    "- The **Scheduler** and **Workers** update the **Metadata Database** with task status and DAG states.  \n",
    "- The **Web Server** then queries the metadata database to extract these statuses.  \n",
    "- Finally, the **UI** displays task states to the user.  \n",
    "\n",
    "This is why you can see real-time updates of task states (like running, success, or failed) in the Airflow UI.\n",
    "\n",
    "![Task Status Workflow](./images/status_workflow.png)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚òÅÔ∏è Managed Workflows for Apache Airflow (MWAA)\n",
    "\n",
    "Amazon provides a managed service called **MWAA** (Managed Workflows for Apache Airflow), which automatically sets up and manages all Airflow components for you.\n",
    "\n",
    "In MWAA:\n",
    "\n",
    "- **DAG Directory** ‚Üí Stored in **Amazon S3**.  \n",
    "- **Metadata Database** ‚Üí Hosted on **Amazon Aurora PostgreSQL**.  \n",
    "- **Schedulers, Workers, Web Server** ‚Üí Managed as AWS services inside a secure **VPC**.  \n",
    "- Additional integrations ‚Üí AWS CloudWatch for logging, SQS for queuing, and ECR for containers.\n",
    "\n",
    "This allows you to use Airflow at scale without worrying about manually configuring or maintaining its infrastructure.\n",
    "\n",
    "![MWAA](./images/mwaa.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaa2eb3",
   "metadata": {},
   "source": [
    "# Video Recommendation: Learn the Airflow UI\n",
    "\n",
    "> Refer to this video to understand the Airflow UI end-to-end:\n",
    "\n",
    "**[Apache Airflow UI Tour | Airflow UI Walkthrough for Beginners](https://www.youtube.com/watch?v=sMIW8dLjzRU)**\n",
    "\n",
    "## What you‚Äôll learn\n",
    "- Navigating DAGs: **Grid**, **Graph**, **Gantt**, **Tree** views  \n",
    "- Triggering DAG runs, **pausing/unpausing**, filtering & tags  \n",
    "- Inspecting **Task Instance** details: logs, retries, XComs  \n",
    "- Monitoring run states and understanding status colors  \n",
    "- Viewing code, variables, connections, and admin panels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ff11c8",
   "metadata": {},
   "source": [
    "# Additional Notes About Airflow Basic Concepts  \n",
    "\n",
    "üìñ Refer to this link for full notes: [Coursera Reading Item](https://www.coursera.org/learn/source-systems-data-ingestion-and-pipelines/supplement/GtHjE/additional-notes-about-airflow-basic-concepts)  \n",
    "\n",
    "This reading item provides extended information about:\n",
    "\n",
    "- **Scheduling Your DAGs & DAG Parameters**  \n",
    "  - Explains `dag_id`, `tags`, `description`, `schedule`, `start_date`, and `catchup`  \n",
    "  - Clarifies **data interval** vs **logical date**  \n",
    "  - Shows how DAG runs are executed **at the end** of the data interval (e.g., run for March 1 executes at March 2 midnight)  \n",
    "  - Introduces **timetables** for custom scheduling  \n",
    "\n",
    "- **Airflow Operators**  \n",
    "  - Core operators: `EmptyOperator`, `PythonOperator`, `BashOperator`, `EmailOperator`  \n",
    "  - Extended operators: AWS operators, database-to-S3 operators, etc.  \n",
    "  - Best practice: prefer using existing operators over writing from scratch  \n",
    "  - Parameters like `task_id`, `python_callable`, `email`, `email_on_retry`, `email_on_failure`, and `retries`  \n",
    "\n",
    "- **Defining Dependencies**  \n",
    "  - Use the bit-shift operator `>>` to set task dependencies  \n",
    "\n",
    "- **Extra References**  \n",
    "  - Links to documentation about **data intervals**, **execution dates**, **timetables**, and **sensors** (a special kind of operator).  \n",
    "\n",
    "üëâ This is the info you get in the *Additional Notes About Airflow Basic Concepts* reading item."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b91acd",
   "metadata": {},
   "source": [
    "# Airflow XCom & Variables\n",
    "\n",
    "![XCom overview](./images/x_com.png)\n",
    "\n",
    "## What is XCom (Cross-Communication)?\n",
    "XCom is Airflow‚Äôs lightweight way to **pass small pieces of data between tasks** (strings, numbers, timestamps, short JSON). Values are stored in the **metadata database**, not in memory, so any downstream task in the same DAG run can fetch them later.\n",
    "\n",
    "> Use XCom for *metadata and tiny payloads*. For data frames, files, or anything large, write to external storage (e.g., S3, GCS, DB) and pass a reference (path/ID) via XCom.\n",
    "\n",
    "---\n",
    "\n",
    "## How XCom works\n",
    "\n",
    "![XCom push/pull workflow](./images/xcom_workflow.png)\n",
    "\n",
    "1. **Producer task** calls `xcom_push(key=..., value=...)`.\n",
    "2. Airflow stores: **value, key, timestamp, dag_id, task_id, run_id** in the metadata DB.\n",
    "3. **Consumer task** calls `xcom_pull(key=..., task_ids=...)` to retrieve the value.\n",
    "\n",
    "---\n",
    "\n",
    "## Using XCom in PythonOperator (via context)\n",
    "\n",
    "> The **Airflow context** is a dict of runtime metadata for the *current task instance* (execution date, dag_id, run_id, task_id, ti, data interval, etc.).  \n",
    "> `PythonOperator` automatically passes this context as `**kwargs` to your callable. You typically read it as `**context` (or just use the `ti` object).\n",
    "\n",
    "![Code setup note](./images/xcom_code.png)\n",
    "\n",
    "### 1) Push from the upstream task\n",
    "```python\n",
    "def extract_from_api(**context):\n",
    "    import requests\n",
    "\n",
    "    # Example: call an API and compute a small metric\n",
    "    resp = requests.get(\n",
    "        \"https://jobicy.com/api/v2/remote-jobs\",\n",
    "        params={\"count\": 40, \"geo\": \"usa\", \"industry\": \"engineering\", \"tag\": \"data engineer\"},\n",
    "        timeout=30,\n",
    "    ).json()\n",
    "\n",
    "    senior = sum(1 for job in resp.get(\"jobs\", []) if job.get(\"jobLevel\") == \"Senior\")\n",
    "    ratio_senior = senior / max(1, len(resp.get(\"jobs\", [])))\n",
    "\n",
    "    # XCom push via task instance\n",
    "    context[\"ti\"].xcom_push(key=\"ratio_senior_jobs\", value=ratio_senior)\n",
    "```\n",
    "\n",
    "![Push with context['ti']](./images/xcom_code_2.png)\n",
    "\n",
    "### 2) Pull in a downstream task\n",
    "```python\n",
    "def print_data(**context):\n",
    "    ratio = context[\"ti\"].xcom_pull(key=\"ratio_senior_jobs\", task_ids=\"extract_metric\")\n",
    "    print(f\"Senior DE ratio: {ratio}\")\n",
    "```\n",
    "\n",
    "![Pull example](./images/xcom_4.png)\n",
    "\n",
    "### 3) Wire the tasks in the DAG\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"demo_example\",\n",
    "    start_date=datetime(2023, 3, 24),\n",
    "    schedule=\"@daily\",\n",
    "    catchup=False,\n",
    "    tags=[\"example\"],\n",
    "):\n",
    "    extract_task = PythonOperator(task_id=\"extract_metric\", python_callable=extract_from_api)\n",
    "    print_task   = PythonOperator(task_id=\"print_metric\",   python_callable=print_data)\n",
    "\n",
    "    extract_task >> print_task\n",
    "```\n",
    "\n",
    "![DAG snippet](./images/xcom_4.png)\n",
    "\n",
    "**Tips**\n",
    "- Namespacing: pick unique `key` names per task/metric.\n",
    "- If you only need a *path/ID*, store that in XCom, not the whole dataset.\n",
    "- Inspect XComs in UI: **Admin ‚Üí XComs**.\n",
    "\n",
    "---\n",
    "\n",
    "## Airflow Variables (User-created/Environment)\n",
    "\n",
    "![Variables overview](./images/create-variable_2.png)\n",
    "\n",
    "**Why Variables?**  \n",
    "They let you **externalize configuration** instead of hard-coding values:\n",
    "- Numbers, flags, strings (e.g., `number_post = 20`)\n",
    "- JSON blobs (e.g., `{\"geo\": [\"usa\",\"canada\",\"france\",\"australia\"]}`)\n",
    "\n",
    "**Where to set them?**\n",
    "- **UI:** *Admin ‚Üí Variables*  \n",
    "  ![Create variable form](./images/create_variable.png)\n",
    "  ![Locations example](./images/variable_for_location.png)\n",
    "  ![Number of posts example](./images/number_of_post.png)\n",
    "- **Env vars / secrets backends:** for sensitive values (recommended in prod).\n",
    "\n",
    "**How to read in code**\n",
    "```python\n",
    "from airflow.models import Variable\n",
    "\n",
    "# Scalar\n",
    "num_posts = int(Variable.get(\"number_post\", default_var=\"20\"))\n",
    "\n",
    "# JSON (return as dict/list)\n",
    "locations = Variable.get(\"locations\", deserialize_json=True)\n",
    "# e.g., {\"geo\": [\"usa\",\"canada\",\"france\",\"australia\"]}\n",
    "\n",
    "# Use them in your API calls or logic\n",
    "params = {\"count\": num_posts, \"geo\": locations[\"geo\"][0]}\n",
    "```\n",
    "\n",
    "**Good practices**\n",
    "- Use Variables for **tunable config** (counts, country lists, feature flags).\n",
    "- Prefer **Secrets Backend** (AWS Secrets Manager, GCP Secret Manager, HashiCorp Vault) for credentials.\n",
    "- Document default values in code and provide safe `default_var` fallbacks.\n",
    "\n",
    "---\n",
    "\n",
    "## When to use what?\n",
    "\n",
    "- **XCom** ‚Üí Pass *small* results/metadata across tasks in the *same DAG run* (e.g., IDs, counts, a filename).\n",
    "- **Variables** ‚Üí Store *run-agnostic configuration* controlled from the UI or secrets backend.\n",
    "- **External storage (S3/GCS/DB)** ‚Üí Exchange *large datasets* between tasks; then pass only a reference via XCom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73854738",
   "metadata": {},
   "source": [
    "# Best Practices for Writing Airflow DAGs\n",
    "\n",
    "Reference: [Coursera - Best Practices](https://www.coursera.org/learn/source-systems-data-ingestion-and-pipelines/supplement/VuIX5/best-practices-for-writing-airflow-dags)\n",
    "\n",
    "## Key Points\n",
    "\n",
    "- **Keep tasks simple & atomic**  \n",
    "  Each task should do one thing (e.g., Extract ‚Üí Transform ‚Üí Load).\n",
    "\n",
    "- **Avoid top-level code**  \n",
    "  Only define DAGs and operators in the DAG file.  \n",
    "  Scheduler parses DAGs ~every 30s, so heavy code here is inefficient.\n",
    "\n",
    "- **Use variables instead of hardcoding**  \n",
    "  - Airflow Variables / environment variables for configs  \n",
    "  - Built-in macros like `{{ ds }}` for logical date\n",
    "\n",
    "- **Use Task Groups for readability**  \n",
    "  Group related tasks in the UI for clarity.\n",
    "\n",
    "- **Airflow is an orchestrator, not an executor**  \n",
    "  Heavy data processing should run in Spark, BigQuery, etc.\n",
    "\n",
    "- **Don‚Äôt use XComs for large data**  \n",
    "  Pass references (e.g., file paths, IDs), not entire DataFrames.\n",
    "\n",
    "- **Keep extra code separate**  \n",
    "  Import reusable logic from external Python modules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d769c6c",
   "metadata": {},
   "source": [
    "# TaskFlow API in Airflow\n",
    "\n",
    "## Why TaskFlow API?\n",
    "\n",
    "Traditionally, when writing DAGs in Airflow, we had to:\n",
    "- Manually instantiate a DAG object.\n",
    "- Use operators like `PythonOperator` for Python tasks.\n",
    "- Keep track of multiple names (task_id, Python function name, and task variable).\n",
    "\n",
    "This often made DAGs **verbose** and harder to maintain for Python-heavy workflows.\n",
    "\n",
    "With **TaskFlow API** (introduced in Airflow 2.0):\n",
    "- DAGs are **easier to write** and **more concise**.\n",
    "- We use **decorators** (`@dag` and `@task`) instead of explicitly writing operators.\n",
    "- We reduce the number of names we must manage.\n",
    "\n",
    "---\n",
    "\n",
    "## Traditional Paradigm\n",
    "\n",
    "In the traditional approach:\n",
    "1. Instantiate a `DAG` object with parameters.\n",
    "2. Define tasks with `PythonOperator`.\n",
    "3. Explicitly set dependencies using the `>>` operator.\n",
    "\n",
    "Example:\n",
    "\n",
    "![Traditional Approach](./images/traditional_approach.png)\n",
    "\n",
    "Here you must:\n",
    "- Track `task_id`, function names, and task variables.\n",
    "- Write extra boilerplate code for each Python task.\n",
    "\n",
    "---\n",
    "\n",
    "## TaskFlow API\n",
    "\n",
    "With **TaskFlow API**:\n",
    "1. Use `@dag` decorator to define the DAG.\n",
    "2. Use `@task` decorator for Python functions (they implicitly become tasks).\n",
    "3. Define dependencies simply by calling functions.\n",
    "\n",
    "Example:\n",
    "\n",
    "![TaskFlow API](./images/taskflow_api.png)\n",
    "\n",
    "- The **function name automatically becomes `task_id`**.\n",
    "- `@task` implicitly calls `PythonOperator`.\n",
    "- Code is **shorter and cleaner**.\n",
    "\n",
    "---\n",
    "\n",
    "## Defining Dependencies\n",
    "\n",
    "Dependencies work the same way, but with less boilerplate:\n",
    "\n",
    "![TaskFlow API Dependencies](./images/taskflow_api_2.png)\n",
    "\n",
    "Instead of creating variables for tasks, you just call the decorated functions and chain them with `>>`.\n",
    "\n",
    "---\n",
    "\n",
    "## XCom with Traditional Approach\n",
    "\n",
    "Traditionally, if you wanted tasks to share data, you had to:\n",
    "- Push values into XCom (`xcom_push`).\n",
    "- Pull values from XCom in another task (`xcom_pull`).\n",
    "\n",
    "Example:\n",
    "\n",
    "![Traditional XCom](./images/traditional_xcom.png)\n",
    "\n",
    "---\n",
    "\n",
    "## XCom with TaskFlow API\n",
    "\n",
    "With TaskFlow API, you just use `return` in one task and parameters in another:\n",
    "- Returning a value automatically pushes it into XCom.\n",
    "- Passing arguments automatically pulls it from XCom.\n",
    "\n",
    "Example:\n",
    "\n",
    "![TaskFlow XCom](./images/taskflow_xcom.png)\n",
    "\n",
    "This makes **data passing very natural**, like calling Python functions.\n",
    "\n",
    "---\n",
    "\n",
    "## When to Use TaskFlow API?\n",
    "\n",
    "- When most tasks are **Python functions**.\n",
    "- When you want **simpler, cleaner DAG code**.\n",
    "- When you need to **pass data between tasks** often (XCom via `return` and arguments is much easier).\n",
    "- For new DAGs (recommended by Airflow community).\n",
    "\n",
    "But remember:\n",
    "- TaskFlow API is **not a replacement**. Some operators (like `SqlToS3Operator`, `BashOperator`, etc.) are still needed.\n",
    "- You can **combine TaskFlow API with Traditional operators** in the same DAG.\n",
    "\n",
    "---\n",
    "\n",
    "## Alternative Approach\n",
    "\n",
    "You can still mix both paradigms depending on your use case:\n",
    "\n",
    "![Alternative](./images/alternative.png)\n",
    "\n",
    "---\n",
    "\n",
    "## DAG Definition Comparison\n",
    "\n",
    "Here‚Äôs how DAGs differ between the two approaches:\n",
    "\n",
    "![DAG Comparison](./images/dag_approach.png)\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ Summary\n",
    "- **Traditional Approach** ‚Üí More boilerplate, explicit operators.\n",
    "- **TaskFlow API** ‚Üí Cleaner, easier, more Pythonic.\n",
    "- Use TaskFlow API when possible, especially for Python-heavy workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cd792e",
   "metadata": {},
   "source": [
    "# Orchestration on AWS\n",
    "\n",
    "Now that you‚Äôve gotten practice with **Apache Airflow**, let‚Äôs look at other orchestration options on AWS and understand the trade-offs.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öñÔ∏è Airflow on AWS: Control vs Convenience\n",
    "\n",
    "Running Airflow on AWS can be done in two ways:\n",
    "\n",
    "- **More Control:**  \n",
    "  Run the open-source version of Airflow on **Amazon EC2** or in a **container**.  \n",
    "  - You get full control over configuration and scaling.  \n",
    "  - But you also need to manage the underlying infrastructure, databases, logs, and integrations yourself.  \n",
    "\n",
    "- **More Convenience (Amazon MWAA):**  \n",
    "  Use **Amazon Managed Workflows for Apache Airflow (MWAA)**.  \n",
    "  - AWS provisions and scales the environment for you.  \n",
    "  - It integrates tightly with AWS services:  \n",
    "    - **Amazon CloudWatch** (logs, metrics)  \n",
    "    - **Amazon S3** (DAG storage)  \n",
    "    - **AWS KMS** (encryption)  \n",
    "\n",
    "![Airflow vs MWAA](./images/aws_archii.png)\n",
    "\n",
    "---\n",
    "\n",
    "## üîß AWS Glue Workflows\n",
    "\n",
    "- Specifically designed for **ETL orchestration**.  \n",
    "- Lets you connect **Jobs**, **Crawlers**, and **Triggers** into workflows.  \n",
    "- Triggers can be:  \n",
    "  - On a **schedule**  \n",
    "  - **On-demand**  \n",
    "  - From an **event** (e.g., Amazon EventBridge)  \n",
    "\n",
    "Glue workflows are ideal if your primary task is **building ETL pipelines** on AWS in a serverless way.\n",
    "\n",
    "![AWS Glue Workflows](./images/aws_glue_workflows.png)\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ AWS Step Functions\n",
    "\n",
    "- Orchestrate multiple AWS services into **state machines**.  \n",
    "- A **state** can be:  \n",
    "  - A **Lambda function**  \n",
    "  - A **Glue job**  \n",
    "  - An **ECS task**  \n",
    "- States can also make **decisions**, act on inputs, and pass outputs downstream.  \n",
    "- Best suited for **AWS-centric workflows** that need **serverless orchestration**.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Comparison\n",
    "\n",
    "![AWS Orchestration Options](./images/aws_orchestration.png)\n",
    "\n",
    "- **Airflow:** Maximum flexibility, great for **complex workflows**.  \n",
    "- **AWS Step Functions:** Best for **AWS service integrations**, serverless.  \n",
    "- **AWS Glue Workflows:** Best for **ETL jobs** (crawlers, Glue jobs, triggers).  \n",
    "\n",
    "---\n",
    " "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
