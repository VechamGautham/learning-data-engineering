{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90efe29a",
   "metadata": {},
   "source": [
    "# Table of Contents \n",
    "\n",
    "- [Cron Jobs (Before Orchestration)](#cron-jobs-before-orchestration)\n",
    "- [Evolution of Orchestration tools](#evolution-of-orchestration-tools)\n",
    "- [Orchestration Basics](#orchestration-basics)\n",
    "- [Airflow Core Components](#airflow-core-components)\n",
    "- [Airflow UI](#video-recommendation-learn-the-airflow-ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7763c68",
   "metadata": {},
   "source": [
    "# Cron Jobs (Before Orchestration)\n",
    "\n",
    "![Cron Logo](./images/cron.png)\n",
    "\n",
    "## What is Cron?\n",
    "**Cron** is a Unix/Linux utility (from the 1970s) that automatically runs commands or scripts on a schedule you define.\n",
    "\n",
    "---\n",
    "\n",
    "## How a Cron Job Works\n",
    "A cron job line has **five timing fields** followed by the command:\n",
    "\n",
    "    MINUTE(0â€“59) HOUR(0â€“23) DAY(1â€“31) MONTH(1â€“12) WEEKDAY(0â€“6)  command\n",
    "\n",
    "You can use `*` (asterisk) to mean â€œany valueâ€.\n",
    "\n",
    "**Examples**\n",
    "- Run at midnight on Jan 1 every year:\n",
    "\n",
    "      0 0 1 1 * echo \"Happy New Year\"\n",
    "\n",
    "- Run every night at midnight:\n",
    "\n",
    "      0 0 * * * python ingest_from_rest_api.py\n",
    "\n",
    "![Cron Fields](./images/cron_work.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Before Orchestration: Pure Scheduling with Cron\n",
    "Teams used to chain data pipeline steps by scheduling **multiple cron jobs** a little apart in time so theyâ€™d (hopefully) run in order:\n",
    "\n",
    "- 12:00 AM â†’ ingest API  \n",
    "- 01:00 AM â†’ transform  \n",
    "- 02:00 AM â†’ combine with DB  \n",
    "- 03:00 AM â†’ load to warehouse\n",
    "\n",
    "This is a **pure scheduling approach**â€”no dependency awareness, just timed starts.\n",
    "\n",
    "---\n",
    "\n",
    "## Problems with Pure Cron Scheduling\n",
    "- âŒ No dependency checks (a 1 AM job runs even if the midnight job failed or ran long)  \n",
    "- âŒ Minimal monitoring/alerting; failures often discovered late  \n",
    "- âŒ Debugging & observability are DIY (logs, alerts, retries)  \n",
    "- âŒ Fragile when task durations vary\n",
    "\n",
    "![Pure Scheduling Drawbacks](./images/pure_scheduiling.png)\n",
    "\n",
    "---\n",
    "\n",
    "## When Cron Is Still a Good Fit\n",
    "- âœ… Simple, independent, recurring tasks (backups, log cleanup, small data fetches)  \n",
    "- âœ… Quick prototypes where a full orchestrator is overkill  \n",
    "- âœ… Environments with very light automation needs\n",
    "\n",
    "**Rule of thumb:** If tasks depend on other tasks finishing, or you need retries, backfills, SLAs, or rich monitoringâ€”use an **orchestration tool** (e.g., Airflow, Prefect, Dagster). Otherwise, Cron is perfectly fine for small periodic jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45bb4a9",
   "metadata": {},
   "source": [
    "# Evolution of Orchestration Tools\n",
    "\n",
    "Orchestration is the backbone of modern data engineering, ensuring that complex workflows run in sequence, on time, and with reliability. Letâ€™s walk through its evolution and see how the tooling landscape has shifted over the years.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“œ Early Days â€“ In-House Solutions\n",
    "Before the past decade, orchestration was largely **limited to big tech companies** because:\n",
    "- Open-source or managed orchestration tools didnâ€™t exist.\n",
    "- Building in-house systems was expensive and complicated.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ•° Timeline of Orchestration Tools\n",
    "\n",
    "![Evolution Timeline](./images/evolution.png)\n",
    "\n",
    "- **Late 2000s**:  \n",
    "  - **Facebookâ€™s DataSwarm** â†’ built internally to manage their growing data workflows.\n",
    "  \n",
    "- **2010s**:  \n",
    "  - **Apache Oozie** â†’ became popular, but it was tied to Hadoop clusters, making it less flexible in heterogeneous environments.\n",
    "\n",
    "- **2014**:  \n",
    "  - **Airbnb released Airflow** â†’ inspired by earlier tools like DataSwarm but designed to be **open-source, flexible, and Python-based**.  \n",
    "  - Quickly became the *industry standard*.\n",
    "\n",
    "- **2019**:  \n",
    "  - **Apache Airflow** graduated to a full Apache Software Foundation project, solidifying its role as the most widely adopted orchestration framework.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒŸ Airflow: Advantages & Challenges\n",
    "\n",
    "![Airflow Advantages](./images/adv_airflow.png)\n",
    "\n",
    "**Advantages**\n",
    "- Written in **Python**, making it flexible and widely accessible.  \n",
    "- Very **active open-source community** with frequent commits and bug fixes.  \n",
    "- Available as a **managed service** through providers like AWS, GCP, and Astronomer.  \n",
    "\n",
    "**Challenges**\n",
    "- Struggles with **scalability** for very large workflows.  \n",
    "- Limited built-in support for **data integrity**.  \n",
    "- Lacks **native support for streaming pipelines**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”® Other Open-Source Orchestration Tools\n",
    "\n",
    "![Other Tools](./images/other_orche_tools.png)\n",
    "\n",
    "As the ecosystem evolved, newer tools emerged, aiming to improve on Airflowâ€™s design while addressing its shortcomings:\n",
    "\n",
    "- **Luigi**: Early workflow management tool.  \n",
    "- **Conductor**: Focused on microservice orchestration.  \n",
    "- **Prefect**: More scalable and developer-friendly orchestration system.  \n",
    "- **Dagster**: Adds built-in data quality testing and transformation features.  \n",
    "- **Mage**: Provides integrated data transformation and monitoring.\n",
    "\n",
    "---\n",
    "\n",
    "## âš¡ Example Improvements\n",
    "\n",
    "![Examples of Other Tools](./images/eg_other_orche.png)\n",
    "\n",
    "- **Prefect** â†’ More scalable than Airflow, making it a good fit for heavy workloads.  \n",
    "- **Dagster & Mage** â†’ Bring built-in **data quality testing** and transformation features, helping ensure correctness beyond just scheduling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104ef677",
   "metadata": {},
   "source": [
    "# Orchestration Basics\n",
    "\n",
    "## Why Orchestration?\n",
    "Orchestration is about managing complex **data pipelines** more reliably than simple Cron jobs. While Cron can schedule tasks, orchestration tools give you advanced control over dependencies, monitoring, alerts, and fallback plans.\n",
    "\n",
    "**Pros:**\n",
    "- Set up dependencies between tasks  \n",
    "- Monitor task execution  \n",
    "- Get alerts on failures  \n",
    "- Create fallback plans  \n",
    "\n",
    "**Cons:**\n",
    "- Adds more operational overhead compared to simple Cron scheduling  \n",
    "\n",
    "![Pros and Cons](./images/pro_con.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Directed Acyclic Graphs (DAGs)\n",
    "At the heart of orchestration is the **Directed Acyclic Graph (DAG)** â€” a structure where:\n",
    "- Each **task** is a node  \n",
    "- Each **arrow** (edge) shows data flow  \n",
    "- Data flows **only in one direction**  \n",
    "- No loops or cycles are allowed  \n",
    "\n",
    "This ensures predictable execution order.\n",
    "\n",
    "![DAG](./images/dag.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Dependencies Between Tasks\n",
    "With Cron, tasks could overlap or break if one runs late.  \n",
    "With orchestration, you can **define dependencies**: a task wonâ€™t start until its upstream tasks are complete.\n",
    "\n",
    "![Task Dependencies](./images/task_based.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Orchestration in Airflow\n",
    "In Airflow, DAGs are defined in Python. You programmatically specify tasks and how they depend on each other.\n",
    "\n",
    "![Code Example](./images/code_for_orchestration_.png)\n",
    "\n",
    "Airflow then lets you:\n",
    "- Visualize DAGs  \n",
    "- Trigger runs manually or on schedule  \n",
    "- Monitor progress & debug issues  \n",
    "\n",
    "---\n",
    "\n",
    "## Time-based vs Event-based Triggers\n",
    "Airflow DAGs can run on **time-based schedules** (like Cron) or be triggered by **events** (like new data arriving).\n",
    "\n",
    "![Time or Event Based](./images/time_or_event_based.png)\n",
    "\n",
    "### Example: Time-based\n",
    "Run daily at midnight:  \n",
    "![Example Time-based](./images/eg_time_based.png)\n",
    "\n",
    "### Example: Event-based\n",
    "Run when a dataset updates:  \n",
    "![Example Event-based](./images/eg_event_based.png)\n",
    "\n",
    "You can even make **part of a DAG** wait for an external event, e.g., a file landing in S3.  \n",
    "\n",
    "![External Flow](./images/external_flow.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Data Quality Checks\n",
    "Another orchestration benefit: embedding **data quality checks** into the DAG.  \n",
    "For example:\n",
    "- Count of null values  \n",
    "- Validating ranges of values  \n",
    "- Schema verification  \n",
    "\n",
    "![Quality Checks](./images/quality_checks.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "Orchestration tools (like Airflow) provide the structure to:\n",
    "- Define pipelines as **DAGs**  \n",
    "- Manage **dependencies** between tasks  \n",
    "- Run on **time or event conditions**  \n",
    "- Monitor and alert on failures  \n",
    "- Enforce **data quality**  \n",
    "\n",
    "Though more complex than Cron, orchestration is essential for reliable and scalable data engineering workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d85b88",
   "metadata": {},
   "source": [
    "# Airflow Core Components\n",
    "\n",
    "Airflow is built around a set of core components that work together to run your DAGs (Directed Acyclic Graphs), monitor dependencies, execute tasks, and display status updates to users.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”‘ Core Components of Airflow\n",
    "\n",
    "The main components of Airflow are:\n",
    "\n",
    "- **Web Server** â†’ Hosts the Airflow **User Interface (UI)**.  \n",
    "- **Scheduler** â†’ Monitors DAGs and determines when tasks should run.  \n",
    "- **Workers** â†’ Execute the tasks that are scheduled.  \n",
    "- **Metadata Database** â†’ Stores the state of DAGs and tasks (success, failure, etc.).  \n",
    "- **DAG Directory** â†’ Stores Python scripts that define your DAGs.\n",
    "\n",
    "All these components are essential parts of an Airflow environment, whether you install it directly or use a managed service like MWAA.\n",
    "\n",
    "![Airflow Components](./images/airflow_components.png)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ–¥ï¸ User Interaction with Airflow\n",
    "\n",
    "- You (the user) write **Python scripts** to define DAGs and place them in the **DAG Directory**.  \n",
    "- These DAGs automatically appear in the **Web Server UI**, where you can:\n",
    "  - Visualize DAGs  \n",
    "  - Monitor tasks  \n",
    "  - Trigger DAGs manually  \n",
    "  - Troubleshoot issues  \n",
    "\n",
    "Thus, the DAG directory + user interface are the **main interaction points** for users, while the other components work in the background.\n",
    "\n",
    "![User Interaction](./images/user_interaction.png)\n",
    "\n",
    "---\n",
    "\n",
    "## â° Scheduling with the Scheduler\n",
    "\n",
    "- The **Scheduler** runs every minute by default.  \n",
    "- It checks all DAGs in the DAG Directory and determines:\n",
    "  - If a task should be triggered by time (schedule-based)  \n",
    "  - If a taskâ€™s dependencies are complete (dependency-based)  \n",
    "- Once ready, the **Scheduler**:\n",
    "  1. Pushes tasks into a queue  \n",
    "  2. Uses an **Executor** to extract tasks from the queue  \n",
    "  3. Sends the tasks to **Workers**, which run them  \n",
    "\n",
    "As tasks move through this process, their status transitions from:  \n",
    "`schedule â†’ queued â†’ running â†’ success/failed`.\n",
    "\n",
    "![Scheduler Workflow](./images/scheduler_workflow.png)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š Task Status and Metadata Database\n",
    "\n",
    "- The **Scheduler** and **Workers** update the **Metadata Database** with task status and DAG states.  \n",
    "- The **Web Server** then queries the metadata database to extract these statuses.  \n",
    "- Finally, the **UI** displays task states to the user.  \n",
    "\n",
    "This is why you can see real-time updates of task states (like running, success, or failed) in the Airflow UI.\n",
    "\n",
    "![Task Status Workflow](./images/status_workflow.png)\n",
    "\n",
    "---\n",
    "\n",
    "## â˜ï¸ Managed Workflows for Apache Airflow (MWAA)\n",
    "\n",
    "Amazon provides a managed service called **MWAA** (Managed Workflows for Apache Airflow), which automatically sets up and manages all Airflow components for you.\n",
    "\n",
    "In MWAA:\n",
    "\n",
    "- **DAG Directory** â†’ Stored in **Amazon S3**.  \n",
    "- **Metadata Database** â†’ Hosted on **Amazon Aurora PostgreSQL**.  \n",
    "- **Schedulers, Workers, Web Server** â†’ Managed as AWS services inside a secure **VPC**.  \n",
    "- Additional integrations â†’ AWS CloudWatch for logging, SQS for queuing, and ECR for containers.\n",
    "\n",
    "This allows you to use Airflow at scale without worrying about manually configuring or maintaining its infrastructure.\n",
    "\n",
    "![MWAA](./images/mwaa.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaa2eb3",
   "metadata": {},
   "source": [
    "# Video Recommendation: Learn the Airflow UI\n",
    "\n",
    "> Refer to this video to understand the Airflow UI end-to-end:\n",
    "\n",
    "**[Apache Airflow UI Tour | Airflow UI Walkthrough for Beginners](https://www.youtube.com/watch?v=sMIW8dLjzRU)**\n",
    "\n",
    "## What youâ€™ll learn\n",
    "- Navigating DAGs: **Grid**, **Graph**, **Gantt**, **Tree** views  \n",
    "- Triggering DAG runs, **pausing/unpausing**, filtering & tags  \n",
    "- Inspecting **Task Instance** details: logs, retries, XComs  \n",
    "- Monitoring run states and understanding status colors  \n",
    "- Viewing code, variables, connections, and admin panels\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
