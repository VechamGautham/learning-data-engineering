{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bce03ec",
   "metadata": {},
   "source": [
    "# 📦 Source Systems in Data Engineering\n",
    "\n",
    "In data engineering, the **first step** in the lifecycle is obtaining data from various **source systems**. These are the systems where raw data originates and flows into your pipeline for processing and analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 What Are Source Systems?\n",
    "\n",
    "Source systems are where your data comes from. As a **data engineer**, you don't typically own these systems — they are created and maintained by other teams like software developers, third-party vendors, or partner platforms.\n",
    "\n",
    "Your job is to build **pipelines** that consume data from these sources and deliver it to **downstream systems** like dashboards, machine learning models, or data warehouses.\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Common Types of Source Systems\n",
    "\n",
    "| Type             | Description                                                                 | Real-World Example                                 |\n",
    "|------------------|-----------------------------------------------------------------------------|----------------------------------------------------|\n",
    "| **Databases**     | Structured data organized into tables or documents                         | Sales transactions from an e-commerce app          |\n",
    "| **Files**         | Unstructured data like CSVs, MP3s, images                                   | Product catalog stored in `products.csv`           |\n",
    "| **APIs**          | On-demand data accessed over the web                                       | Twitter API providing trending hashtags            |\n",
    "| **IoT Devices**   | Real-time data streamed from connected devices                              | GPS trackers on delivery vehicles                  |\n",
    "| **Data Sharing Platforms** | External datasets provided by other organizations                | AWS Data Exchange sharing market research files    |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Why Understanding Source Systems Is Important\n",
    "\n",
    "- Source systems are **not in your control**.\n",
    "- They can **fail**, change format/schema, or update without notice.\n",
    "- If you rely on unstable source systems without planning, your downstream pipelines may **break** silently.\n",
    "\n",
    "> Example: A software team renames or deletes columns in their database without telling you. Your pipeline crashes because it expects columns that no longer exist.\n",
    "\n",
    "---\n",
    "\n",
    "## 🤝 Best Practices\n",
    "\n",
    "- **Collaborate** with the owners of source systems\n",
    "- Understand how the data is **generated and updated**\n",
    "- Know what can **change** — and when\n",
    "- Design pipelines to be **resilient** to schema or format changes\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Analogy\n",
    "\n",
    "> Imagine you're a chef (data engineer). Your ingredients (data) come from various suppliers (source systems). One day, a supplier changes packaging or skips delivery without notice — your kitchen operations (pipelines) are disrupted unless you're prepared and have good communication.\n",
    "\n",
    "---\n",
    "\n",
    "## 📷 Diagram: Source Systems → Downstream Systems\n",
    "\n",
    "![Source Systems Flow](./image/source_systems.png)\n",
    "\n",
    "In the diagram above:\n",
    "- The left side shows **data sources** like databases, files, APIs, IoT, and data sharing platforms.\n",
    "- These sources **deliver data** through pipelines.\n",
    "- The right side shows **downstream systems** that consume the processed data.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Summary\n",
    "\n",
    "- **Source systems** are external systems where your pipeline starts.\n",
    "- They include databases, APIs, files, IoT devices, and more.\n",
    "- As a data engineer, your success depends on **understanding, monitoring, and adapting** to these systems.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b7de79",
   "metadata": {},
   "source": [
    "# 🔄 Data Ingestion in Data Engineering\n",
    "\n",
    "## 1. Source Systems\n",
    "\n",
    "In the first stage of the data engineering lifecycle, data originates from **source systems**. These are systems that generate or hold the raw data we want to work with.\n",
    "\n",
    "### Common Source Systems:\n",
    "\n",
    "- **Databases** (e.g., sales or customer databases)\n",
    "- **Files** (CSV, JSON, audio, video)\n",
    "- **APIs** (e.g., Twitter API, product data API)\n",
    "- **IoT Devices** (e.g., GPS trackers, sensors)\n",
    "- **Data Sharing Platforms** (e.g., internal dashboards, 3rd-party datasets)\n",
    "\n",
    "These systems may be maintained by:\n",
    "- Other internal teams (e.g., backend engineers)\n",
    "- External vendors\n",
    "- Partner organizations\n",
    "\n",
    "As a data engineer, you don’t control these systems—but your pipelines depend on their structure and consistency.\n",
    "\n",
    "> 🔄 *Analogy:* Think of source systems as different taps (faucets) from which water (data) flows into a treatment plant (your data pipeline). You don’t control how the taps are built, but you must design your plant to handle the water reliably.\n",
    "\n",
    "### 📌 Image: Source Systems Diagram\n",
    "\n",
    "![Source Systems](./images/source_systems.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Frequency of Ingestion\n",
    "\n",
    "Once the data source is identified, you need to decide how **frequently** data should be ingested from it:\n",
    "\n",
    "- **Batch Ingestion**: Collect and move large chunks of data periodically (e.g., every hour/day).\n",
    "- **Streaming Ingestion**: Capture and process events/data in near real-time.\n",
    "\n",
    "> 💡 *Example:*  \n",
    "> - Batch: Moving website logs daily into a warehouse for weekly analysis.  \n",
    "> - Stream: Capturing user clicks in real-time to recommend products instantly.\n",
    "\n",
    "> 📦 *Analogy:*  \n",
    "> - **Batch**: Like collecting mail once a day from a mailbox.  \n",
    "> - **Streaming**: Like receiving WhatsApp messages instantly as they come in.\n",
    "\n",
    "### 📌 Image: Frequency of Ingestion\n",
    "\n",
    "![Frequency of Ingestion](./image/ingestion_frequency.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Batch Ingestion\n",
    "\n",
    "**Batch ingestion** means pulling data in chunks at scheduled times or after a set data size.\n",
    "\n",
    "- Often used in analytics and model training\n",
    "- Simple and resource-efficient\n",
    "- Great when real-time updates are not critical\n",
    "\n",
    "> 🕒 Example: Pulling transaction records from a POS (Point of Sale) system every night at 2 AM for analysis.\n",
    "\n",
    "### 📌 Image: Batch Ingestion\n",
    "\n",
    "![Batch Ingestion](./image/batch_ingestion.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Streaming and Batch Together\n",
    "\n",
    "Most real-world systems use a combination of batch and streaming ingestion.\n",
    "\n",
    "- **Batch Use Case**: Train ML models on historical data\n",
    "- **Streaming Use Case**: Detect fraud or anomalies in real time\n",
    "\n",
    "You can stream data continuously, store it, and periodically run batch processes on the stored data.\n",
    "\n",
    "### 📌 Image: Streaming and Batch Components\n",
    "\n",
    "![Streaming and Batch Components](./image/streaming_and_batch_ingestion.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Streaming Ingestion Internals\n",
    "\n",
    "Streaming ingestion uses tools like:\n",
    "- **Apache Kafka**\n",
    "- **Amazon Kinesis**\n",
    "- **Google Pub/Sub**\n",
    "\n",
    "These tools ingest continuous data and forward it to processing layers with minimal delay (seconds or milliseconds).\n",
    "\n",
    "> ⏱ *Example:* Streaming GPS signals from a fleet of delivery trucks to track their location live.\n",
    "\n",
    "### 📌 Image: Streaming Ingestion Architecture\n",
    "\n",
    "![Streaming Ingestion](./image/streaming_ingestion.png)\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Summary\n",
    "\n",
    "- Source systems are where data originates.\n",
    "- Batch is great for periodic, reliable ingestion.\n",
    "- Streaming is useful when near real-time decisions are needed.\n",
    "- Most pipelines use a hybrid of both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79ad8ca",
   "metadata": {},
   "source": [
    "# 📦 Understanding Storage in Data Engineering\n",
    "\n",
    "Data is constantly being created, moved, and stored — whether on your laptop, phone, or in massive cloud systems. As a data engineer, your ability to manage this data depends on how well you understand different **layers of storage**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 1. Raw Ingredients of Storage\n",
    "\n",
    "These are the **fundamental hardware and processes** that make all storage possible.\n",
    "\n",
    "- **Physical components**:\n",
    "  - 💽 Hard Disks (HDD) – cheap and large, but slow.\n",
    "  - ⚡ Solid State Drives (SSD) – faster, more costly.\n",
    "  - 🧠 RAM – very fast but volatile and expensive.\n",
    "\n",
    "- **Software-level processes**:\n",
    "  - Networking\n",
    "  - CPU operations\n",
    "  - Serialization\n",
    "  - Compression\n",
    "  - Caching\n",
    "\n",
    "📸 *Raw ingredients of storage:*\n",
    "\n",
    "![Raw Ingredients](./image/storage.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 🗃️ 2. Storage Systems\n",
    "\n",
    "Built on top of raw ingredients, these systems **organize, store, and manage access** to data.\n",
    "\n",
    "- **Database Management Systems** – For structured data (e.g., PostgreSQL)\n",
    "- **Object Storage** – For blobs/files (e.g., Amazon S3)\n",
    "- **Apache Iceberg/Hudi** – For handling big data tables\n",
    "- **Cache/Memory Systems** – e.g., Redis\n",
    "- **Streaming Storage** – e.g., Kafka for real-time data streams\n",
    "\n",
    "📸 *Common storage systems:*\n",
    "\n",
    "![Storage Systems](./image/storage_systems.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 🧱 3. Storage Abstractions\n",
    "\n",
    "These are **combinations of storage systems** that serve higher-level business needs.\n",
    "\n",
    "- **Data Warehouse** – Optimized for fast queries (e.g., Snowflake, BigQuery)\n",
    "- **Data Lake** – Stores all types of raw data (e.g., AWS S3)\n",
    "- **Data Lakehouse** – Hybrid of the above two (e.g., Databricks Delta Lake)\n",
    "\n",
    "📸 *Types of storage abstractions:*\n",
    "\n",
    "![Storage Abstractions](./image/storage_abstractions.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 🪜 4. Storage Hierarchy Overview\n",
    "\n",
    "Storage can be visualized as a 3-layer hierarchy:\n",
    "\n",
    "1. **Raw Ingredients** – HDDs, SSDs, RAM, Networking\n",
    "2. **Storage Systems** – Databases, object stores, caches\n",
    "3. **Storage Abstractions** – Warehouses, Lakes, Lakehouses\n",
    "\n",
    "As a data engineer, you often work with **abstractions**, but understanding the **lower levels** makes your system faster, cheaper, and more scalable.\n",
    "\n",
    "📸 *Full storage hierarchy:*\n",
    "\n",
    "![Storage Hierarchy](./image/storage_hierarchy.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Summary\n",
    "\n",
    "- You interact with storage systems constantly, even without realizing it.\n",
    "- Storage involves hardware, software, and smart architecture.\n",
    "- **Efficiency and cost** are determined by how well you choose your storage strategy.\n",
    "- Always understand where your data is going and how it's stored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ccc591",
   "metadata": {},
   "source": [
    "# 💾 In-Depth Look: How HDD, SSD, and RAM Actually Work\n",
    "\n",
    "Understanding the **physical mechanisms** behind how data is stored and accessed helps data engineers design systems that are optimized for performance, reliability, and cost.\n",
    "\n",
    "---\n",
    "\n",
    "## 💽 Hard Disk Drives (HDDs)\n",
    "\n",
    "### 🔧 What Happens Inside?\n",
    "\n",
    "An HDD consists of:\n",
    "- **Spinning platters** coated with magnetic material.\n",
    "- A **read/write head** mounted on an actuator arm.\n",
    "- A **motor** that spins the platters at 5400–7200 RPM (or higher).\n",
    "- **Firmware** that controls operations.\n",
    "\n",
    "### 🧲 How Is Data Stored?\n",
    "\n",
    "- Data is stored magnetically in **tiny regions** on the platter called **magnetic domains**.\n",
    "- These regions can be **magnetized in one of two directions**:\n",
    "  - One direction = binary `1`\n",
    "  - Opposite direction = binary `0`\n",
    "- The platter is divided into:\n",
    "  - **Tracks** (concentric circles)\n",
    "  - **Sectors** (segments of tracks)\n",
    "  - **Cylinders** (aligned tracks across platters)\n",
    "\n",
    "### ✍️ Writing Data (Engraving Analogy):\n",
    "\n",
    "- The **write head** generates a magnetic field.\n",
    "- It **flips the magnetic direction** of a region to represent 1s and 0s.\n",
    "- This is similar to an **engraving tool carving marks** on a rotating disc — the tool (head) needs to be **positioned precisely** to write data.\n",
    "\n",
    "### 🔍 Reading Data:\n",
    "\n",
    "- The **read head** senses the magnetic polarity of each domain as the platter spins.\n",
    "- The magnetic change is translated into a stream of binary data.\n",
    "\n",
    "### 🧠 Summary:\n",
    "- ❌ Slower due to mechanical movement.\n",
    "- ✅ Great for large capacity, cheap storage.\n",
    "- ⚠️ Fragile — can be damaged by shocks.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚡ Solid State Drives (SSDs)\n",
    "\n",
    "### 🔧 What Happens Inside?\n",
    "\n",
    "An SSD contains:\n",
    "- **NAND Flash memory chips** (non-volatile)\n",
    "- **Controller** chip to manage read/write operations\n",
    "- No moving parts\n",
    "\n",
    "### 🧠 How Data Is Stored (Electric Charge):\n",
    "\n",
    "- Data is stored in **floating-gate transistors**, which **trap electrons**.\n",
    "- Electrons inside the gate represent binary `1`, and absence of charge is binary `0`.\n",
    "- These transistors are organized into:\n",
    "  - **Cells** (SLC, MLC, TLC depending on how many bits per cell)\n",
    "  - **Pages** (group of cells)\n",
    "  - **Blocks** (group of pages)\n",
    "\n",
    "### ✍️ Writing Data (Charge Manipulation):\n",
    "\n",
    "- To write, a **voltage is applied** to \"trap\" electrons in a floating gate.\n",
    "- To erase, the **voltage releases the trapped electrons**.\n",
    "- This is **slower than reading**, and **blocks must be erased before being rewritten**.\n",
    "\n",
    "### 🔍 Reading Data:\n",
    "\n",
    "- A small voltage is applied.\n",
    "- The presence or absence of current flow tells whether the bit is 0 or 1.\n",
    "\n",
    "### 📦 Summary:\n",
    "- ✅ Fast access, no moving parts.\n",
    "- ⚠️ Limited write cycles — cells wear out over time.\n",
    "- 💸 More expensive than HDDs.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧬 Random Access Memory (RAM)\n",
    "\n",
    "### 🔧 What Happens Inside?\n",
    "\n",
    "RAM consists of:\n",
    "- Millions of **capacitor-transistor pairs**\n",
    "- Each **pair stores one bit** of data\n",
    "- Volatile — **requires constant power** to retain data\n",
    "\n",
    "### ⚡ How Data Is Stored (Capacitor Charging):\n",
    "\n",
    "- A **charged capacitor** = binary `1`\n",
    "- A **discharged capacitor** = binary `0`\n",
    "- Transistors act like gates that allow read/write access to the capacitor\n",
    "\n",
    "### 🔍 Reading Data:\n",
    "\n",
    "- The system checks whether a capacitor holds charge.\n",
    "- This check **discharges the capacitor**, so RAM must **refresh data** constantly (thousands of times per second).\n",
    "\n",
    "### ✍️ Writing Data:\n",
    "\n",
    "- A transistor opens a path to the capacitor.\n",
    "- A voltage is applied to **store charge** (1) or **drain it** (0).\n",
    "\n",
    "### 🧠 Summary:\n",
    "- 🚀 Extremely fast (nanoseconds latency)\n",
    "- ❌ Volatile (data is lost when power goes off)\n",
    "- ✅ Perfect for temporary processing (e.g., active variables in programs)\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Final Comparison (Physically)\n",
    "\n",
    "| Feature          | HDD                            | SSD                                 | RAM                                 |\n",
    "|------------------|---------------------------------|--------------------------------------|--------------------------------------|\n",
    "| Storage Method   | Magnetic domains on platters   | Floating-gate transistors            | Charge in capacitors                 |\n",
    "| Moving Parts     | Yes                             | No                                   | No                                   |\n",
    "| Read/Write Speed | Slow (ms)                       | Fast (μs)                            | Very Fast (ns)                       |\n",
    "| Volatile?        | No                              | No                                   | Yes                                  |\n",
    "| Use Case         | Archive, backups                | OS, active pipelines                 | In-memory compute, temp storage      |\n",
    "| Failure Risk     | Higher (mechanical)             | Lower (but finite write endurance)   | Data lost on power loss              |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Analogy Recap\n",
    "\n",
    "- **HDD** = Like a record player writing grooves on a vinyl disc.\n",
    "- **SSD** = Like a chalkboard where you \"charge\" and \"discharge\" cells.\n",
    "- **RAM** = Like a whiteboard used for calculations — fast, but wiped clean when power is off.\n",
    "\n",
    "---\n",
    "\n",
    "Understanding these differences allows you to pick the right type of storage for your data engineering architecture — whether you're dealing with **hot data**, **cold data**, or **high-speed temporary computation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f4bb2a",
   "metadata": {},
   "source": [
    "## 🔄 Data Transformation in the Data Engineering Lifecycle\n",
    "\n",
    "The **transformation stage** is where a data engineer starts to deliver **real business value**. While ingesting and storing raw data is important, it doesn't directly help downstream users like analysts or data scientists.\n",
    "\n",
    "Transformation is the stage where **raw data is turned into something useful**.\n",
    "\n",
    "### 👥 Who Benefits from Transformation?\n",
    "\n",
    "- **Business Analysts**: They might need quick access to clean, structured data like `customer_id`, `product_name`, `quantity`, and `time_of_sale` to generate reports.\n",
    "- **Data Scientists / ML Engineers**: They rely on you to prepare features and cleaned datasets for model training.\n",
    "\n",
    "Transformation includes **3 major components**:\n",
    "- Queries\n",
    "- Modeling\n",
    "- Transformation logic\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Queries\n",
    "\n",
    "A **query** is simply a request to read records from a database or other storage system. SQL is the most commonly used query language.\n",
    "\n",
    "> Poorly written queries can slow down performance, overload databases, or even crash your infrastructure (e.g., row explosion from bad joins).\n",
    "\n",
    "#### 📘 SQL Commands in Transformation\n",
    "\n",
    "- **Data Cleaning**: `DROP`, `TRUNCATE`, `TRIM`, `REPLACE`, `SELECT DISTINCT`\n",
    "- **Data Joining**: `INNER JOIN`, `LEFT JOIN`, `RIGHT JOIN`, `FULL JOIN`, `UNION`\n",
    "- **Data Aggregating**: `SUM`, `AVG`, `COUNT`, `MAX`, `MIN`, `GROUP BY`\n",
    "- **Data Filtering**: `WHERE`, `AND`, `OR`, `IS NULL`, `IS NOT NULL`, `IN`, `LIKE`\n",
    "\n",
    "![Query Commands](./image//sql_commands.png)\n",
    "\n",
    "---\n",
    "\n",
    "### 📐 Data Modeling\n",
    "\n",
    "Data modeling involves choosing the **right structure** to represent data for business needs.\n",
    "\n",
    "- If data comes from **normalized relational databases** (separate tables for orders, products, customers), you may need to **denormalize** it for faster analytics.\n",
    "- Example: A business analyst shouldn't need to join five tables to get product sales data.\n",
    "\n",
    "Good models reflect:\n",
    "- Business logic\n",
    "- Terminology (e.g., how different departments define “customer”)\n",
    "- Reporting or ML requirements\n",
    "\n",
    "You'll learn more about **normalization** and **data modeling** later in the specialization.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 Transformation Logic\n",
    "\n",
    "Transformation happens **across multiple stages** of the pipeline:\n",
    "\n",
    "- At the **source system**: timestamps or metadata added\n",
    "- During **ingestion**: data type mapping, standardization\n",
    "- In **streaming pipelines**: records enriched or calculated\n",
    "- Before **machine learning**: features engineered\n",
    "- Before **reporting**: aggregation, schema reshaping\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 Examples of Transformation Use Cases\n",
    "\n",
    "#### 🧑‍💻 Business Analyst\n",
    "\n",
    "Goal: Generate daily sales reports\n",
    "\n",
    "![Transformation for Analyst](./image//transformation.png)\n",
    "\n",
    "---\n",
    "\n",
    "#### 👩‍🔬 Data Scientist\n",
    "\n",
    "Goal: Use transformed data for predictive analytics\n",
    "\n",
    "![Transformation for DS](./image/transformation_ds.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eceaf6c",
   "metadata": {},
   "source": [
    "# Serving Data in the Data Engineering Lifecycle\n",
    "\n",
    "Once you've ingested, transformed, and stored your data, you're ready for the final stage of the data engineering lifecycle: **serving**. This is when your work directly creates business value by enabling stakeholders to consume and act on the data.\n",
    "\n",
    "Serving isn't a one-size-fits-all process—it depends on the use case. Let’s break it down.\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Analytics\n",
    "\n",
    "Analytics is about identifying patterns and insights from data. There are 3 main types:\n",
    "\n",
    "### 1. Business Intelligence (BI)\n",
    "\n",
    "- **Example**: The marketing team wants to see **weekly signup trends** from different cities.\n",
    "- **You as a Data Engineer**:\n",
    "  - Ingest user signup logs from the website.\n",
    "  - Transform the data (extract city names, dates, counts).\n",
    "  - Store in a clean reporting table.\n",
    "  - Serve it via a BI dashboard (e.g., Tableau or Looker).\n",
    "\n",
    "### 2. Operational Analytics\n",
    "\n",
    "- **Example**: An e-commerce website wants to **track orders per minute** to detect site crashes or slowdowns.\n",
    "- **You as a Data Engineer**:\n",
    "  - Build a real-time streaming pipeline using tools like Apache Kafka + Spark.\n",
    "  - Transform the incoming order events.\n",
    "  - Push the metrics to a live dashboard.\n",
    "  - Set up alerts for low or zero activity.\n",
    "\n",
    "### 3. Embedded Analytics\n",
    "\n",
    "- **Example**: A food delivery app shows customers their **monthly spend** and **top restaurants**.\n",
    "- **You as a Data Engineer**:\n",
    "  - Join order history, prices, and restaurant data.\n",
    "  - Aggregate spend per month.\n",
    "  - Provide a real-time or scheduled API or dataset to the app team for embedding.\n",
    "\n",
    "![Analytics](./image/analytics.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 🤖 Machine Learning\n",
    "\n",
    "Machine learning requires serving data for model training, inference, and tracking.\n",
    "\n",
    "### Example: Product Recommendation System\n",
    "\n",
    "- **Goal**: Recommend products to users based on their past purchases.\n",
    "- **You as a Data Engineer**:\n",
    "  - Ingest user purchase logs.\n",
    "  - Extract features like categories purchased, price range, time of day.\n",
    "  - Store in a **feature store** (a clean structured table).\n",
    "  - Serve this data for:\n",
    "    - **Training** the ML model.\n",
    "    - **Real-time inference** when a user visits the site.\n",
    "    - Track when the model was trained and with which data (lineage).\n",
    "\n",
    "![Machine Learning](./image/ml.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Reverse ETL\n",
    "\n",
    "Reverse ETL = Sending cleaned and enhanced data **back to source tools** like CRMs or ad platforms.\n",
    "\n",
    "### Example: Lead Scoring in CRM\n",
    "\n",
    "- **Goal**: Prioritize customers who are most likely to buy.\n",
    "- **You as a Data Engineer**:\n",
    "  - Ingest CRM data (names, interactions).\n",
    "  - Transform into features (e.g., # of visits, email opens).\n",
    "  - Data scientist trains a **lead score model**.\n",
    "  - Serve the lead scores **back into the CRM** so the sales team sees them next to each client’s profile.\n",
    "\n",
    "### Another Simple Example: Email Targeting\n",
    "\n",
    "- Your marketing team wants to send emails only to users **who haven’t logged in for 30 days**.\n",
    "- You write a job that finds those users from your warehouse.\n",
    "- Then push their emails **back to the email platform** (like Mailchimp or HubSpot).\n",
    "\n",
    "![Reverse ETL](./image/reverse_etl.png)\n",
    "\n",
    "---\n",
    "\n",
    "This final stage—**serving**—is where data becomes **useful** and **visible** to the business. Whether it's through dashboards, apps, machine learning models, or external systems, your job is to **deliver clean, usable, timely data** to wherever it's needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e9d629",
   "metadata": {},
   "source": [
    "## 🌊 Undercurrents of the Data Engineering Lifecycle\n",
    "\n",
    "In data engineering, **undercurrents** are foundational themes that apply across all stages of the lifecycle — from ingestion and transformation to storage and serving.\n",
    "\n",
    "They’re not specific steps but **core disciplines** that support and strengthen your entire data system.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔐 1. Security\n",
    "\n",
    "Security ensures data is protected from unauthorized access, leaks, and misuse.\n",
    "\n",
    "- Apply **least privilege access**\n",
    "- Use **IAM**, encryption, and secure network protocols\n",
    "- Prevent security breaches due to human error or misconfigurations\n",
    "- Build a **culture of security**, not just compliance\n",
    "\n",
    "---\n",
    "\n",
    "### 🗂️ 2. Data Management\n",
    "\n",
    "Effective data management ensures your data is organized, traceable, and high quality.\n",
    "\n",
    "- Maintain **metadata catalogs**\n",
    "- Enable **data lineage** tracking (where the data came from and how it changed)\n",
    "- Handle **schema evolution** as data changes over time\n",
    "- Keep audit logs for compliance and monitoring\n",
    "\n",
    "---\n",
    "\n",
    "### 🏗️ 3. Data Architecture\n",
    "\n",
    "Data architecture is the blueprint of how your data flows and is structured.\n",
    "\n",
    "- Choose between **batch** or **streaming**\n",
    "- Design **data lakes**, **data warehouses**, or **lakehouses**\n",
    "- Select the right **storage formats** (e.g., Parquet, Avro, Delta)\n",
    "- Structure data for performance, cost, and scalability\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 4. DataOps & Orchestration\n",
    "\n",
    "This undercurrent focuses on operationalizing and automating data workflows.\n",
    "\n",
    "- Use tools like **Apache Airflow**, **Dagster**, or **Prefect**\n",
    "- Define **data pipelines** as code\n",
    "- Monitor pipelines and handle failures gracefully\n",
    "- Enable **continuous integration/deployment** for data workflows\n",
    "\n",
    "---\n",
    "\n",
    "### 🧑‍💻 5. Software Engineering Practices\n",
    "\n",
    "Data engineers are also software engineers.\n",
    "\n",
    "- Write **modular, testable, reusable** code\n",
    "- Use **version control** (Git), code reviews, and CI/CD pipelines\n",
    "- Document data systems and code\n",
    "- Follow testing best practices (unit tests, integration tests)\n",
    "\n",
    "---\n",
    "\n",
    "These undercurrents are not isolated — they all work together to build resilient, secure, scalable, and efficient data platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b5fbe7",
   "metadata": {},
   "source": [
    "## 🔐 Security\n",
    "\n",
    "As a data engineer, you work with sensitive and valuable data. You're responsible for making sure this data is safe — from both external attackers and internal misuse.\n",
    "\n",
    "### 🧠 Defensive Mindset\n",
    "\n",
    "Security starts with your behavior:\n",
    "\n",
    "- **Be cautious with sensitive data** — never share passwords or credentials casually.\n",
    "- **Design for potential attacks** — expect phishing, impersonation, and social engineering attempts.\n",
    "\n",
    "![Defensive Mindset](./image/defensive_mindset.png)\n",
    "\n",
    "---\n",
    "\n",
    "### ☁️ Security in the Cloud\n",
    "\n",
    "Most modern pipelines are cloud-native. That means understanding cloud security is essential.\n",
    "\n",
    "#### Key Concepts:\n",
    "\n",
    "- **IAM (Identity and Access Management):**  \n",
    "  Control who can access what, and under what conditions.\n",
    "\n",
    "- **Encryption Methods:**  \n",
    "  Encrypt data at rest and in transit.\n",
    "\n",
    "- **Networking Protocols:**  \n",
    "  Use secure networks, VPNs, and firewalls.\n",
    "\n",
    "📌 Example:  \n",
    "Only let specific users run queries on sensitive tables using IAM roles. Encrypt backups using KMS.\n",
    "\n",
    "![Security in the Cloud](./image/security_in_cloud.png)\n",
    "\n",
    "---\n",
    "\n",
    "### 🛑 Principle of Least Privilege\n",
    "\n",
    "Only give users the minimum access they need — and only for as long as they need it.\n",
    "\n",
    "📌 Example:  \n",
    "A dashboard user shouldn’t have write access to raw production data.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧯 Avoid Unnecessary Sensitive Data\n",
    "\n",
    "If there’s no good reason to store data like passwords, SSNs, or credit cards — don’t store it at all.\n",
    "\n",
    "---\n",
    "\n",
    "### 🚫 Human Mistakes = Real Risk\n",
    "\n",
    "Most leaks happen because someone:\n",
    "\n",
    "- Made a bucket public by accident\n",
    "- Reused a weak password\n",
    "- Clicked on a phishing link\n",
    "- Pasted credentials into public GitHub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d90618",
   "metadata": {},
   "source": [
    "## 📚 Data Management\n",
    "\n",
    "Data management is the practice of treating data as a valuable asset — just like money or property. It involves organizing, protecting, and making data useful across its lifecycle. Without good data management, even the best infrastructure is useless.\n",
    "\n",
    "One of the most respected sources on this topic is **DAMA International**, which created the **DMBOK** (Data Management Book of Knowledge). It outlines **11 key data knowledge areas**, all centered around **Data Governance**.\n",
    "\n",
    "### 🌀 11 Data Knowledge Areas\n",
    "\n",
    "![Data Management Wheel](./image/11_data_knowledge_areas.png)\n",
    "\n",
    "Each slice of the wheel represents an essential discipline:\n",
    "\n",
    "- **Data Governance**: Sets policies, roles, and responsibilities for data.\n",
    "- **Data Architecture**: How data is structured across systems.\n",
    "- **Data Modeling & Design**: Defining how data is stored (tables, schema).\n",
    "- **Data Storage & Operations**: Managing how and where data is stored.\n",
    "- **Data Integration & Interoperability**: Combining data from multiple systems.\n",
    "- **Data Security**: Protecting sensitive information.\n",
    "- **Reference & Master Data**: Maintaining core business data (e.g., product codes).\n",
    "- **Metadata**: Data that describes other data (e.g., table descriptions).\n",
    "- **Data Warehousing & BI**: Centralizing data for analysis and reporting.\n",
    "- **Document & Content Management**: Organizing unstructured documents.\n",
    "- **Data Quality**: Ensuring accuracy, completeness, and reliability.\n",
    "\n",
    "### 🏗️ What’s Your Role as a Data Engineer?\n",
    "\n",
    "As a data engineer, you don’t have to master all 11 areas — but you’ll directly work on:\n",
    "\n",
    "- **Data Integration**: Connecting APIs and systems.\n",
    "- **Data Storage**: Setting up data lakes, warehouses.\n",
    "- **Data Quality**: Validating data through rules and checks.\n",
    "- **Metadata**: Tagging datasets so others understand them.\n",
    "- **Data Governance**: Applying access control and logging usage.\n",
    "\n",
    "### 🧠 Real-World Analogy: A Warehouse\n",
    "\n",
    "Imagine you're running a warehouse:\n",
    "- Only trusted people can enter 🔐 (Security)\n",
    "- Every item is labeled clearly 🏷️ (Metadata)\n",
    "- Nothing is expired or broken ✅ (Data Quality)\n",
    "- You can combine items from different suppliers 🔄 (Integration)\n",
    "\n",
    "A good data management system works just like that — but for digital data.\n",
    "\n",
    "### 💼 Real Example: Zomato\n",
    "\n",
    "At Zomato:\n",
    "- **Governance** defines what “active user” means.\n",
    "- **Quality checks** ensure delivery data is accurate.\n",
    "- **Metadata** helps teams understand the schema.\n",
    "- **Integration** merges app + restaurant data into one view.\n",
    "\n",
    "---\n",
    "\n",
    "With strong data management, organizations can build trust, drive insights, and avoid costly mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f6f586",
   "metadata": {},
   "source": [
    "# 🏛️ Data Architecture\n",
    "\n",
    "Data Architecture is like a **blueprint or roadmap** for how data systems are designed to meet current and future organizational needs. It’s not a one-time task — it’s an **ongoing process** that adapts as business goals, technology, and data evolve.\n",
    "\n",
    "> “Data architecture is the design of systems to support the evolving data needs of an enterprise, achieved by flexible and reversible decisions reached through a careful evaluation of trade-offs.”\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Why It Matters (for Data Engineers)\n",
    "\n",
    "- In large companies: Architects design → Engineers implement\n",
    "- In startups: You may be both architect and engineer\n",
    "- **Thinking like an architect** helps you:\n",
    "  - Choose better tools\n",
    "  - Solve scalability issues\n",
    "  - Avoid costly mistakes\n",
    "  - Lead teams as you grow\n",
    "\n",
    "---\n",
    "\n",
    "## 🧱 Real-Life Analogy\n",
    "\n",
    "Imagine designing a **modular kitchen**:\n",
    "\n",
    "- If you design with **flexibility**, you can upgrade appliances easily.\n",
    "- If not, changing even one part requires **tearing the whole kitchen down**.\n",
    "\n",
    "Good data architecture works the same way — **flexibility and reversibility** are key.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 9 Principles of Good Data Architecture\n",
    "\n",
    "| # | Principle | Description |\n",
    "|---|-----------|-------------|\n",
    "| 1️⃣ | **Choose common components wisely** | Pick tools that multiple teams can use efficiently |\n",
    "| 2️⃣ | **Plan for failure** | Expect and design for things to break |\n",
    "| 3️⃣ | **Architect for scalability** | Make it easy to scale up and down with demand |\n",
    "| 4️⃣ | **Architecture is leadership** | Good architecture sets a strong direction for teams |\n",
    "| 5️⃣ | **Always be architecting** | Revisit your architecture regularly as needs evolve |\n",
    "| 6️⃣ | **Build loosely coupled systems** | Design components to operate independently |\n",
    "| 7️⃣ | **Make reversible decisions** | Use plug-and-play components that can be changed later |\n",
    "| 8️⃣ | **Prioritize security** | Use principles like least privilege and zero trust |\n",
    "| 9️⃣ | **Embrace FinOps** | Design for both cost-efficiency and performance |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧑‍💻 Real-World Example\n",
    "\n",
    "### 🟢 Music Streaming Service Data Platform\n",
    "\n",
    "- Use **Kafka** to stream live listening events (loosely coupled)\n",
    "- Store results in **Snowflake** (can reverse to BigQuery)\n",
    "- Apply **IAM roles and encryption** (security)\n",
    "- Scale systems up at peak hours, down during low traffic (scalable + FinOps)\n",
    "\n",
    "---\n",
    "\n",
    "🧠 **Takeaway:** Whether you are an engineer or architect, always think of **designing systems that adapt, scale, and evolve** — and never forget to bake in security and cost-awareness from day one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b3bfa3",
   "metadata": {},
   "source": [
    "# 🚀 DataOps\n",
    "\n",
    "DataOps improves the development process and quality of data products.  \n",
    "It is a **set of cultural habits and engineering practices** adapted from DevOps, Agile, and Lean principles.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 DataOps: Philosophy & Culture\n",
    "\n",
    "![DataOps Overview](./image/dataops.png)\n",
    "\n",
    "**Core Practices:**\n",
    "- **Communication & Collaboration** between Data Engineers and Stakeholders  \n",
    "- **Continuous Improvement** through feedback loops  \n",
    "- **Rapid Iteration** on data product cycles  \n",
    "\n",
    "---\n",
    "\n",
    "## 🏛️ Pillars of DataOps\n",
    "\n",
    "![Pillars of DataOps](./image/dataops_pillers.png)\n",
    "\n",
    "**3 Core Pillars:**\n",
    "1. **Automation**  \n",
    "2. **Observability & Monitoring**  \n",
    "3. **Incident Response**\n",
    "\n",
    "🎯 Goal: Deliver **high-quality, reliable, and timely** data products\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Pillar 1: Automation\n",
    "\n",
    "### 🧱 DevOps CI/CD (for Software)\n",
    "\n",
    "![DevOps CI/CD](./image/automation.png)\n",
    "\n",
    "**CI/CD Pipeline**  \n",
    "Automates the process from Build → Test → Integrate → Deploy\n",
    "\n",
    "**Benefits:**\n",
    "- Faster release cycles  \n",
    "- Fewer bugs  \n",
    "- Reproducible environments  \n",
    "\n",
    "---\n",
    "\n",
    "### 🛠️ DataOps Automation (for Data Pipelines)\n",
    "\n",
    "DataOps applies automation to the **entire data lifecycle**:  \n",
    "- Ingestion  \n",
    "- Transformation  \n",
    "- Storage  \n",
    "- Serving for ML or Analytics  \n",
    "\n",
    "---\n",
    "\n",
    "### 🧑‍🔧 Manual Execution (No Automation)\n",
    "\n",
    "![Manual Execution](./image/manual_execution.png)\n",
    "\n",
    "In early-stage projects or small teams, pipelines may be run manually.\n",
    "\n",
    "**Challenges:**\n",
    "- Prone to human error  \n",
    "- Time-consuming  \n",
    "- No dependency tracking  \n",
    "- No failure alerts\n",
    "\n",
    "---\n",
    "\n",
    "### ⏱️ Minimum Automation: Pure Scheduling\n",
    "\n",
    "![Scheduling](./image/scheduling.png)\n",
    "\n",
    "**What it is:**  \n",
    "Each task is run on a **fixed schedule** (e.g., cron job)\n",
    "\n",
    "**Pros:**\n",
    "- Easy to implement  \n",
    "**Cons:**\n",
    "- Doesn't check if the previous task succeeded  \n",
    "- Failures go unnoticed  \n",
    "- Still fragile and hard to scale\n",
    "\n",
    "---\n",
    "\n",
    "### 🤖 Orchestration Frameworks (e.g., Airflow)\n",
    "\n",
    "![Orchestration Framework](./image/orchestration_framework.png)\n",
    "\n",
    "**What it does:**\n",
    "- Manages task dependencies  \n",
    "- Automatically executes downstream tasks  \n",
    "- Sends failure notifications  \n",
    "- Improves pipeline observability\n",
    "\n",
    "**Benefits:**\n",
    "- Robust pipelines  \n",
    "- Dependency-aware execution  \n",
    "- Easier collaboration  \n",
    "- Reproducible runs\n",
    "\n",
    "---\n",
    "\n",
    "## 👁️ Pillar 2: Observability & Monitoring\n",
    "\n",
    "![Observability](./image/observability_&_moniitoring.png)\n",
    "\n",
    "> \"Everything fails all the time.\" — **Werner Vogels, AWS CTO**\n",
    "\n",
    "With observability, data teams can:\n",
    "- Detect pipeline failures early  \n",
    "- Avoid stale/inaccurate dashboards  \n",
    "- Restore trust in data-driven decisions  \n",
    "\n",
    "---\n",
    "\n",
    "## 🚨 Pillar 3: Incident Response\n",
    "\n",
    "![Incident Response](./image/incident_response.png)\n",
    "\n",
    "Failures **will** happen. The key is how you respond:\n",
    "\n",
    "✅ With proper incident response:\n",
    "- Root causes are identified quickly  \n",
    "- Stakeholders are informed  \n",
    "- Failures are resolved faster  \n",
    "- Blameless communication is encouraged  \n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Summary\n",
    "\n",
    "| Pillar            | Role                                                                 |\n",
    "|-------------------|----------------------------------------------------------------------|\n",
    "| Automation        | Reduces manual effort, speeds up pipeline runs                       |\n",
    "| Observability     | Detects and prevents issues before they escalate                     |\n",
    "| Incident Response | Ensures quick recovery, accountability, and continuous reliability   |\n",
    "\n",
    "---\n",
    "\n",
    "Next, we’ll explore **how orchestration frameworks** and **observability tools** are implemented in production!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3784a5",
   "metadata": {},
   "source": [
    "# 🧩 Orchestration\n",
    "\n",
    "Data pipelines have many moving parts that must work together in harmony. As a data engineer, **you are the conductor** orchestrating these components to ensure smooth and timely data flow.\n",
    "\n",
    "Orchestration ensures that tasks in a pipeline are executed in the correct order, under the right conditions, and with automated error handling and monitoring.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Why Orchestration Matters\n",
    "\n",
    "- **Manual execution** works only for initial prototypes.\n",
    "- **Scheduling** (e.g., using cron jobs) helps automate tasks but fails when things go wrong.\n",
    "- **Modern orchestration frameworks** go beyond time-based triggers — they let you:\n",
    "  - Trigger tasks based on **dependencies** or **events**\n",
    "  - Implement **monitoring and alerts**\n",
    "  - Handle **failures gracefully**\n",
    "  - Visualize your data pipeline as a **DAG (Directed Acyclic Graph)**\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Orchestration Frameworks\n",
    "\n",
    "These tools help implement modern orchestration pipelines:\n",
    "\n",
    "![Orchestration Frameworks](./image/orchestration_frameworkk.png)\n",
    "\n",
    "Popular frameworks:\n",
    "- **Apache Airflow** – most popular, highly extensible\n",
    "- **Dagster** – modern and developer-friendly\n",
    "- **Prefect** – focuses on simplicity and observability\n",
    "- **Mage** – good for modern data teams\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 DAG (Directed Acyclic Graph)\n",
    "\n",
    "Orchestration frameworks use **DAGs** to define the structure of a pipeline.\n",
    "\n",
    "A **DAG**:\n",
    "- Is made of **nodes** (tasks) and **edges** (data flow)\n",
    "- Is **directed**: data flows in one direction\n",
    "- Is **acyclic**: no loops allowed\n",
    "\n",
    "Here’s what a DAG looks like in the context of a data pipeline:\n",
    "\n",
    "![DAG - Data Flow](./image/normal_graph.png)\n",
    "\n",
    "Another DAG view with labeled tasks:\n",
    "\n",
    "![DAG Graph View](./image/dag.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Manual Execution → Automation → Orchestration\n",
    "\n",
    "Let’s look at the evolution of orchestration:\n",
    "\n",
    "### ❌ Manual Execution (Not Scalable)\n",
    "You run each task one by one using shell scripts or Jupyter commands.\n",
    "\n",
    "### ✅ Minimal Automation (Pure Scheduling)\n",
    "You schedule tasks using cron jobs or time-based triggers.\n",
    "\n",
    "### ✅✅ Full Orchestration\n",
    "You define DAGs with:\n",
    "- **Dependency rules**\n",
    "- **Event-based triggers**\n",
    "- **Automatic retries**\n",
    "- **Failure alerts**\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Key Concepts for Orchestration\n",
    "\n",
    "| Concept              | Description |\n",
    "|----------------------|-------------|\n",
    "| **Task Dependency**  | One task runs only after another completes |\n",
    "| **Event Triggers**   | Start a task when new data arrives |\n",
    "| **Monitoring**       | Alerts you when a task fails or takes too long |\n",
    "| **Retry Logic**      | Automatically retry failed tasks |\n",
    "| **Scalability**      | Run large workloads in parallel across nodes |\n",
    "\n",
    "---\n",
    "\n",
    "By using orchestration frameworks, you reduce manual effort, avoid data quality issues, and increase the reliability of your pipelines.\n",
    "\n",
    "This is a **core skill** for any production-grade data engineer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e57ec29",
   "metadata": {},
   "source": [
    "# 💻 Software Engineering\n",
    "\n",
    "Of all the undercurrents in the data engineering lifecycle, **Software Engineering** is the most straightforward — **you must know how to read and write code**.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Why It Matters\n",
    "\n",
    "- As a data engineer, you’ll write code for:\n",
    "  - **Data ingestion**\n",
    "  - **Transformation**\n",
    "  - **Storage & Serving**\n",
    "- Code needs to be:\n",
    "  - Clean ✅\n",
    "  - Readable 🧠\n",
    "  - Testable ✅\n",
    "  - Deployable 🚀\n",
    "\n",
    "---\n",
    "\n",
    "## 🏗️ Then vs Now\n",
    "\n",
    "- **Past**: Data engineering was just a part of a software engineer’s job.\n",
    "- **Now**: Data engineering is its own field, thanks to the **explosion of data**.\n",
    "\n",
    "This shift led to better tools, frameworks, and managed services — helping you **focus on what matters most**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 Tools and Languages You’ll Use\n",
    "\n",
    "![Tools and Languages](./image/tools_and_languages.png)\n",
    "\n",
    "You’ll work with:\n",
    "- **SQL** – for querying data\n",
    "- **Spark & Kafka** – for big data processing and streaming\n",
    "- **Python** – most commonly used scripting language\n",
    "- **Java / Scala** – common in JVM(java virtual machine)-based data platforms\n",
    "- **Bash** – for command-line automation\n",
    "- **R / Rust / Go** – in some specialized teams\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ Real-World Coding as a Data Engineer\n",
    "\n",
    "- **Open Source Contributions**: You may adopt and contribute to open-source tools.\n",
    "- **Infrastructure as Code**: You’ll use code to define infrastructure or pipelines (like Terraform or DAGs).\n",
    "- **Everyday Scripts**: Writing scripts to solve real-world problems in the pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Key Takeaways\n",
    "\n",
    "- Writing **high-quality code** is essential.\n",
    "- Focus on **core languages**: SQL, Python, and Bash.\n",
    "- Be ready to collaborate and **learn from software engineers**.\n",
    "- Clean code = reliable pipelines = real business value 💼\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
