{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0e7dfbb",
   "metadata": {},
   "source": [
    "# What is Data Engineering?\n",
    "\n",
    "**Data Engineering** is the practice of designing, building, and maintaining systems that enable the collection, storage, processing, and analysis of data at scale.\n",
    "\n",
    "It provides the foundation that allows data scientists, analysts, and business teams to work with clean, reliable, and accessible data.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Responsibilities of a Data Engineer:\n",
    "\n",
    "- **Build data pipelines** to move data from source systems to data warehouses or data lakes.\n",
    "- **Clean and transform raw data** so it's usable for analytics and machine learning.\n",
    "- **Design scalable architectures** for storing and querying large volumes of data.\n",
    "- **Ensure data quality, security, and governance.**\n",
    "- **Collaborate with stakeholders** to understand data needs and translate them into technical systems.\n",
    "\n",
    "---\n",
    "\n",
    "## Why It Matters\n",
    "\n",
    "Without data engineering:\n",
    "- Data scientists and analysts waste time trying to access or clean data.\n",
    "- Business decisions may be based on incomplete or incorrect data.\n",
    "- Machine learning models cannot be trained or deployed effectively.\n",
    "\n",
    "In short, **Data Engineering powers the entire data ecosystem** in a company."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6c7907",
   "metadata": {},
   "source": [
    "# The Evolution of Data Engineering\n",
    "\n",
    "In the early days, **data engineering** didn't exist as a dedicated role. The original data engineers were simply **software engineers**, focused on building applications. The **data** generated by these applications was treated as a byproduct ‚Äî useful mainly for **debugging** or **monitoring**, but not much else.\n",
    "\n",
    "This data was like **\"exhaust\" from a car** ‚Äî a natural outcome, but not something with standalone value.\n",
    "\n",
    "---\n",
    "\n",
    "Over time, as companies recognized the **intrinsic value of data**, especially with the rise in **volume and variety**, the role of engineers shifted. Engineers started building systems *specifically* for data ingestion, transformation, and delivery. This led to the formalization of the **Data Engineer** role.\n",
    "\n",
    "---\n",
    "\n",
    "## What Is Data Engineering?\n",
    "\n",
    "In the book *Fundamentals of Data Engineering*, the authors Joe Reis and Matt Housley define data engineering as:\n",
    "\n",
    "> **\"The development, implementation, and maintenance of systems and processes that take in raw data and produce high-quality, consistent information that supports downstream use cases, such as analysis and machine learning.\"**\n",
    "\n",
    "---\n",
    "\n",
    "### Data Engineering Sits at the Intersection of:\n",
    "\n",
    "- **Security**\n",
    "- **Data Management**\n",
    "- **DataOps**\n",
    "- **Data Architecture**\n",
    "- **Orchestration**\n",
    "- **Software Engineering**\n",
    "\n",
    "These are referred to as the **undercurrents** of data engineering ‚Äî they apply across every stage of the data life cycle.\n",
    "\n",
    "---\n",
    "\n",
    "## The Data Engineering Life Cycle\n",
    "\n",
    "The lifecycle can be visualized as a flow from data generation to end use cases:\n",
    "\n",
    "1. **Data Generation & Source Systems**  \n",
    "   (e.g. app logs, sensor data, uploaded files)\n",
    "\n",
    "2. **Ingestion**  \n",
    "   (bringing raw data into your systems)\n",
    "\n",
    "3. **Storage**  \n",
    "   (saving raw or processed data efficiently)\n",
    "\n",
    "4. **Transformation**  \n",
    "   (cleaning, filtering, enriching, and shaping the data)\n",
    "\n",
    "5. **Serving**  \n",
    "   (making processed data available for use)\n",
    "\n",
    "6. **End Use Cases**  \n",
    "   (analytics, machine learning, reverse ETL)\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Data Engineering Lifecycle Diagram\n",
    "![data engineering life cycle](image/data_engineering_lifecycle.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Final Thought\n",
    "\n",
    "As a Data Engineer, your goal is to **transform raw data into useful, reliable, and accessible information** ‚Äî supporting analytics, machine learning, and other business needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f38b37",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "948b3cee",
   "metadata": {},
   "source": [
    "# History of Data Engineering ‚Äì What I Understood\n",
    "\n",
    "From the video, I understood that data has always existed ‚Äî not just in the form of numbers or words, but also as natural signals like wind, sound, or light. But the kind of data we focus on in data engineering is **digitally recorded data**, the type that can be stored on a computer and used for analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## The Early Days (1960s‚Äì1980s)\n",
    "\n",
    "Data engineering didn‚Äôt start as a formal role. In the 1960s, computers introduced the first **digital databases**. In the 1970s, IBM engineers created **relational databases** and **SQL** (Structured Query Language), which is still widely used today.\n",
    "\n",
    "In the 1980s, **Bill Inmon** came up with the idea of the **data warehouse**, which helped businesses transform and organize data for better decision-making. These early efforts laid the foundation for what would eventually become the field of data engineering.\n",
    "\n",
    "---\n",
    "\n",
    "## The 1990s: Growth of BI and the Internet\n",
    "\n",
    "In the 1990s, companies began building **data pipelines** to support **Business Intelligence (BI)** and reporting. Two important names during this time were **Bill Inmon** and **Ralph Kimball**, who proposed different approaches to data modeling for analytics.\n",
    "\n",
    "Also, the rise of the **Internet** (and companies like Amazon) created massive growth in data. This made data pipelines and infrastructure even more important.\n",
    "\n",
    "---\n",
    "\n",
    "## The 2000s: Big Data Era Begins\n",
    "\n",
    "In the early 2000s, tech giants like Google, Amazon, and Yahoo faced data at a scale that traditional systems couldn‚Äôt handle. This led to the **Big Data movement**.\n",
    "\n",
    "Here‚Äôs what stood out:\n",
    "- **Big Data** was defined by the 3 V's: **Volume**, **Velocity**, and **Variety**.\n",
    "- In **2004**, Google released the **MapReduce** paper, which introduced a new way to process huge amounts of data.\n",
    "- In **2006**, Yahoo open-sourced **Apache Hadoop**, inspired by Google‚Äôs paper. This was a major shift ‚Äî suddenly, even small companies could work with massive datasets.\n",
    "- Around the same time, **Amazon Web Services (AWS)** launched services like **EC2**, **S3**, and **DynamoDB**, making cloud computing accessible to everyone.\n",
    "\n",
    "These developments created a new generation of **data engineers**, especially those focused on solving problems at scale.\n",
    "\n",
    "---\n",
    "\n",
    "## The 2010s: Cloud and Real-Time Data\n",
    "\n",
    "By the 2010s, public cloud platforms like **AWS, Google Cloud, and Azure** became mainstream. This made it much easier to manage and process data without setting up massive infrastructure.\n",
    "\n",
    "A big shift also happened from **batch processing** (processing data in chunks) to **real-time streaming**, where data could be processed as it arrived. Tools like **Apache Kafka** and **Apache Spark** helped with this transition.\n",
    "\n",
    "Eventually, the term **Big Data** faded because working with large-scale data became a common part of engineering. It wasn‚Äôt a ‚Äúspecial case‚Äù anymore ‚Äî just part of the job.\n",
    "\n",
    "---\n",
    "\n",
    "## Today: The Role of Modern Data Engineers\n",
    "\n",
    "Today, **data engineering** is a central and strategic part of any data-driven business. Data engineers now work with:\n",
    "- Cloud-native tools (like AWS Glue, Snowflake, Redshift)\n",
    "- Workflow orchestrators (like Airflow or Dagster)\n",
    "- Real-time data processing tools (like Kafka and Flink)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cec7e5",
   "metadata": {},
   "source": [
    "# Stakeholders in the Data Engineering Workflow\n",
    "\n",
    "In a data engineering workflow, the data engineer plays a central role ‚Äî acting as a bridge between upstream and downstream stakeholders.\n",
    "\n",
    "---\n",
    "\n",
    "## Downstream Stakeholders\n",
    "\n",
    "Downstream stakeholders are the consumers of processed and served data. These include:\n",
    "\n",
    "- **Analysts**\n",
    "- **Data Scientists**\n",
    "- **Machine Learning Engineers**\n",
    "- **Salespeople**\n",
    "- **Marketing Professionals**\n",
    "- **Executives**\n",
    "\n",
    "Each stakeholder group has different data needs, goals, and expectations.\n",
    "\n",
    "To serve downstream stakeholders effectively, the data engineer must understand:\n",
    "- **How often** data is needed (e.g., real-time vs daily)\n",
    "- **What information** is needed (specific tables, metrics, joins, aggregations)\n",
    "- **How much latency** is acceptable (seconds, hours, days)\n",
    "\n",
    "An example:  \n",
    "A business analyst may need to run SQL queries for dashboards and trend analysis. To support this, the data engineer must ensure:\n",
    "- Data is refreshed frequently enough\n",
    "- Queries run efficiently (through pre-aggregations or materialized views)\n",
    "- Time zones and definitions (like start/end of \"day\") are consistent and well-aligned\n",
    "\n",
    "---\n",
    "\n",
    "## Upstream Stakeholders\n",
    "\n",
    "Upstream stakeholders are the owners of source systems that generate raw data. Most often, these are **software engineers**, either within the organization or from external systems.\n",
    "\n",
    "From upstream stakeholders, the data engineer needs to understand:\n",
    "- **Data volume** (how much data to expect)\n",
    "- **Frequency** (how often data is emitted)\n",
    "- **Format** (JSON, CSV, Avro, etc.)\n",
    "- **Data security** and **compliance** considerations\n",
    "- **Schema changes**, system outages, or disruptions to the data flow\n",
    "\n",
    "In many ways, the data engineer becomes the **consumer** of upstream data, similar to how analysts consume the outputs produced by the data engineer.\n",
    "\n",
    "Establishing a strong communication loop with upstream teams can help anticipate changes and design resilient data pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "## Core Responsibilities\n",
    "\n",
    "The data engineer must:\n",
    "- Translate upstream raw data into usable, reliable formats\n",
    "- Serve that data to downstream users in a way that supports their specific goals\n",
    "- Maintain communication in both directions to ensure smooth data operations and system alignment\n",
    "\n",
    "---\n",
    "\n",
    "## Visual Summary\n",
    "\n",
    "![Data Engineering Stakeholders Diagram](image/workflow_data_engineering.png)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73925a5b",
   "metadata": {},
   "source": [
    "# Requirements Gathering for Data Engineering Systems\n",
    "\n",
    "Before building any data system ‚Äî writing code, provisioning infrastructure, or deploying to the Cloud ‚Äî it is critical to **understand stakeholder needs** and translate them into system requirements.\n",
    "\n",
    "---\n",
    "\n",
    "## Types of Requirements\n",
    "\n",
    "### 1. Business Requirements  \n",
    "These define the **high-level goals of the organization**, such as:\n",
    "- Increasing revenue\n",
    "- Growing user base\n",
    "- Improving customer experience\n",
    "\n",
    "### 2. Stakeholder Requirements  \n",
    "These are the **individual needs** of downstream stakeholders (analysts, data scientists, executives, etc.) ‚Äî the tasks they need data to help accomplish.\n",
    "\n",
    "### 3. System Requirements  \n",
    "System requirements define what the system must do to meet business and stakeholder needs. These are split into:\n",
    "\n",
    "- **Functional Requirements** (the **what**)  \n",
    "  - Example: \"Refresh the dashboard data every 24 hours\"\n",
    "  - Example: \"Send alerts when data anomalies are detected\"\n",
    "\n",
    "- **Non-Functional Requirements** (the **how**)  \n",
    "  - Example: \"Use a cloud-based ingestion system that supports real-time streaming\"\n",
    "  - Example: \"Ensure latency is under 2 minutes and uptime is 99.9%\"\n",
    "\n",
    "---\n",
    "\n",
    "## What to Consider in System Requirements\n",
    "\n",
    "- Business goals and stakeholder needs\n",
    "- Features and attributes of your data products\n",
    "- Compute, memory, and storage capacity\n",
    "- Data freshness and latency expectations\n",
    "- Security and regulatory compliance\n",
    "- Cost constraints\n",
    "\n",
    "---\n",
    "\n",
    "## The Process of Requirements Gathering\n",
    "\n",
    "Requirements gathering starts with **talking to stakeholders**. However, stakeholders rarely present their needs in the form of technical specifications. Instead, they express **business goals** or frustrations.\n",
    "\n",
    "It‚Äôs the data engineer‚Äôs responsibility to:\n",
    "- Ask the right questions\n",
    "- Translate ambiguous needs into clear system requirements\n",
    "- Tailor conversations based on the stakeholder's technical background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4a977d",
   "metadata": {},
   "source": [
    "# Stakeholder Conversation: Data Engineer & Data Scientist\n",
    "\n",
    "This section summarizes a mock stakeholder conversation between Joe (Data Engineer) and Colleen (Data Scientist). The goal of the conversation is to gather requirements for a data system that supports marketing and product analytics.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objective\n",
    "\n",
    "To understand the challenges faced by the data scientist and translate them into **system requirements** that the data engineer can address through ingestion, transformation, and serving of data.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Key Problems Identified\n",
    "\n",
    "### 1. Limited and Manual Data Access\n",
    "- Sales data resides in a **production database**.\n",
    "- Colleen does not have direct access due to risks; receives **daily data dumps** instead.\n",
    "- Data is delivered in **CSV/JSON formats**.\n",
    "\n",
    "### 2. Dirty and Overwhelming Data\n",
    "- 90% of the data received is not useful.\n",
    "- Colleen spends **80% of her time** cleaning and preparing the data.\n",
    "- Scripts **frequently break** due to anomalies or schema changes.\n",
    "\n",
    "### 3. Outdated Data for Analytics\n",
    "- Marketing team wants **real-time** sales data by region.\n",
    "- Current dashboards show data that is **2 days old**.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Current Use Cases\n",
    "\n",
    "### Dashboards for Marketing\n",
    "- Show 30-day sales trends by **product category** and **region**.\n",
    "- Support **drill-down** to hourly/product-level insights.\n",
    "- Require fresher data for **campaign timing and evaluation**.\n",
    "\n",
    "### Product Recommendation Engine\n",
    "- Currently based on **most popular products**.\n",
    "- Future goal: **personalized recommendations** using content-based filtering.\n",
    "- Needs:\n",
    "  - Real-time or recent **user behavior data**\n",
    "  - A way to **deploy and serve model outputs**\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Insights from the Conversation\n",
    "\n",
    "- Need to **automate** data ingestion and processing.\n",
    "- Requirement for **low-latency** data (exact threshold TBD).\n",
    "- Marketing may benefit from:\n",
    "  - Data updated **hourly or more frequently**\n",
    "  - Ability to **target campaigns** using fresh user data\n",
    "- Additional follow-up needed with marketing team to clarify:\n",
    "  - **Actionable use cases**\n",
    "  - **Required freshness/latency**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Initial System Requirements\n",
    "\n",
    "### Functional Requirements (What the system should do)\n",
    "- Ingest sales data more **frequently and automatically**\n",
    "- Transform and clean data to remove irrelevant parts\n",
    "- Serve aggregated, cleaned data to dashboards\n",
    "- Support training and deployment of **ML recommendation models**\n",
    "\n",
    "### Non-Functional Requirements (How the system should work)\n",
    "- Handle **schema changes gracefully**\n",
    "- Maintain **data freshness** suitable for near real-time needs\n",
    "- Be **resilient** and monitored for anomalies or failures\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Next Steps\n",
    "\n",
    "- Follow up with **marketing stakeholders** to refine data freshness and use case requirements.\n",
    "- Design a pipeline that supports:\n",
    "  - Reliable and automated **data ingestion**\n",
    "  - Scalable **data transformation and storage**\n",
    "  - Efficient **data serving** for both dashboards and ML use cases\n",
    "\n",
    "---\n",
    "\n",
    "This conversation highlights how initial stakeholder interviews are essential for uncovering not only technical needs but also organizational goals and pain points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86702a07",
   "metadata": {},
   "source": [
    "# Breaking Down a Stakeholder Conversation: Requirements Gathering\n",
    "\n",
    "In this section, we reflect on the mock conversation between the data engineer and the data scientist (Colleen) to extract actionable requirements and outline a reusable approach to stakeholder interviews in data engineering projects.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Purpose\n",
    "\n",
    "To demonstrate how to:\n",
    "- Extract system requirements from stakeholder conversations.\n",
    "- Clarify vague terminology like ‚Äúreal-time.‚Äù\n",
    "- Identify additional stakeholders to speak with.\n",
    "- Translate business needs into data engineering solutions.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Key Steps in Requirements Gathering\n",
    "\n",
    "### 1. Understand Existing Systems and Pain Points\n",
    "- Learn what systems are currently in place.\n",
    "- Identify inefficiencies, delays, manual steps, and risks.\n",
    "- In this case:\n",
    "  - Only daily data dumps from the production database.\n",
    "  - Data is messy and frequently breaks scripts due to schema changes.\n",
    "\n",
    "### 2. Ask What Actions Stakeholders Will Take with the Data\n",
    "- Don't just ask what data they want ‚Äî ask what decisions or operations they will perform.\n",
    "- Helps clarify how critical the data is and what latency is acceptable.\n",
    "- Example:\n",
    "  - Dashboards are used to optimize marketing campaigns.\n",
    "  - Recommendation engine is used to influence customer purchases during browsing.\n",
    "\n",
    "### 3. Confirm Your Understanding\n",
    "- Summarize and repeat back what you've heard.\n",
    "- Ensure alignment before building any solution.\n",
    "- In this case:\n",
    "  - Automating ingestion and transformation would be a huge help.\n",
    "  - Serving clean, near real-time data would improve dashboard and ML performance.\n",
    "\n",
    "### 4. Identify Other Stakeholders\n",
    "- Your current stakeholder may not have all the information.\n",
    "- Additional conversations are needed with:\n",
    "  - **Software Engineers** (who manage the source system)\n",
    "  - **Marketing Team** (to clarify what \"real-time\" means and what they‚Äôll do with the data)\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Requirements Identified\n",
    "\n",
    "### Functional Requirements (What the system should do)\n",
    "- Ingest data from the sales platform.\n",
    "- Transform and clean the data automatically.\n",
    "- Serve data to:\n",
    "  - Dashboards (marketing use case)\n",
    "  - Recommendation engine (ML use case)\n",
    "\n",
    "### Non-Functional Requirements (How the system should perform)\n",
    "- Define and meet latency thresholds (e.g., hourly vs real-time).\n",
    "- Handle schema changes without failure.\n",
    "- Ensure reliability and scalability of pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Important Tactic\n",
    "\n",
    "### Ask: ‚ÄúWhat action will you take based on the data?‚Äù\n",
    "- Avoid letting stakeholders define the system themselves.\n",
    "- Focus on understanding their **business use case** first.\n",
    "- Then derive the appropriate technical implementation.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaway:\n",
    "> The foundation of a successful data engineering project is built during **requirements gathering**, where listening, translating, and aligning business goals with technical execution is critical.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a169e5",
   "metadata": {},
   "source": [
    "# Thinking Like a Data Engineer\n",
    "\n",
    "In any data engineering project, success depends on more than just technical implementation. It starts with understanding the business context and ends with continuous iteration based on stakeholder needs. Below is a four-stage framework for thinking like a data engineer.\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ Framework Overview\n",
    "\n",
    "This framework consists of four main stages:\n",
    "\n",
    "1. **Identify business goals & stakeholder needs**\n",
    "2. **Define system requirements**\n",
    "3. **Choose tools & technologies**\n",
    "4. **Build, evaluate, iterate & evolve**\n",
    "\n",
    "This is not a strict linear process ‚Äî in real-world scenarios, you'll often revisit earlier stages as goals and technologies evolve.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Stage 1: Identify Business Goals & Stakeholder Needs\n",
    "\n",
    "- Identify business goals and the stakeholders you are serving.\n",
    "- Explore existing systems and what pain points stakeholders currently face.\n",
    "- Ask stakeholders: **\"What actions will you take using the data product?\"**\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Stage 2: Define System Requirements\n",
    "\n",
    "- Translate stakeholder needs into **functional requirements** (what the system should do).\n",
    "- Define **non-functional requirements** (how the system should perform).\n",
    "- Document and confirm requirements with stakeholders to ensure alignment.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Stage 3: Choose Tools & Technologies\n",
    "\n",
    "- Identify tools and technologies that meet the non-functional requirements.\n",
    "- Conduct a **cost-benefit analysis** to compare different options.\n",
    "- **Prototype and test** your design to check if it meets stakeholder expectations.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Stage 4: Build, Evaluate, Iterate & Evolve\n",
    "\n",
    "- Build and deploy the **production-ready system**.\n",
    "- Continuously **monitor and evaluate** its performance.\n",
    "- Iterate and evolve the system based on feedback and changing needs.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Visual Summary\n",
    "![Thinking Like a Data Engineer](image/framework.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
