{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f00b10f",
   "metadata": {},
   "source": [
    "# C1_W2_Assignment\n",
    "\n",
    "## 🔍 Goal of the Assignment\n",
    "\n",
    "In this assignment, you act as a **Data Engineer** at a retail company that sells scale models of classic cars. Your job is to build a simple batch data pipeline that:\n",
    "\n",
    "1. **Extracts** customer and sales data from an Amazon RDS (MySQL) database,  \n",
    "2. **Transforms** it into a star schema for analytical querying, and  \n",
    "3. **Loads** it into an Amazon S3 bucket.\n",
    "\n",
    "The transformed data can then be queried by analysts using **Amazon Athena**.\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 What Was Provided\n",
    "\n",
    "- A running Amazon RDS instance containing the classic car database (relational database).\n",
    "- The ETL script that transforms the raw data into a star schema.\n",
    "- Pre-written Terraform scripts to provision the AWS infrastructure (S3 buckets, Glue Jobs, etc.)\n",
    "- A Jupyter Notebook to query the final output.\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ Architectural Diagram\n",
    "\n",
    "The image below gives a high-level overview of what you are building:\n",
    "\n",
    "![Architectural Diagram](../image/archi_dia.png)\n",
    "\n",
    "### 🧩 Components:\n",
    "\n",
    "- **Amazon RDS**: Source MySQL database with tables like `products`, `orders`, `customers`, etc.\n",
    "- **AWS Glue ETL**: Extracts raw data from RDS, transforms it into a star schema.\n",
    "- **Amazon S3**: Target storage where the transformed data is loaded.\n",
    "- **Glue Crawler**: Reads the data from S3 and updates the AWS Glue Data Catalog.  \n",
    "  🔗 Learn more: [AWS Glue Data Catalog Docs](https://docs.aws.amazon.com/glue/latest/dg/components-overview.html)\n",
    "- **Amazon Athena**: SQL-based querying engine to analyze data in S3.  \n",
    "  🔗 Learn more: [What is Amazon Athena?](https://aws.amazon.com/athena/)  \n",
    "  ✳️ **Note:** Athena is not a data warehouse. It allows SQL-like queries on S3 directly without managing infrastructure. Unlike Redshift, it's serverless and queries are slower for very large analytical jobs.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 Pipeline Scenario\n",
    "\n",
    "This diagram shows the original relational structure of your MySQL RDS database.\n",
    "\n",
    "![Pipeline Scenario](../image/pipeline_scenario.png)\n",
    "\n",
    "- You are tasked with extracting only the data that the analyst needs.\n",
    "- You will transform this data into a format that is **easier and faster to query**.\n",
    "- You will load this data into **S3** for long-term storage and access.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔃 ETL Process\n",
    "\n",
    "This is a classical **ETL** (Extract-Transform-Load) pipeline:\n",
    "\n",
    "- **Extract**: Pull data from the source (RDS)\n",
    "- **Transform**: Restructure it using a **star schema**\n",
    "- **Load**: Save transformed data into an S3 bucket\n",
    "\n",
    "The Glue job automates this process.\n",
    "\n",
    "---\n",
    "\n",
    "## 🌟 Structure of Transformed Data\n",
    "\n",
    "The following image shows the **Star Schema** used to restructure the relational data.\n",
    "\n",
    "![Star Schema](../image/star_schema.png)\n",
    "\n",
    "### 🔷 What is a Star Schema?\n",
    "\n",
    "A **star schema** is a type of database schema that is optimized for querying large-scale analytics data. It consists of a central **fact table** linked to multiple **dimension tables**.\n",
    "\n",
    "🔗 Learn more: [Star Schema Explained](https://vertabelo.com/blog/data-warehouse-modeling-the-star-schema/)\n",
    "\n",
    "### 🟨 Fact Table:\n",
    "- `fact_orders`: central table with metrics like `quantityOrdered`, `orderAmount`, etc.\n",
    "\n",
    "### 🟦 Dimension Tables:\n",
    "- `dim_customers`, `dim_products`, `dim_locations`: provide context for analytical aggregations (e.g., sales by product, sales by country)\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 How This Was Built — Terraform\n",
    "\n",
    "All infrastructure (Glue Jobs, Crawlers, S3 buckets) was created using **Terraform**, which is an Infrastructure as Code (IaC) tool.\n",
    "\n",
    "![Terraform Setup](../image/terrafomr.png)\n",
    "\n",
    "The Terraform code was already provided. In **Course 2**, you’ll learn how to write these scripts yourself.\n",
    "\n",
    "---\n",
    "\n",
    "## 💬 Notes for Future Reference\n",
    "\n",
    "- The code and infra setup is **already provided**. You are not expected to write them from scratch for now.\n",
    "- **Glue scripts, Lambda functions, and Terraform code** might look complex now — this is normal.\n",
    "- Future courses will cover these in detail."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
