{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b19c8ed7",
   "metadata": {},
   "source": [
    "# 📘 Week 4 Assignment – Building a Recommender System Using Batch and Streaming Pipelines on AWS\n",
    "\n",
    "🛠️ **Note:** All AWS resources (like RDS, S3 buckets, Lambda functions, Firehose, etc.) were pre-configured and provisioned in the assignment. Our main job was to establish proper connections between components and understand the end-to-end working of the recommender architecture. You’ll dive deeper into Terraform and AWS tools in future courses.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Assignment Objective\n",
    "\n",
    "In this lab, we are tasked with implementing a complete **Recommender System Pipeline** using AWS services. This includes:\n",
    "\n",
    "- ✅ Extracting and transforming training data for a recommender system  \n",
    "- ✅ Making the transformed data available to data scientists  \n",
    "- ✅ Using the trained model outputs (embeddings) to create a **Vector Database**  \n",
    "- ✅ Setting up a **streaming architecture** to deliver real-time product recommendations to users  \n",
    "- ✅ Ensuring all outputs (recommendations) are stored properly in an S3 bucket for future use  \n",
    "\n",
    "---\n",
    "\n",
    "## 📦 Overview of the Architecture\n",
    "\n",
    "We are essentially helping **build a machine learning-powered recommender system** that uses **batch processing for training** and **streaming processing for real-time inference**. Here's how it works at a high level:\n",
    "\n",
    "- Raw data from an **Amazon RDS** instance is extracted and transformed using **AWS Glue ETL**  \n",
    "- The transformed data is stored in **Amazon S3** and used by data scientists to train the recommender model  \n",
    "- The **trained model**, along with **user/item embeddings**, is stored in another S3 bucket  \n",
    "- These embeddings are uploaded to a **Vector Database** (PostgreSQL) for efficient similarity searches  \n",
    "- A **streaming pipeline** is created using **Kinesis Data Streams**, **Lambda functions**, and **Data Firehose**  \n",
    "- Real-time user interactions are used to compute recommendations using the trained model and embeddings  \n",
    "- Final recommendations are sent to the client and also stored in a dedicated **S3 recommendations bucket**\n",
    "\n",
    "---\n",
    "\n",
    "## 🧱 Step 1: Batch Pipeline Setup\n",
    "\n",
    "Our first task is to extract ratings data from a MySQL database hosted in **Amazon RDS**. This data represents how users rated products and is the key input for training a supervised ML model.\n",
    "\n",
    "- We use **AWS Glue ETL** to extract and transform the ratings data  \n",
    "- The output of this transformation is stored in an **S3 bucket** called `data lake`  \n",
    "- A **Glue Crawler** is then used to create a catalog/table from the S3 data for easy access by data scientists  \n",
    "\n",
    "📸 *Batch Pipeline Diagram*  \n",
    "![Batch Pipeline](../images/batch_pipline_data_lake.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Step 2: Model Training (by Data Scientist)\n",
    "\n",
    "Once the transformed data is available in the data lake bucket, it is used by the **data scientist** to train a recommender model.\n",
    "\n",
    "- The **trained model** is stored in another S3 bucket called `ML Artifacts`  \n",
    "- This bucket contains:\n",
    "  - `models/` → serialized model (e.g. .pkl)\n",
    "  - `embeddings/` → user and item embeddings CSV files\n",
    "  - `scalars/` → preprocessing information  \n",
    "\n",
    "You are not training the model — this is handled by the data scientist. Your role is to enable the next part of the pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧲 Step 3: Vector Database Setup\n",
    "\n",
    "The data scientist asks us to create a **Vector Database** using PostgreSQL to enable fast similarity searches based on item/user embeddings.\n",
    "\n",
    "- We create a PostgreSQL database via Terraform  \n",
    "- We collect the DB host, username, and password from the outputs  \n",
    "- We use SQL scripts to upload the contents of `item_embeddings.csv` and `user_embeddings.csv` from the `ML Artifacts` bucket into the vector database  \n",
    "- These embeddings allow us to retrieve similar products later during inference  \n",
    "\n",
    "📸 *Vector Database Diagram*  \n",
    "![Vector Database](../images/vector_database.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Step 4: Real-time Streaming Pipeline\n",
    "\n",
    "Now that the model and embeddings are ready, we move to the **streaming pipeline** that will power live recommendations based on user activity.\n",
    "\n",
    "- AWS **Kinesis Data Streams** are already set up in the background and continuously push user events (cart actions, clicks)\n",
    "- We configure three components via Terraform:\n",
    "  - `Model Inference Lambda` → uses trained model + vector DB to generate recommendations  \n",
    "  - `Stream Transformation Lambda` → extracts user/item features from Kinesis records  \n",
    "  - `Amazon Data Firehose` → orchestrates the flow: reads Kinesis, invokes both Lambdas, and stores results  \n",
    "\n",
    "📸 *Streaming Pipeline Diagram*  \n",
    "![Streaming Pipeline](../images/streaming_pipeline.png)\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 Streaming Pipeline Flow (Step-by-step)\n",
    "\n",
    "1. User actions are pushed to **Kinesis Data Streams**  \n",
    "2. **Firehose** reads the incoming records  \n",
    "3. Firehose invokes the **Stream Transformation Lambda** to extract user/product data  \n",
    "4. The output is passed to the **Model Inference Lambda**, which:\n",
    "   - Loads the trained model from the `ML Artifacts` S3 bucket  \n",
    "   - Connects to the vector DB  \n",
    "   - Retrieves similar items for the given user/cart combination  \n",
    "5. Firehose stores the **final recommendations** in a dedicated **S3 bucket (`recommendations`)**  \n",
    "6. These recommendations can also be served back to the user via the platform\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Summary\n",
    "\n",
    "By the end of this lab:\n",
    "\n",
    "- We implemented a complete **batch + streaming recommendation system**  \n",
    "- We transformed data from **RDS → S3 → ML model**  \n",
    "- We stored model artifacts and **created a vector DB** for similarity search  \n",
    "- We connected all pieces in a **real-time stream** using Kinesis, Lambda, and Firehose  \n",
    "- All resources were created via Terraform — your task was to **understand connections, not configurations**\n",
    "\n",
    "This lab provides foundational experience in designing and implementing modern ML pipelines using AWS — exactly what data engineers are expected to build in real-world projects."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
