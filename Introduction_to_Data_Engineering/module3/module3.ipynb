{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a98483f",
   "metadata": {},
   "source": [
    "# üìö Index\n",
    "\n",
    "- [Enterprise Architecture](#enterprise-architecture)\n",
    "\n",
    "- [Conway‚Äôs Law](#conways-law)\n",
    "\n",
    "- [Principles of Good Data Architecture](#principles-of-good-data-architecture)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe58e9f",
   "metadata": {},
   "source": [
    "# Enterprise Architecture\n",
    "\n",
    "Enterprise Architecture is the design of systems to support **change** in an enterprise ‚Äî achieved by **flexible and reversible decisions** through careful evaluation of trade-offs.\n",
    "\n",
    "It consists of **4 core components**:\n",
    "\n",
    "![Enterprise Architecture Diagram](./images/enterprice_architecture.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. üß© Business Architecture  \n",
    "**Purpose:** Defines the **product or service strategy** and **business model** of the enterprise.\n",
    "\n",
    "**Example:**  \n",
    "An e-commerce company wants:\n",
    "- 1-day delivery\n",
    "- Personalized product recommendations\n",
    "- Expansion to new regions\n",
    "\n",
    "**Data Engineer's Role:**  \n",
    "You need to understand these goals and build pipelines to track:\n",
    "- Order delivery times  \n",
    "- Customer behavior  \n",
    "- Sales by region\n",
    "\n",
    "---\n",
    "\n",
    "## 2. üèó Application Architecture  \n",
    "**Purpose:** Describes the **structure and interaction** of key applications that serve business needs.\n",
    "\n",
    "**Example:**  \n",
    "Flipkart might use:\n",
    "- Login Service  \n",
    "- Product Catalog  \n",
    "- Recommendation Engine  \n",
    "- Order Management  \n",
    "\n",
    "**Data Engineer's Role:**  \n",
    "You extract data via:\n",
    "- APIs  \n",
    "- Event streams (Kafka)  \n",
    "- Database snapshots  \n",
    "\n",
    "Then push it into the warehouse/lake for analytics.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. üñ• Technical Architecture  \n",
    "**Purpose:** Defines the **software and hardware** infrastructure (cloud, compute, network, tools).\n",
    "\n",
    "**Example:**  \n",
    "- AWS EC2 for compute  \n",
    "- S3 for storage  \n",
    "- Glue & Spark for processing  \n",
    "- Airflow for orchestration  \n",
    "- Terraform for IaC  \n",
    "\n",
    "**Data Engineer's Role:**  \n",
    "You build and deploy using:\n",
    "- Scalable, secure, cost-effective cloud resources  \n",
    "- Tools like Terraform to manage infrastructure as code  \n",
    "\n",
    "---\n",
    "\n",
    "## 4. üóÉ Data Architecture  \n",
    "**Purpose:** Supports the **evolving data needs** of the organization.\n",
    "\n",
    "**Example Workflow:**\n",
    "- Extract from MySQL (RDS)  \n",
    "- Transform via Glue into a star schema  \n",
    "- Store in S3 as Parquet  \n",
    "- Query via Athena  \n",
    "- Visualize via Jupyter or QuickSight  \n",
    "\n",
    "**Data Engineer's Role:**  \n",
    "Own the end-to-end data pipeline: from ingestion ‚Üí transformation ‚Üí serving.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Why It Matters to You as a Data Engineer\n",
    "\n",
    "- Your pipelines must **support changing business needs**\n",
    "- You must make **reversible choices** when possible (2-way doors)\n",
    "- You contribute not just to the tech, but to **how the organization runs**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf7b3c5",
   "metadata": {},
   "source": [
    "## Conway‚Äôs Law\n",
    "\n",
    "> **\"Any organization that designs a system will produce a design whose structure is a copy of the organization‚Äôs communication structure.\"**  \n",
    "> ‚Äî Melvin Conway\n",
    "\n",
    "### üîç What It Means\n",
    "\n",
    "Conway's Law suggests that the way teams **communicate and organize internally** will directly influence the **structure of the systems** they build.\n",
    "\n",
    "For example, if your company has separate departments that rarely collaborate (like Sales, Marketing, Finance, and Operations), each team may create their own isolated data systems ‚Äî leading to **siloed architectures**.\n",
    "\n",
    "üìâ **Siloed Teams = Siloed Systems**\n",
    "\n",
    "![Siloed Systems](./images/conway_law_1.png)\n",
    "\n",
    "But if the same departments work in a **collaborative, cross-functional** way ‚Äî communicating frequently ‚Äî the systems they build will be **more integrated and unified**.\n",
    "\n",
    "üìà **Collaborative Teams = Unified Architecture**\n",
    "\n",
    "![Unified System](./images/conway_law_2.png)\n",
    "\n",
    "### üßë‚Äçüíª Why It Matters to You as a Data Engineer\n",
    "\n",
    "Before designing a data architecture, **understand how your company communicates**:\n",
    "- Are teams siloed or cross-functional?\n",
    "- Do they share common goals or work in isolation?\n",
    "\n",
    "Even if your architecture looks perfect on paper, if it **clashes with your org's communication structure**, it‚Äôs likely to fail in practice.\n",
    "\n",
    "> ‚úÖ Good data architecture reflects how people in the company work together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ca9ef9",
   "metadata": {},
   "source": [
    "## Principles of Good Data Architecture\n",
    "\n",
    "![Principles of Good Data Architecture](./images/principles_data_archi.png)\n",
    "\n",
    "Data architecture is not just about tools ‚Äî it's about making smart, flexible, and impactful decisions that evolve with the organization.\n",
    "\n",
    "### üîπ Theme 1: How Architecture Affects Others\n",
    "- **Choose common components wisely**  \n",
    "  Use tools that benefit multiple teams (e.g., Git, S3, Spark).\n",
    "- **Architecture is leadership**  \n",
    "  Architects lead by enabling others, mentoring, and setting standards.\n",
    "\n",
    "### üîπ Theme 2: Architecture Is an Ongoing Process\n",
    "- **Always be architecting**  \n",
    "  Keep improving as needs change.\n",
    "- **Build loosely coupled systems**  \n",
    "  Make systems modular for flexibility.\n",
    "- **Make reversible decisions**  \n",
    "  Favor decisions you can back out of (e.g., changing storage classes).\n",
    "\n",
    "### üîπ Theme 3: Unspoken But Understood Priorities\n",
    "- **Plan for failure**  \n",
    "  Assume systems will break ‚Äî design for recovery.\n",
    "- **Architect for scalability**  \n",
    "  Plan ahead for data/user growth.\n",
    "- **Prioritize security**  \n",
    "  Build in data protection from day one.\n",
    "- **Embrace FinOps**  \n",
    "  Design systems with cost efficiency in mind.\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Common Components\n",
    "\n",
    "![Common Components](./images/common_components.png)\n",
    "\n",
    "Common components are tools and platforms shared across teams to increase efficiency and reduce duplication.\n",
    "\n",
    "### üîß Examples of Common Components\n",
    "- **Object Storage** ‚Äì like Amazon S3, shared by all teams  \n",
    "- **Version Control Systems** ‚Äì like Git for code collaboration  \n",
    "- **Monitoring & Observability** ‚Äì systems to track health/performance  \n",
    "- **Processing Engines** ‚Äì e.g., Spark, for distributed data processing\n",
    "\n",
    "Choosing common components well promotes collaboration, avoids silos, and reduces maintenance overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d70786",
   "metadata": {},
   "source": [
    "# Plan for Failure\n",
    "\n",
    "One of the key responsibilities of a data engineer is to **anticipate system failure** and design resilient, secure, and scalable architectures that minimize its impact. This principle is broken down into the following components:\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Availability\n",
    "\n",
    "> **Definition**: The percentage of time an IT service or component is expected to be in an operable state.\n",
    "\n",
    "Availability ensures users can access the system when needed. For example:\n",
    "\n",
    "- **Amazon S3 One Zone-IA**: 99.5% (‚âà 44 hours downtime/year)\n",
    "- **Amazon S3 Standard**: 99.99% (‚âà 1 hour downtime/year)\n",
    "\n",
    "![Availability](./images/plf_availability.png)\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Reliability\n",
    "\n",
    "> **Definition**: The probability of a service or component performing its intended function within a specific time interval.\n",
    "\n",
    "Reliable systems operate predictably and meet defined performance standards, ensuring smooth user experience.\n",
    "\n",
    "![Reliability](./images/plf_reliability.png)\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Durability\n",
    "\n",
    "> **Definition**: The ability of a system to **withstand data loss** from hardware failures, software bugs, or natural disasters.\n",
    "\n",
    "Durability ensures data is not lost. For instance, **Amazon S3** offers 99.999999999% durability ‚Äî also called **11 nines**.\n",
    "\n",
    "![Durability](./images/plf_durability.png)\n",
    "\n",
    "---\n",
    "\n",
    "## üîê Prioritize Security\n",
    "\n",
    "Security is critical to prevent breaches and ensure that failures do not lead to data loss or corruption.\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "- üõ°Ô∏è Build a **Culture of Security**\n",
    "- üîë Apply **Principle of Least Privilege**\n",
    "- üö´ Adopt **Zero-Trust Security** (no implicit trust, every action must be authenticated)\n",
    "\n",
    "![Security](./images/prioritize_security.png)\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Recovery Objectives\n",
    "\n",
    "Understanding **how quickly** and **how much data** can be recovered helps mitigate risks during failures.\n",
    "\n",
    "- **RTO (Recovery Time Objective)**: Max acceptable outage time.\n",
    "- **RPO (Recovery Point Objective)**: Max acceptable data loss after recovery.\n",
    "\n",
    "These guide architectural decisions such as storage class or backup frequency.\n",
    "\n",
    "![RTO and RPO](./images/rto_rpo.png)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Architect for Scalability & üí∞ Embrace FinOps\n",
    "\n",
    "Anticipating load spikes or failures also means building systems that can scale cost-effectively.\n",
    "\n",
    "### Risks:\n",
    "- üî∫ Unforeseen high cloud costs\n",
    "- üîª Lost revenue due to system crashes during peak demand\n",
    "\n",
    "**Recommendations:**\n",
    "- Use **on-demand vs spot instances** wisely\n",
    "- Optimize for **cost and performance**\n",
    "- Build elastic systems that **scale up or down as needed**\n",
    "\n",
    "![Scalability and FinOps](./images/archi_for_scalability.png)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "A good data engineer doesn‚Äôt just build systems that work under ideal conditions ‚Äî they build systems that:\n",
    "\n",
    "- Remain **available** and **reliable** under stress  \n",
    "- **Protect data** from loss and corruption  \n",
    "- Are **secure** by design  \n",
    "- Are **cost-effective** and **scalable**  \n",
    "- Are built with **failure recovery** in mind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0a32b6",
   "metadata": {},
   "source": [
    "## üß± Batch Data Architectures: ETL vs ELT\n",
    "\n",
    "Batch data architectures process data in **fixed intervals (batches)** rather than in real-time. Two popular patterns are:\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ ETL ‚Äì Extract, Transform, Load\n",
    "\n",
    "ETL is the **traditional data pipeline** used when real-time analysis isn't required.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Extract** data from source systems (databases, files, APIs).\n",
    "2. **Transform** data in a staging area (cleaning, aggregating, standardizing).\n",
    "3. **Load** transformed data into a data warehouse.\n",
    "\n",
    "![ETL](./images/etl.png)\n",
    "\n",
    "**‚úÖ When to use ETL:**\n",
    "- Your transformations are **complex** or require **external processing tools**.\n",
    "- You want **control over data quality** before loading.\n",
    "- You‚Äôre working with **smaller volumes** of data.\n",
    "- You‚Äôre using legacy or on-premise systems.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ ELT ‚Äì Extract, Load, Transform\n",
    "\n",
    "ELT is a **modern pattern** made possible by the power of cloud data warehouses (e.g., BigQuery, Snowflake).\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Extract** data from sources.\n",
    "2. **Load** raw data directly into the data warehouse.\n",
    "3. **Transform** inside the warehouse using SQL or built-in tools.\n",
    "\n",
    "![ELT](./images/elt.png)\n",
    "\n",
    "**‚úÖ When to use ELT:**\n",
    "- Your data warehouse supports **high-performance computation**.\n",
    "- You‚Äôre dealing with **large-scale data** (big data).\n",
    "- You want to **defer transformation** to be more flexible for analysis.\n",
    "- You prefer a **schema-on-read** approach (raw data first, model later).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öñÔ∏è Trade-offs Between ETL and ELT\n",
    "\n",
    "| Criteria         | ETL                                | ELT                                |\n",
    "|------------------|-------------------------------------|-------------------------------------|\n",
    "| **Flexibility**  | Less flexible (schema-on-write)     | More flexible (schema-on-read)      |\n",
    "| **Speed**        | Slower (transformation before load) | Faster ingestion                    |\n",
    "| **Complexity**   | Complex transformations outside DB  | Simplified using SQL in warehouse   |\n",
    "| **Use Case**     | Legacy systems, strict governance   | Modern cloud data platforms         |\n",
    "| **Cost**         | May require external tools          | Optimized using warehouse compute   |\n",
    "\n",
    "---\n",
    "\n",
    "**In summary**:  \n",
    "- Use **ETL** when you need control over transformation before data reaches your warehouse.  \n",
    "- Use **ELT** to **leverage warehouse power** and increase flexibility in modeling and analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7c9202",
   "metadata": {},
   "source": [
    "# ‚õìÔ∏è Lambda, ‚ö° Kappa, and üîÅ Unified Architectures in Data Engineering\n",
    "\n",
    "## ‚öôÔ∏è Streaming Frameworks\n",
    "\n",
    "Before diving into the architectures, it's important to know the tools that enable real-time data processing:\n",
    "\n",
    "![Streaming Frameworks](./images/streaming_frameworks.png)\n",
    "\n",
    "- **Apache Kafka**: A distributed event streaming platform that stores and transports events reliably at scale.\n",
    "- **Apache Storm**: A real-time computation system for processing unbounded streams of data.\n",
    "- **Apache Samza**: Works with Kafka to process event streams in near real time.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚õìÔ∏è Lambda Architecture\n",
    "\n",
    "![Lambda Architecture](./images/lambda_archi.png)\n",
    "\n",
    "### üí° What It Is:\n",
    "Lambda uses **two parallel pipelines** ‚Äî one for batch, one for streaming ‚Äî to handle both historical and real-time data.\n",
    "\n",
    "### üîÑ How It Works:\n",
    "- **Batch Layer**:\n",
    "  - Processes historical data in large chunks.\n",
    "  - Uses a data warehouse (e.g., BigQuery, Redshift) for storage and querying.\n",
    "- **Speed (Streaming) Layer**:\n",
    "  - Handles real-time data from sources like Kafka.\n",
    "  - Stores output in a NoSQL database (e.g., Cassandra).\n",
    "- **Serving Layer**:\n",
    "  - Combines both outputs to deliver a **complete view** for dashboards or ML models.\n",
    "\n",
    "### ‚úÖ Pros:\n",
    "- Supports both fresh (stream) and comprehensive (batch) data.\n",
    "- Can serve accurate analytics with mixed granularity.\n",
    "\n",
    "### ‚ùå Cons:\n",
    "- Requires maintaining **two separate pipelines**.\n",
    "- Duplicate logic and maintenance effort.\n",
    "- Possible inconsistency between batch and stream outputs.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö° Kappa Architecture\n",
    "\n",
    "![Kappa Architecture](./images/kappa_archi.png)\n",
    "\n",
    "### üí° What It Is:\n",
    "Kappa eliminates the batch pipeline and uses only **streaming**. It treats all data as events and enables reprocessing from historical streams.\n",
    "\n",
    "### üîÑ How It Works:\n",
    "- Data from **source systems** flows into a **stream processing engine** (e.g., Kafka Streams, Flink).\n",
    "- The processed data feeds into a **single serving layer** for querying and consumption.\n",
    "- **Historical replay**: Because Kafka can retain logs, older data can be reprocessed if logic changes.\n",
    "\n",
    "### ‚úÖ Pros:\n",
    "- **Simpler** than Lambda ‚Äî only one codebase to maintain.\n",
    "- Supports real-time and reprocessing use cases.\n",
    "\n",
    "### ‚ùå Cons:\n",
    "- Not great for very large historical aggregations.\n",
    "- Needs long-term stream retention if historical replays are required.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Unified Batch & Streaming Architecture\n",
    "\n",
    "![Unified Architecture](./images/unified_batch_streaming.png)\n",
    "\n",
    "### üí° What It Is:\n",
    "Unified architecture views **batch as a special case of streaming**. Uses a **single codebase** for both.\n",
    "\n",
    "### üîÑ How It Works:\n",
    "- Treats all data as events:\n",
    "  - Real-time data = **unbounded event streams**\n",
    "  - Batch = **bounded slices** of event streams (e.g., hourly/daily windows)\n",
    "- Stream processors (e.g., **Apache Beam**, **Apache Flink**, **Google Dataflow**) can apply the same transformations on both batch and streaming data.\n",
    "- Output goes to warehouses, dashboards, ML pipelines, etc.\n",
    "\n",
    "### ‚úÖ Pros:\n",
    "- One codebase = less duplication, easier maintenance.\n",
    "- Flexible ‚Äî adapts easily as batch or streaming.\n",
    "- Scalable and modern.\n",
    "\n",
    "### ‚ùå Cons:\n",
    "- Steeper learning curve.\n",
    "- Needs modern infrastructure and tools to implement correctly.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Summary\n",
    "\n",
    "| Feature            | Lambda                    | Kappa                    | Unified Batch & Streaming    |\n",
    "|--------------------|----------------------------|--------------------------|-------------------------------|\n",
    "| Pipelines          | Batch + Stream             | Stream only              | Single (unified)              |\n",
    "| Codebase           | Two                        | One                      | One                           |\n",
    "| Complexity         | High                       | Medium                   | Medium                        |\n",
    "| Historical Replay  | Yes (via batch)            | Yes (via stream replay)  | Yes (via event windowing)     |\n",
    "| Tools              | Kafka, Hadoop, Cassandra   | Kafka, Flink             | Beam, Flink, Dataflow         |\n",
    "\n",
    "---\n",
    "\n",
    "> ‚ö†Ô∏è Tip: In modern data engineering, **Unified** is the most preferred approach. But Kappa is a great real-time-first alternative when batch is not a priority. Lambda is more legacy but still seen in enterprises transitioning to real-time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
