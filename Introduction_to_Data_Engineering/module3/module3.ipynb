{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a98483f",
   "metadata": {},
   "source": [
    "# 📚 Index\n",
    "\n",
    "- [Enterprise Architecture](#enterprise-architecture)\n",
    "\n",
    "- [Conway’s Law](#conways-law)\n",
    "\n",
    "- [Principles of Good Data Architecture](#principles-of-good-data-architecture)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe58e9f",
   "metadata": {},
   "source": [
    "# Enterprise Architecture\n",
    "\n",
    "Enterprise Architecture is the design of systems to support **change** in an enterprise — achieved by **flexible and reversible decisions** through careful evaluation of trade-offs.\n",
    "\n",
    "It consists of **4 core components**:\n",
    "\n",
    "![Enterprise Architecture Diagram](./images/enterprice_architecture.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 🧩 Business Architecture  \n",
    "**Purpose:** Defines the **product or service strategy** and **business model** of the enterprise.\n",
    "\n",
    "**Example:**  \n",
    "An e-commerce company wants:\n",
    "- 1-day delivery\n",
    "- Personalized product recommendations\n",
    "- Expansion to new regions\n",
    "\n",
    "**Data Engineer's Role:**  \n",
    "You need to understand these goals and build pipelines to track:\n",
    "- Order delivery times  \n",
    "- Customer behavior  \n",
    "- Sales by region\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 🏗 Application Architecture  \n",
    "**Purpose:** Describes the **structure and interaction** of key applications that serve business needs.\n",
    "\n",
    "**Example:**  \n",
    "Flipkart might use:\n",
    "- Login Service  \n",
    "- Product Catalog  \n",
    "- Recommendation Engine  \n",
    "- Order Management  \n",
    "\n",
    "**Data Engineer's Role:**  \n",
    "You extract data via:\n",
    "- APIs  \n",
    "- Event streams (Kafka)  \n",
    "- Database snapshots  \n",
    "\n",
    "Then push it into the warehouse/lake for analytics.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 🖥 Technical Architecture  \n",
    "**Purpose:** Defines the **software and hardware** infrastructure (cloud, compute, network, tools).\n",
    "\n",
    "**Example:**  \n",
    "- AWS EC2 for compute  \n",
    "- S3 for storage  \n",
    "- Glue & Spark for processing  \n",
    "- Airflow for orchestration  \n",
    "- Terraform for IaC  \n",
    "\n",
    "**Data Engineer's Role:**  \n",
    "You build and deploy using:\n",
    "- Scalable, secure, cost-effective cloud resources  \n",
    "- Tools like Terraform to manage infrastructure as code  \n",
    "\n",
    "---\n",
    "\n",
    "## 4. 🗃 Data Architecture  \n",
    "**Purpose:** Supports the **evolving data needs** of the organization.\n",
    "\n",
    "**Example Workflow:**\n",
    "- Extract from MySQL (RDS)  \n",
    "- Transform via Glue into a star schema  \n",
    "- Store in S3 as Parquet  \n",
    "- Query via Athena  \n",
    "- Visualize via Jupyter or QuickSight  \n",
    "\n",
    "**Data Engineer's Role:**  \n",
    "Own the end-to-end data pipeline: from ingestion → transformation → serving.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Why It Matters to You as a Data Engineer\n",
    "\n",
    "- Your pipelines must **support changing business needs**\n",
    "- You must make **reversible choices** when possible (2-way doors)\n",
    "- You contribute not just to the tech, but to **how the organization runs**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf7b3c5",
   "metadata": {},
   "source": [
    "## Conway’s Law\n",
    "\n",
    "> **\"Any organization that designs a system will produce a design whose structure is a copy of the organization’s communication structure.\"**  \n",
    "> — Melvin Conway\n",
    "\n",
    "### 🔍 What It Means\n",
    "\n",
    "Conway's Law suggests that the way teams **communicate and organize internally** will directly influence the **structure of the systems** they build.\n",
    "\n",
    "For example, if your company has separate departments that rarely collaborate (like Sales, Marketing, Finance, and Operations), each team may create their own isolated data systems — leading to **siloed architectures**.\n",
    "\n",
    "📉 **Siloed Teams = Siloed Systems**\n",
    "\n",
    "![Siloed Systems](./images/conway_law_1.png)\n",
    "\n",
    "But if the same departments work in a **collaborative, cross-functional** way — communicating frequently — the systems they build will be **more integrated and unified**.\n",
    "\n",
    "📈 **Collaborative Teams = Unified Architecture**\n",
    "\n",
    "![Unified System](./images/conway_law_2.png)\n",
    "\n",
    "### 🧑‍💻 Why It Matters to You as a Data Engineer\n",
    "\n",
    "Before designing a data architecture, **understand how your company communicates**:\n",
    "- Are teams siloed or cross-functional?\n",
    "- Do they share common goals or work in isolation?\n",
    "\n",
    "Even if your architecture looks perfect on paper, if it **clashes with your org's communication structure**, it’s likely to fail in practice.\n",
    "\n",
    "> ✅ Good data architecture reflects how people in the company work together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ca9ef9",
   "metadata": {},
   "source": [
    "## Principles of Good Data Architecture\n",
    "\n",
    "![Principles of Good Data Architecture](./images/principles_data_archi.png)\n",
    "\n",
    "Data architecture is not just about tools — it's about making smart, flexible, and impactful decisions that evolve with the organization.\n",
    "\n",
    "### 🔹 Theme 1: How Architecture Affects Others\n",
    "- **Choose common components wisely**  \n",
    "  Use tools that benefit multiple teams (e.g., Git, S3, Spark).\n",
    "- **Architecture is leadership**  \n",
    "  Architects lead by enabling others, mentoring, and setting standards.\n",
    "\n",
    "### 🔹 Theme 2: Architecture Is an Ongoing Process\n",
    "- **Always be architecting**  \n",
    "  Keep improving as needs change.\n",
    "- **Build loosely coupled systems**  \n",
    "  Make systems modular for flexibility.\n",
    "- **Make reversible decisions**  \n",
    "  Favor decisions you can back out of (e.g., changing storage classes).\n",
    "\n",
    "### 🔹 Theme 3: Unspoken But Understood Priorities\n",
    "- **Plan for failure**  \n",
    "  Assume systems will break — design for recovery.\n",
    "- **Architect for scalability**  \n",
    "  Plan ahead for data/user growth.\n",
    "- **Prioritize security**  \n",
    "  Build in data protection from day one.\n",
    "- **Embrace FinOps**  \n",
    "  Design systems with cost efficiency in mind.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔗 Common Components\n",
    "\n",
    "![Common Components](./images/common_components.png)\n",
    "\n",
    "Common components are tools and platforms shared across teams to increase efficiency and reduce duplication.\n",
    "\n",
    "### 🔧 Examples of Common Components\n",
    "- **Object Storage** – like Amazon S3, shared by all teams  \n",
    "- **Version Control Systems** – like Git for code collaboration  \n",
    "- **Monitoring & Observability** – systems to track health/performance  \n",
    "- **Processing Engines** – e.g., Spark, for distributed data processing\n",
    "\n",
    "Choosing common components well promotes collaboration, avoids silos, and reduces maintenance overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d70786",
   "metadata": {},
   "source": [
    "# Plan for Failure\n",
    "\n",
    "One of the key responsibilities of a data engineer is to **anticipate system failure** and design resilient, secure, and scalable architectures that minimize its impact. This principle is broken down into the following components:\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Availability\n",
    "\n",
    "> **Definition**: The percentage of time an IT service or component is expected to be in an operable state.\n",
    "\n",
    "Availability ensures users can access the system when needed. For example:\n",
    "\n",
    "- **Amazon S3 One Zone-IA**: 99.5% (≈ 44 hours downtime/year)\n",
    "- **Amazon S3 Standard**: 99.99% (≈ 1 hour downtime/year)\n",
    "\n",
    "![Availability](./images/plf_availability.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Reliability\n",
    "\n",
    "> **Definition**: The probability of a service or component performing its intended function within a specific time interval.\n",
    "\n",
    "Reliable systems operate predictably and meet defined performance standards, ensuring smooth user experience.\n",
    "\n",
    "![Reliability](./images/plf_reliability.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Durability\n",
    "\n",
    "> **Definition**: The ability of a system to **withstand data loss** from hardware failures, software bugs, or natural disasters.\n",
    "\n",
    "Durability ensures data is not lost. For instance, **Amazon S3** offers 99.999999999% durability — also called **11 nines**.\n",
    "\n",
    "![Durability](./images/plf_durability.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 🔐 Prioritize Security\n",
    "\n",
    "Security is critical to prevent breaches and ensure that failures do not lead to data loss or corruption.\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "- 🛡️ Build a **Culture of Security**\n",
    "- 🔑 Apply **Principle of Least Privilege**\n",
    "- 🚫 Adopt **Zero-Trust Security** (no implicit trust, every action must be authenticated)\n",
    "\n",
    "![Security](./images/prioritize_security.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ Recovery Objectives\n",
    "\n",
    "Understanding **how quickly** and **how much data** can be recovered helps mitigate risks during failures.\n",
    "\n",
    "- **RTO (Recovery Time Objective)**: Max acceptable outage time.\n",
    "- **RPO (Recovery Point Objective)**: Max acceptable data loss after recovery.\n",
    "\n",
    "These guide architectural decisions such as storage class or backup frequency.\n",
    "\n",
    "![RTO and RPO](./images/rto_rpo.png)\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Architect for Scalability & 💰 Embrace FinOps\n",
    "\n",
    "Anticipating load spikes or failures also means building systems that can scale cost-effectively.\n",
    "\n",
    "### Risks:\n",
    "- 🔺 Unforeseen high cloud costs\n",
    "- 🔻 Lost revenue due to system crashes during peak demand\n",
    "\n",
    "**Recommendations:**\n",
    "- Use **on-demand vs spot instances** wisely\n",
    "- Optimize for **cost and performance**\n",
    "- Build elastic systems that **scale up or down as needed**\n",
    "\n",
    "![Scalability and FinOps](./images/archi_for_scalability.png)\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Summary\n",
    "\n",
    "A good data engineer doesn’t just build systems that work under ideal conditions — they build systems that:\n",
    "\n",
    "- Remain **available** and **reliable** under stress  \n",
    "- **Protect data** from loss and corruption  \n",
    "- Are **secure** by design  \n",
    "- Are **cost-effective** and **scalable**  \n",
    "- Are built with **failure recovery** in mind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0a32b6",
   "metadata": {},
   "source": [
    "## 🧱 Batch Data Architectures: ETL vs ELT\n",
    "\n",
    "Batch data architectures process data in **fixed intervals (batches)** rather than in real-time. Two popular patterns are:\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 ETL – Extract, Transform, Load\n",
    "\n",
    "ETL is the **traditional data pipeline** used when real-time analysis isn't required.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Extract** data from source systems (databases, files, APIs).\n",
    "2. **Transform** data in a staging area (cleaning, aggregating, standardizing).\n",
    "3. **Load** transformed data into a data warehouse.\n",
    "\n",
    "![ETL](./images/etl.png)\n",
    "\n",
    "**✅ When to use ETL:**\n",
    "- Your transformations are **complex** or require **external processing tools**.\n",
    "- You want **control over data quality** before loading.\n",
    "- You’re working with **smaller volumes** of data.\n",
    "- You’re using legacy or on-premise systems.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 ELT – Extract, Load, Transform\n",
    "\n",
    "ELT is a **modern pattern** made possible by the power of cloud data warehouses (e.g., BigQuery, Snowflake).\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Extract** data from sources.\n",
    "2. **Load** raw data directly into the data warehouse.\n",
    "3. **Transform** inside the warehouse using SQL or built-in tools.\n",
    "\n",
    "![ELT](./images/elt.png)\n",
    "\n",
    "**✅ When to use ELT:**\n",
    "- Your data warehouse supports **high-performance computation**.\n",
    "- You’re dealing with **large-scale data** (big data).\n",
    "- You want to **defer transformation** to be more flexible for analysis.\n",
    "- You prefer a **schema-on-read** approach (raw data first, model later).\n",
    "\n",
    "---\n",
    "\n",
    "### ⚖️ Trade-offs Between ETL and ELT\n",
    "\n",
    "| Criteria         | ETL                                | ELT                                |\n",
    "|------------------|-------------------------------------|-------------------------------------|\n",
    "| **Flexibility**  | Less flexible (schema-on-write)     | More flexible (schema-on-read)      |\n",
    "| **Speed**        | Slower (transformation before load) | Faster ingestion                    |\n",
    "| **Complexity**   | Complex transformations outside DB  | Simplified using SQL in warehouse   |\n",
    "| **Use Case**     | Legacy systems, strict governance   | Modern cloud data platforms         |\n",
    "| **Cost**         | May require external tools          | Optimized using warehouse compute   |\n",
    "\n",
    "---\n",
    "\n",
    "**In summary**:  \n",
    "- Use **ETL** when you need control over transformation before data reaches your warehouse.  \n",
    "- Use **ELT** to **leverage warehouse power** and increase flexibility in modeling and analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7c9202",
   "metadata": {},
   "source": [
    "# ⛓️ Lambda, ⚡ Kappa, and 🔁 Unified Architectures in Data Engineering\n",
    "\n",
    "## ⚙️ Streaming Frameworks\n",
    "\n",
    "Before diving into the architectures, it's important to know the tools that enable real-time data processing:\n",
    "\n",
    "![Streaming Frameworks](./images/streaming_frameworks.png)\n",
    "\n",
    "- **Apache Kafka**: A distributed event streaming platform that stores and transports events reliably at scale.\n",
    "- **Apache Storm**: A real-time computation system for processing unbounded streams of data.\n",
    "- **Apache Samza**: Works with Kafka to process event streams in near real time.\n",
    "\n",
    "---\n",
    "\n",
    "## ⛓️ Lambda Architecture\n",
    "\n",
    "![Lambda Architecture](./images/lambda_archi.png)\n",
    "\n",
    "### 💡 What It Is:\n",
    "Lambda uses **two parallel pipelines** — one for batch, one for streaming — to handle both historical and real-time data.\n",
    "\n",
    "### 🔄 How It Works:\n",
    "- **Batch Layer**:\n",
    "  - Processes historical data in large chunks.\n",
    "  - Uses a data warehouse (e.g., BigQuery, Redshift) for storage and querying.\n",
    "- **Speed (Streaming) Layer**:\n",
    "  - Handles real-time data from sources like Kafka.\n",
    "  - Stores output in a NoSQL database (e.g., Cassandra).\n",
    "- **Serving Layer**:\n",
    "  - Combines both outputs to deliver a **complete view** for dashboards or ML models.\n",
    "\n",
    "### ✅ Pros:\n",
    "- Supports both fresh (stream) and comprehensive (batch) data.\n",
    "- Can serve accurate analytics with mixed granularity.\n",
    "\n",
    "### ❌ Cons:\n",
    "- Requires maintaining **two separate pipelines**.\n",
    "- Duplicate logic and maintenance effort.\n",
    "- Possible inconsistency between batch and stream outputs.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚡ Kappa Architecture\n",
    "\n",
    "![Kappa Architecture](./images/kappa_archi.png)\n",
    "\n",
    "### 💡 What It Is:\n",
    "Kappa eliminates the batch pipeline and uses only **streaming**. It treats all data as events and enables reprocessing from historical streams.\n",
    "\n",
    "### 🔄 How It Works:\n",
    "- Data from **source systems** flows into a **stream processing engine** (e.g., Kafka Streams, Flink).\n",
    "- The processed data feeds into a **single serving layer** for querying and consumption.\n",
    "- **Historical replay**: Because Kafka can retain logs, older data can be reprocessed if logic changes.\n",
    "\n",
    "### ✅ Pros:\n",
    "- **Simpler** than Lambda — only one codebase to maintain.\n",
    "- Supports real-time and reprocessing use cases.\n",
    "\n",
    "### ❌ Cons:\n",
    "- Not great for very large historical aggregations.\n",
    "- Needs long-term stream retention if historical replays are required.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Unified Batch & Streaming Architecture\n",
    "\n",
    "![Unified Architecture](./images/unified_batch_streaming.png)\n",
    "\n",
    "### 💡 What It Is:\n",
    "Unified architecture views **batch as a special case of streaming**. Uses a **single codebase** for both.\n",
    "\n",
    "### 🔄 How It Works:\n",
    "- Treats all data as events:\n",
    "  - Real-time data = **unbounded event streams**\n",
    "  - Batch = **bounded slices** of event streams (e.g., hourly/daily windows)\n",
    "- Stream processors (e.g., **Apache Beam**, **Apache Flink**, **Google Dataflow**) can apply the same transformations on both batch and streaming data.\n",
    "- Output goes to warehouses, dashboards, ML pipelines, etc.\n",
    "\n",
    "### ✅ Pros:\n",
    "- One codebase = less duplication, easier maintenance.\n",
    "- Flexible — adapts easily as batch or streaming.\n",
    "- Scalable and modern.\n",
    "\n",
    "### ❌ Cons:\n",
    "- Steeper learning curve.\n",
    "- Needs modern infrastructure and tools to implement correctly.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Summary\n",
    "\n",
    "| Feature            | Lambda                    | Kappa                    | Unified Batch & Streaming    |\n",
    "|--------------------|----------------------------|--------------------------|-------------------------------|\n",
    "| Pipelines          | Batch + Stream             | Stream only              | Single (unified)              |\n",
    "| Codebase           | Two                        | One                      | One                           |\n",
    "| Complexity         | High                       | Medium                   | Medium                        |\n",
    "| Historical Replay  | Yes (via batch)            | Yes (via stream replay)  | Yes (via event windowing)     |\n",
    "| Tools              | Kafka, Hadoop, Cassandra   | Kafka, Flink             | Beam, Flink, Dataflow         |\n",
    "\n",
    "---\n",
    "\n",
    "> ⚠️ Tip: In modern data engineering, **Unified** is the most preferred approach. But Kappa is a great real-time-first alternative when batch is not a priority. Lambda is more legacy but still seen in enterprises transitioning to real-time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
