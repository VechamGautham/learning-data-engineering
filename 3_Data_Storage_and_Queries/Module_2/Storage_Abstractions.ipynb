{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53b1364d",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [Data Warehouse - Overview & Architecture](#data-warehouse--overview--architecture)\n",
    "- [Database vs Data Lkae vs Data Warehouse](#database-vs-data-lake-vs-data-warehouse--comparison-table)\n",
    "- [Cloud Data Warehouse and MPP Architecture](#cloud-data-warehouse--mpp-architecture)\n",
    "- [Data Lake - Overview & Shortcomings](#data-lake--overview--shortcomings)\n",
    "- [Organizing and Managing Data in Data Lake](#organizing-and-managing-data-in-data-lakes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09adf05c",
   "metadata": {},
   "source": [
    "# Data Warehouse â€” Overview & Architecture\n",
    "\n",
    "## ğŸ“Œ Key Characteristics of a Data Warehouse\n",
    "\n",
    "![Data Warehouse](./images/data_warehouse.png)\n",
    "\n",
    "- **Subject-Oriented** ğŸ§   \n",
    "  Organizes and stores data around key business domains such as Customers, Products, Sales, Finance.  \n",
    "  â†’ Data is modeled to support **decision-making**, not transactions.\n",
    "\n",
    "- **Integrated** ğŸ”—  \n",
    "  Combines data from multiple, heterogeneous sources into a **consistent schema**.\n",
    "\n",
    "- **Non-Volatile** ğŸ“„  \n",
    "  Data is **read-only** â€” it cannot be updated or deleted.  \n",
    "  â†’ Preserves snapshots for historical analysis.\n",
    "\n",
    "- **Time-Variant** ğŸ•°  \n",
    "  Stores both **current and historical** data, unlike OLTP systems.  \n",
    "  â†’ Enables trend analysis over time.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§± Data Warehouseâ€“Centric Architecture (ETL Pattern)\n",
    "\n",
    "![Data Warehouse ETL Architecture](./images/data_warehouse-centric_architecture.png)\n",
    "\n",
    "1. **Extract**  \n",
    "   Pull data from various operational sources â€” databases, APIs, files, etc.\n",
    "\n",
    "2. **Transform**  \n",
    "   Clean, standardize, and model the data in a **staging area**.\n",
    "\n",
    "3. **Load**  \n",
    "   Push transformed data into the **Data Warehouse** with a comprehensive schema.\n",
    "\n",
    "4. **Data Marts** ğŸª  \n",
    "   - Department-specific subsets (e.g. Sales, Marketing, Finance).  \n",
    "   - Often follow **simplified or denormalized schemas**.  \n",
    "   - Improve query performance for specific use cases.\n",
    "\n",
    "5. **Analytics & Reports** ğŸ“Š  \n",
    "   BI tools and analysts use the Data Marts & Warehouse for dashboards and decision-making.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”„ Change Data Capture (CDC)\n",
    "\n",
    "![Change Data Capture](./images/data_warehouse-centric_archi_cdc.png)\n",
    "\n",
    "- Instead of extracting the **entire dataset** every time,  \n",
    "  **CDC tracks only changes** (inserts, updates, deletes) in the source systems.\n",
    "\n",
    "- Reduces load on production OLTP databases âœ…  \n",
    "- Keeps the Data Warehouse **incrementally in sync** with source systems.  \n",
    "- Commonly implemented using ETL pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "## âš¡ Evolution of Data Warehouse Implementations\n",
    "\n",
    "![Data Warehouse Implementation](./images/data_warehouse_implementations.png)\n",
    "\n",
    "| Era | Architecture | Key Features |\n",
    "|-----|-------------|--------------|\n",
    "| **Early DW** ğŸ§± | Monolithic servers | Limited performance, single big machine |\n",
    "| **MPP DW** âš™ï¸ | Massively Parallel Processing | Distributes queries across multiple nodes, scans data in parallel |\n",
    "| **Modern Cloud DW** â˜ï¸ | Snowflake, BigQuery, Redshift | Separates **compute & storage**, scales elastically, cost-efficient |\n",
    "\n",
    "### âœ¨ Modern Cloud Data Warehouses\n",
    "- **Amazon Redshift**  \n",
    "- **Google BigQuery**  \n",
    "- **Snowflake**\n",
    "\n",
    "âœ… **Key Advantage**:  \n",
    "- Compute and storage are **independent**, allowing cost-effective scaling.  \n",
    "- Ideal for analytical workloads on very large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Recap\n",
    "\n",
    "- OLTP systems are designed for **transactions**, not heavy analytics.  \n",
    "- Data Warehouses were introduced to:\n",
    "  - Consolidate data from multiple sources\n",
    "  - Provide **historical context**\n",
    "  - Enable **analytical queries** efficiently\n",
    "- **ETL pipelines** â†’ clean & load data into a **centralized warehouse**  \n",
    "- **Data Marts** â†’ business functionâ€“specific views for easier analysis  \n",
    "- **CDC** â†’ keeps data up to date without full reloads  \n",
    "- **Modern DW** leverage **cloud + MPP** for scale & performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bc90e8",
   "metadata": {},
   "source": [
    "# Database vs Data Lake vs Data Warehouse â€” Comparison Table\n",
    "\n",
    "| Feature ğŸ§                          | Database (OLTP) ğŸ§¾                          | Data Lake ğŸŒŠ                                      | Data Warehouse ğŸ¢                                                   |\n",
    "|-------------------------------------|---------------------------------------------|--------------------------------------------------|---------------------------------------------------------------------|\n",
    "| **Primary Purpose**                | Handle **day-to-day transactions**         | Store **large volumes of raw / semi-structured data** cheaply | **Analytics, reporting & decision-making**                         |\n",
    "| **Schema**                          | Strict, normalized                         | Flexible or **schema-on-read**                   | Structured, **modeled** (e.g. Star / Snowflake schema)              |\n",
    "| **Data Type**                       | Mostly structured                          | Structured, semi-structured, unstructured        | Structured, cleaned & integrated                                   |\n",
    "| **Data Processing**                | OLTP (row-based inserts/updates)          | Batch or streaming (big data frameworks)        | OLAP (columnar scans, aggregations, joins)                         |\n",
    "| **Storage Layer**                  | Disk + RAM (local)                         | Disk / Object storage (e.g., S3, ADLS, HDFS)     | Disk-based (often columnar format on cloud object storage)         |\n",
    "| **RAM Usage**                      | High â€” for fast concurrent writes & lookups | Low for storage; RAM needed only during Spark/compute jobs | Moderate; used mainly during **query execution**, not for storage |\n",
    "| **Query Patterns**                 | Fast reads/writes on individual records   | Heavy ETL/ELT, ML workloads, large scans        | Analytical queries, aggregations, dashboards                       |\n",
    "| **Performance Optimization**       | Indexes, B-trees, in-memory caching       | Parallel processing, schema-on-read            | Columnar storage, MPP (Massively Parallel Processing), caching     |\n",
    "| **Cost**                            | Expensive to scale for big analytics       | Cheap storage, variable compute cost            | Cost-effective analytics, separates **compute & storage** in modern cloud DWs |\n",
    "| **Data Freshness**                 | Real-time (transactions)                  | Near real-time or batch                        | Batch or near real-time (depending on ETL/CDC setup)               |\n",
    "| **Examples**                        | MySQL, PostgreSQL, Oracle, SQL Server     | S3, ADLS, HDFS, Delta Lake                      | Snowflake, BigQuery, Redshift, Synapse                             |\n",
    "| **Best For**                        | Transactional apps, operational systems   | Data exploration, staging, ML feature stores   | Business intelligence, dashboards, historical analysis             |\n",
    "\n",
    "---\n",
    "\n",
    "## âœ¨ Key Insights\n",
    "\n",
    "- ğŸ§¾ **Databases** are great for **fast, reliable transactions** but are **not optimized for analytical queries** over large datasets.  \n",
    "- ğŸŒŠ **Data Lakes** are great for **raw, large-scale data storage** at low cost, but **query performance depends on the compute engine** (e.g., Spark).  \n",
    "- ğŸ¢ **Data Warehouses** sit in between: they combine structured modeling, columnar storage, and parallel processing to support **high-performance analytics**.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Final One-liner\n",
    "\n",
    "> â€œA **database** is built for transactions, a **data lake** is built for storage, and a **data warehouse** is built for analytics.â€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171b4176",
   "metadata": {},
   "source": [
    "# Cloud Data Warehouse & MPP Architecture \n",
    "\n",
    "## ğŸ§  1. MPP Architecture for Amazon Redshift\n",
    "\n",
    "![MPP Architecture](./images/mpp_archi.png)\n",
    "\n",
    "- **Redshift Cluster** = Collection of computing resources.\n",
    "- Consists of:\n",
    "  - ğŸ§  **Leader Node** â†’ Coordinates the cluster, plans queries.\n",
    "  - ğŸ’» **Compute Nodes** â†’ Store data & execute query steps in parallel.\n",
    "- Each compute node is divided into **Slices** ğŸ§©:\n",
    "  - Each slice gets a share of **CPU**, **memory**, and **disk**.\n",
    "  - Data is distributed across slices for parallel processing.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ 2. How MPP Query Execution Works\n",
    "\n",
    "![MPP Execution Steps](./images/mpp_archi_steps.png)\n",
    "\n",
    "1. **Client Application** connects to Redshift using JDBC/ODBC.  \n",
    "2. The **Leader Node**:\n",
    "   - Parses the SQL query.  \n",
    "   - Generates an **execution plan**.  \n",
    "   - Compiles the plan into optimized steps.  \n",
    "3. The **Leader Node distributes code** to slices on compute nodes.  \n",
    "4. Each slice **processes its portion of data** in parallel.  \n",
    "5. Compute nodes return intermediate results to the leader node.  \n",
    "6. The leader node **aggregates the results** and sends the final answer back to the client.\n",
    "\n",
    "ğŸ‘‰ This is what gives Redshift **Massively Parallel Processing (MPP)** power â€” multiple slices work simultaneously.\n",
    "\n",
    "---\n",
    "\n",
    "## âš¡ 3. Scaling Up â€” Adding More Compute Power\n",
    "\n",
    "![Scaling Redshift](./images/more_compute.png)\n",
    "\n",
    "- As data volume or workload increases, you can **scale the cluster** by:\n",
    "  - **Adding more compute nodes** horizontally.\n",
    "  - **Upgrading the node type** vertically (e.g., to more powerful instances).  \n",
    "- This improves:\n",
    "  - **Parallel processing capacity** ğŸ§®  \n",
    "  - **Query performance** ğŸš€  \n",
    "  - **Throughput for concurrent users** ğŸ‘¥\n",
    "\n",
    "âœ¨ One of the biggest advantages of **cloud data warehouses** is that scaling is **elastic** â€” you donâ€™t need to provision huge upfront infrastructure like in traditional on-premises systems.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§­ 5. Data Warehouseâ€“Centric ELT Architecture\n",
    "\n",
    "![Architecture](./images/archi.png)\n",
    "\n",
    "1. **Extract** â†’ Ingest raw data from multiple sources.  \n",
    "2. **Load** â†’ Push raw data directly into the cloud data warehouse.  \n",
    "3. **Transform** â†’ Clean & model data inside the warehouse using its compute power.\n",
    "\n",
    "ğŸ‘‰ Modern cloud DWs (Redshift, BigQuery, Snowflake) make **ELT** faster than traditional ETL because they can handle massive transformations internally.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒ 6. Traditional vs Cloud Data Warehouse\n",
    "\n",
    "![Cloud Data Warehouse Comparison](./images/cloud_Data_warhosu.png)\n",
    "\n",
    "| Feature ğŸ§° | Traditional DW ğŸ§± | Cloud DW â˜ï¸ |\n",
    "|-----------|-------------------|-------------|\n",
    "| **Structure** | Highly structured | Highly structured |\n",
    "| **Modeling** | Data modeled for analytics | Data modeled for analytics |\n",
    "| **Processing Power** | Limited to fixed MPP capacity | **Elastic scaling**, on-demand |\n",
    "| **Storage Format** | Row or columnar | **Columnar + compression** |\n",
    "| **Scaling** | Vertical, expensive | Horizontal + vertical, elastic |\n",
    "| **Cost** | Upfront infra | Pay-as-you-go |\n",
    "| **Use Case** | Fixed infra, on-prem | Modern, scalable analytics |\n",
    "\n",
    "---\n",
    "\n",
    "## âœ¨ 7. Why Cloud Data Warehouses Are Powerful\n",
    "\n",
    "- **Elastic scaling** â†’ Add nodes or upgrade types when needed.  \n",
    "- **Columnar storage** â†’ Efficient analytical queries.  \n",
    "- **Separation of storage & compute** â†’ Cost-performance optimization.  \n",
    "- **MPP** â†’ Parallel query execution for petabyte-scale analytics.  \n",
    "- **Supports ELT** â†’ Load raw data, transform inside warehouse.\n",
    "\n",
    "ğŸ‘‰ Platforms like **Snowflake**, **BigQuery**, and **Redshift** enable organizations to run high-volume analytics with flexibility and speed.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ Summary\n",
    "\n",
    "| Concept | Key Idea |\n",
    "|--------|----------|\n",
    "| **MPP** | Distribute data & processing across nodes for speed. |\n",
    "| **Leader Node** | Coordinates, plans, and aggregates queries. |\n",
    "| **Slices** | Sub-units of compute nodes for parallel processing. |\n",
    "| **Elastic Scaling** | Add/upgrade nodes easily to handle bigger workloads. |\n",
    "| **ELT** | Load first, transform inside DW. |\n",
    "| **Cloud DW** | Scalable, cost-efficient, high-performance analytics engine. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9297760e",
   "metadata": {},
   "source": [
    "# Data Lake â€” Overview & Shortcomings\n",
    "\n",
    "## ğŸ§  1. What is a Data Lake?\n",
    "\n",
    "![Data Lake](./images/datalake.png)\n",
    "\n",
    "A **Data Lake** is a **central repository** that allows you to store **structured, semi-structured, and unstructured data** at any scale.\n",
    "\n",
    "### âœ¨ Key Features\n",
    "\n",
    "- ğŸª£ **Centralized Storage**  \n",
    "  Store huge volumes of diverse data types (text, images, videos, JSON, etc.) in one place.\n",
    "\n",
    "- ğŸ“ **No Fixed Schema**  \n",
    "  You donâ€™t need to define schema or transformations before storing data.\n",
    "\n",
    "- ğŸ“š **Schema-on-Read Pattern**  \n",
    "  - Schema is applied **when the data is read**, not when it's written.  \n",
    "  - This gives flexibility to handle evolving data formats.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ— 2. Data Lake 1.0 â€” The First Generation\n",
    "\n",
    "Data Lake 1.0 (2000sâ€“early 2010s) combined **storage** + **processing** technologies to create cheap, large-scale data repositories.\n",
    "\n",
    "| Component         | Technologies Used |\n",
    "|--------------------|----------------------|\n",
    "| ğŸ—„ **Storage**     | HDFS â†’ later Amazon S3 for virtually unlimited, cheap storage |\n",
    "| âš™ **Processing** | Hadoop MapReduce, Apache Pig, Spark, Presto, Hive, etc. |\n",
    "\n",
    "ğŸ‘‰ The goal was to **store everything first**, then decide later how to process it.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§° 3. Why Data Lakes Became Popular\n",
    "\n",
    "Imagine youâ€™re a **data engineer** at an e-commerce company:\n",
    "- You have structured data from ğŸ§¾ **sales orders**.\n",
    "- Semi-structured data from ğŸ’¬ **CRM systems**.\n",
    "- Unstructured data like ğŸ“ **text reviews**, ğŸ“¸ **images**, ğŸ§ **audio**, ğŸ“¹ **video**.\n",
    "\n",
    "A data warehouse forces you to **predefine schema** â†’ not flexible for fast, diverse data ingestion.\n",
    "\n",
    "âœ… **Data Lakes solved this** by letting you dump everything into one central storage (e.g., S3) without worrying about structure first.\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ 4. Shortcomings of Data Lake 1.0\n",
    "\n",
    "![Shortcomings of Data Lake 1.0](./images/shortcomings.png)\n",
    "\n",
    "Despite their promise, early data lakes often became **Data Swamps** ğŸŠ\n",
    "\n",
    "### ğŸ—‘ **Data Swamp Problems**\n",
    "- âŒ No proper data management  \n",
    "- âŒ No data cataloging or discovery tools  \n",
    "- âŒ No quality guarantees â€” outdated or inaccurate data\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ **Write-Only Storage**\n",
    "- Basic DML operations (e.g., DELETE, UPDATE) were hard to implement.  \n",
    "- Often required **creating new tables** for every change.  \n",
    "- âš–ï¸ Difficult to comply with data regulations (e.g., GDPR's â€œRight to be Forgottenâ€).\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“‰ **No Schema Management / Modeling**\n",
    "- Processing stored data was hard.  \n",
    "- Data wasnâ€™t optimized for queries (e.g., joins were painful in MapReduce).  \n",
    "- Required specialized engineering skills to make sense of the data.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§­ 5. Summary Table â€” Data Lake 1.0\n",
    "\n",
    "| Feature | Data Lake 1.0 |\n",
    "|--------|---------------|\n",
    "| **Schema** | Schema-on-read |\n",
    "| **Data Types** | Structured, semi-structured, unstructured |\n",
    "| **Storage** | Cheap, scalable (e.g., S3, HDFS) |\n",
    "| **Processing** | Hadoop, Spark, Pig, Presto, Hive |\n",
    "| **Pros** | Flexible ingestion, cheap storage, central repository |\n",
    "| **Cons** | No governance, poor data quality, hard DML, difficult querying |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec319a2a",
   "metadata": {},
   "source": [
    "# Organizing and Managing Data in Data Lakes\n",
    "\n",
    "To overcome the shortcomings of **Data Lake 1.0** (like lack of structure, discoverability, and performance issues), modern data lake designs use the following techniques:\n",
    "\n",
    "- ğŸ“Œ **Data Zones**  \n",
    "- ğŸ§© **Data Partitioning**  \n",
    "- ğŸ“š **Data Catalog**  \n",
    "- ğŸ” **Integration with Data Warehouses** for analytics\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒ 1. Data Zones\n",
    "\n",
    "Data Zones are used to organize data in a data lake based on its **processing stage**.  \n",
    "Each zone contains data that has been processed to a different degree:\n",
    "\n",
    "- **Landing / Raw Zone** â€” raw ingested data (CSV, JSON, images, audio, etc.)  \n",
    "- **Cleaned / Transformed Zone** â€” after validation, standardization, and PII removal  \n",
    "- **Curated / Enriched Zone** â€” modeled, business-ready data for analytics & ML\n",
    "\n",
    "This structure helps maintain **data governance**, apply **access controls**, and ensure **data quality** at each stage.\n",
    "\n",
    "![Data Zones](./images/data_zones.png)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© 2. Data Partitioning\n",
    "\n",
    "Partitioning is the process of splitting a large dataset into **smaller, more manageable parts** based on criteria like date, region, or category.  \n",
    "This improves query performance because the engine scans **only relevant partitions**.\n",
    "\n",
    "For example, partitioning sales data by year and month allows queries for â€œJuly 2023â€ to read just that partition instead of the entire dataset.\n",
    "\n",
    "![Data Partitioning](./images/data_partitioning.png)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š 3. Data Catalog\n",
    "\n",
    "A **data catalog** is a centralized metadata repository that describes:\n",
    "- Dataset owner and source  \n",
    "- Schema and partitions  \n",
    "- Business definitions for columns  \n",
    "- Change history over time\n",
    "\n",
    "The catalog enables:\n",
    "- ğŸ” Easy data discovery  \n",
    "- ğŸ§  Consistent understanding of datasets  \n",
    "- ğŸ“ˆ Schema evolution tracking\n",
    "\n",
    "This makes it far easier for analysts, engineers, and data scientists to find and use the right data.\n",
    "\n",
    "![Data Catalog](./images/data_catalog.png)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¢ 4. Data Lakes + Data Warehouses\n",
    "\n",
    "Historically, many organizations used **both**:\n",
    "- **Data Lake** for cheap, large-scale storage (e.g., ML training data, logs, raw files)\n",
    "- **Data Warehouse** for fast analytical queries (e.g., dashboards, reports)\n",
    "\n",
    "They ingest all data into the lake, then **ETL a subset** into the warehouse for performance.\n",
    "\n",
    "However, this approach can be **costly and error-prone**, since:\n",
    "- ETL pipelines can fail or introduce duplication\n",
    "- Thereâ€™s ongoing storage + compute overhead\n",
    "- Synchronization between lake and warehouse is tricky\n",
    "\n",
    "![Separate DL and DW](./images/separte_dl_dw.png)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Summary Table\n",
    "\n",
    "| Feature               | Data Zones ğŸ§­                   | Partitioning ğŸ§©                     | Data Catalog ğŸ“š                            | Data Lake + DW ğŸ¢                                  |\n",
    "|------------------------|----------------------------------|--------------------------------------|---------------------------------------------|---------------------------------------------------|\n",
    "| ğŸ“Œ **Purpose**        | Organize data by processing stage | Speed up queries on large datasets   | Improve discoverability and metadata mgmt   | Combine cheap storage with fast analytics         |\n",
    "| ğŸ§  **Key Benefit**   | Better governance & quality      | Query efficiency & scalability      | Easier search, schema consistency          | Leverage strengths of both lake and warehouse     |\n",
    "| ğŸ§° **Typical Use**   | Raw â†’ Cleaned â†’ Curated pipeline | Time/date/location-based partitioning | Company-wide data catalog or data hub     | ETL from lake â†’ warehouse for dashboards          |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Why This Matters\n",
    "\n",
    "These techniques **transform a chaotic data swamp** into a **well-governed, performant data platform**.  \n",
    "Theyâ€™re essential building blocks of **modern data architectures** like the **Data Lakehouse**, which blends the best of both lakes and warehouses into a single system."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
