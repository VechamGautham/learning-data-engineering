{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53b1364d",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [Data Warehouse - Overview & Architecture](#data-warehouse--overview--architecture)\n",
    "- [Database vs Data Lkae vs Data Warehouse](#database-vs-data-lake-vs-data-warehouse--comparison-table)\n",
    "- [Cloud Data Warehouse and MPP Architecture](#cloud-data-warehouse--mpp-architecture)\n",
    "- [Data Lake - Overview & Shortcomings](#data-lake--overview--shortcomings)\n",
    "- [Organizing and Managing Data in Data Lake](#organizing-and-managing-data-in-data-lakes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09adf05c",
   "metadata": {},
   "source": [
    "# Data Warehouse — Overview & Architecture\n",
    "\n",
    "## 📌 Key Characteristics of a Data Warehouse\n",
    "\n",
    "![Data Warehouse](./images/data_warehouse.png)\n",
    "\n",
    "- **Subject-Oriented** 🧠  \n",
    "  Organizes and stores data around key business domains such as Customers, Products, Sales, Finance.  \n",
    "  → Data is modeled to support **decision-making**, not transactions.\n",
    "\n",
    "- **Integrated** 🔗  \n",
    "  Combines data from multiple, heterogeneous sources into a **consistent schema**.\n",
    "\n",
    "- **Non-Volatile** 📄  \n",
    "  Data is **read-only** — it cannot be updated or deleted.  \n",
    "  → Preserves snapshots for historical analysis.\n",
    "\n",
    "- **Time-Variant** 🕰  \n",
    "  Stores both **current and historical** data, unlike OLTP systems.  \n",
    "  → Enables trend analysis over time.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧱 Data Warehouse–Centric Architecture (ETL Pattern)\n",
    "\n",
    "![Data Warehouse ETL Architecture](./images/data_warehouse-centric_architecture.png)\n",
    "\n",
    "1. **Extract**  \n",
    "   Pull data from various operational sources — databases, APIs, files, etc.\n",
    "\n",
    "2. **Transform**  \n",
    "   Clean, standardize, and model the data in a **staging area**.\n",
    "\n",
    "3. **Load**  \n",
    "   Push transformed data into the **Data Warehouse** with a comprehensive schema.\n",
    "\n",
    "4. **Data Marts** 🏪  \n",
    "   - Department-specific subsets (e.g. Sales, Marketing, Finance).  \n",
    "   - Often follow **simplified or denormalized schemas**.  \n",
    "   - Improve query performance for specific use cases.\n",
    "\n",
    "5. **Analytics & Reports** 📊  \n",
    "   BI tools and analysts use the Data Marts & Warehouse for dashboards and decision-making.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 Change Data Capture (CDC)\n",
    "\n",
    "![Change Data Capture](./images/data_warehouse-centric_archi_cdc.png)\n",
    "\n",
    "- Instead of extracting the **entire dataset** every time,  \n",
    "  **CDC tracks only changes** (inserts, updates, deletes) in the source systems.\n",
    "\n",
    "- Reduces load on production OLTP databases ✅  \n",
    "- Keeps the Data Warehouse **incrementally in sync** with source systems.  \n",
    "- Commonly implemented using ETL pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚡ Evolution of Data Warehouse Implementations\n",
    "\n",
    "![Data Warehouse Implementation](./images/data_warehouse_implementations.png)\n",
    "\n",
    "| Era | Architecture | Key Features |\n",
    "|-----|-------------|--------------|\n",
    "| **Early DW** 🧱 | Monolithic servers | Limited performance, single big machine |\n",
    "| **MPP DW** ⚙️ | Massively Parallel Processing | Distributes queries across multiple nodes, scans data in parallel |\n",
    "| **Modern Cloud DW** ☁️ | Snowflake, BigQuery, Redshift | Separates **compute & storage**, scales elastically, cost-efficient |\n",
    "\n",
    "### ✨ Modern Cloud Data Warehouses\n",
    "- **Amazon Redshift**  \n",
    "- **Google BigQuery**  \n",
    "- **Snowflake**\n",
    "\n",
    "✅ **Key Advantage**:  \n",
    "- Compute and storage are **independent**, allowing cost-effective scaling.  \n",
    "- Ideal for analytical workloads on very large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## 📝 Recap\n",
    "\n",
    "- OLTP systems are designed for **transactions**, not heavy analytics.  \n",
    "- Data Warehouses were introduced to:\n",
    "  - Consolidate data from multiple sources\n",
    "  - Provide **historical context**\n",
    "  - Enable **analytical queries** efficiently\n",
    "- **ETL pipelines** → clean & load data into a **centralized warehouse**  \n",
    "- **Data Marts** → business function–specific views for easier analysis  \n",
    "- **CDC** → keeps data up to date without full reloads  \n",
    "- **Modern DW** leverage **cloud + MPP** for scale & performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bc90e8",
   "metadata": {},
   "source": [
    "# Database vs Data Lake vs Data Warehouse — Comparison Table\n",
    "\n",
    "| Feature 🧠                         | Database (OLTP) 🧾                          | Data Lake 🌊                                      | Data Warehouse 🏢                                                   |\n",
    "|-------------------------------------|---------------------------------------------|--------------------------------------------------|---------------------------------------------------------------------|\n",
    "| **Primary Purpose**                | Handle **day-to-day transactions**         | Store **large volumes of raw / semi-structured data** cheaply | **Analytics, reporting & decision-making**                         |\n",
    "| **Schema**                          | Strict, normalized                         | Flexible or **schema-on-read**                   | Structured, **modeled** (e.g. Star / Snowflake schema)              |\n",
    "| **Data Type**                       | Mostly structured                          | Structured, semi-structured, unstructured        | Structured, cleaned & integrated                                   |\n",
    "| **Data Processing**                | OLTP (row-based inserts/updates)          | Batch or streaming (big data frameworks)        | OLAP (columnar scans, aggregations, joins)                         |\n",
    "| **Storage Layer**                  | Disk + RAM (local)                         | Disk / Object storage (e.g., S3, ADLS, HDFS)     | Disk-based (often columnar format on cloud object storage)         |\n",
    "| **RAM Usage**                      | High — for fast concurrent writes & lookups | Low for storage; RAM needed only during Spark/compute jobs | Moderate; used mainly during **query execution**, not for storage |\n",
    "| **Query Patterns**                 | Fast reads/writes on individual records   | Heavy ETL/ELT, ML workloads, large scans        | Analytical queries, aggregations, dashboards                       |\n",
    "| **Performance Optimization**       | Indexes, B-trees, in-memory caching       | Parallel processing, schema-on-read            | Columnar storage, MPP (Massively Parallel Processing), caching     |\n",
    "| **Cost**                            | Expensive to scale for big analytics       | Cheap storage, variable compute cost            | Cost-effective analytics, separates **compute & storage** in modern cloud DWs |\n",
    "| **Data Freshness**                 | Real-time (transactions)                  | Near real-time or batch                        | Batch or near real-time (depending on ETL/CDC setup)               |\n",
    "| **Examples**                        | MySQL, PostgreSQL, Oracle, SQL Server     | S3, ADLS, HDFS, Delta Lake                      | Snowflake, BigQuery, Redshift, Synapse                             |\n",
    "| **Best For**                        | Transactional apps, operational systems   | Data exploration, staging, ML feature stores   | Business intelligence, dashboards, historical analysis             |\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ Key Insights\n",
    "\n",
    "- 🧾 **Databases** are great for **fast, reliable transactions** but are **not optimized for analytical queries** over large datasets.  \n",
    "- 🌊 **Data Lakes** are great for **raw, large-scale data storage** at low cost, but **query performance depends on the compute engine** (e.g., Spark).  \n",
    "- 🏢 **Data Warehouses** sit in between: they combine structured modeling, columnar storage, and parallel processing to support **high-performance analytics**.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Final One-liner\n",
    "\n",
    "> “A **database** is built for transactions, a **data lake** is built for storage, and a **data warehouse** is built for analytics.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171b4176",
   "metadata": {},
   "source": [
    "# Cloud Data Warehouse & MPP Architecture \n",
    "\n",
    "## 🧠 1. MPP Architecture for Amazon Redshift\n",
    "\n",
    "![MPP Architecture](./images/mpp_archi.png)\n",
    "\n",
    "- **Redshift Cluster** = Collection of computing resources.\n",
    "- Consists of:\n",
    "  - 🧠 **Leader Node** → Coordinates the cluster, plans queries.\n",
    "  - 💻 **Compute Nodes** → Store data & execute query steps in parallel.\n",
    "- Each compute node is divided into **Slices** 🧩:\n",
    "  - Each slice gets a share of **CPU**, **memory**, and **disk**.\n",
    "  - Data is distributed across slices for parallel processing.\n",
    "\n",
    "---\n",
    "\n",
    "## 📝 2. How MPP Query Execution Works\n",
    "\n",
    "![MPP Execution Steps](./images/mpp_archi_steps.png)\n",
    "\n",
    "1. **Client Application** connects to Redshift using JDBC/ODBC.  \n",
    "2. The **Leader Node**:\n",
    "   - Parses the SQL query.  \n",
    "   - Generates an **execution plan**.  \n",
    "   - Compiles the plan into optimized steps.  \n",
    "3. The **Leader Node distributes code** to slices on compute nodes.  \n",
    "4. Each slice **processes its portion of data** in parallel.  \n",
    "5. Compute nodes return intermediate results to the leader node.  \n",
    "6. The leader node **aggregates the results** and sends the final answer back to the client.\n",
    "\n",
    "👉 This is what gives Redshift **Massively Parallel Processing (MPP)** power — multiple slices work simultaneously.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚡ 3. Scaling Up — Adding More Compute Power\n",
    "\n",
    "![Scaling Redshift](./images/more_compute.png)\n",
    "\n",
    "- As data volume or workload increases, you can **scale the cluster** by:\n",
    "  - **Adding more compute nodes** horizontally.\n",
    "  - **Upgrading the node type** vertically (e.g., to more powerful instances).  \n",
    "- This improves:\n",
    "  - **Parallel processing capacity** 🧮  \n",
    "  - **Query performance** 🚀  \n",
    "  - **Throughput for concurrent users** 👥\n",
    "\n",
    "✨ One of the biggest advantages of **cloud data warehouses** is that scaling is **elastic** — you don’t need to provision huge upfront infrastructure like in traditional on-premises systems.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 🧭 5. Data Warehouse–Centric ELT Architecture\n",
    "\n",
    "![Architecture](./images/archi.png)\n",
    "\n",
    "1. **Extract** → Ingest raw data from multiple sources.  \n",
    "2. **Load** → Push raw data directly into the cloud data warehouse.  \n",
    "3. **Transform** → Clean & model data inside the warehouse using its compute power.\n",
    "\n",
    "👉 Modern cloud DWs (Redshift, BigQuery, Snowflake) make **ELT** faster than traditional ETL because they can handle massive transformations internally.\n",
    "\n",
    "---\n",
    "\n",
    "## 🌐 6. Traditional vs Cloud Data Warehouse\n",
    "\n",
    "![Cloud Data Warehouse Comparison](./images/cloud_Data_warhosu.png)\n",
    "\n",
    "| Feature 🧰 | Traditional DW 🧱 | Cloud DW ☁️ |\n",
    "|-----------|-------------------|-------------|\n",
    "| **Structure** | Highly structured | Highly structured |\n",
    "| **Modeling** | Data modeled for analytics | Data modeled for analytics |\n",
    "| **Processing Power** | Limited to fixed MPP capacity | **Elastic scaling**, on-demand |\n",
    "| **Storage Format** | Row or columnar | **Columnar + compression** |\n",
    "| **Scaling** | Vertical, expensive | Horizontal + vertical, elastic |\n",
    "| **Cost** | Upfront infra | Pay-as-you-go |\n",
    "| **Use Case** | Fixed infra, on-prem | Modern, scalable analytics |\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ 7. Why Cloud Data Warehouses Are Powerful\n",
    "\n",
    "- **Elastic scaling** → Add nodes or upgrade types when needed.  \n",
    "- **Columnar storage** → Efficient analytical queries.  \n",
    "- **Separation of storage & compute** → Cost-performance optimization.  \n",
    "- **MPP** → Parallel query execution for petabyte-scale analytics.  \n",
    "- **Supports ELT** → Load raw data, transform inside warehouse.\n",
    "\n",
    "👉 Platforms like **Snowflake**, **BigQuery**, and **Redshift** enable organizations to run high-volume analytics with flexibility and speed.\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Summary\n",
    "\n",
    "| Concept | Key Idea |\n",
    "|--------|----------|\n",
    "| **MPP** | Distribute data & processing across nodes for speed. |\n",
    "| **Leader Node** | Coordinates, plans, and aggregates queries. |\n",
    "| **Slices** | Sub-units of compute nodes for parallel processing. |\n",
    "| **Elastic Scaling** | Add/upgrade nodes easily to handle bigger workloads. |\n",
    "| **ELT** | Load first, transform inside DW. |\n",
    "| **Cloud DW** | Scalable, cost-efficient, high-performance analytics engine. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9297760e",
   "metadata": {},
   "source": [
    "# Data Lake — Overview & Shortcomings\n",
    "\n",
    "## 🧠 1. What is a Data Lake?\n",
    "\n",
    "![Data Lake](./images/datalake.png)\n",
    "\n",
    "A **Data Lake** is a **central repository** that allows you to store **structured, semi-structured, and unstructured data** at any scale.\n",
    "\n",
    "### ✨ Key Features\n",
    "\n",
    "- 🪣 **Centralized Storage**  \n",
    "  Store huge volumes of diverse data types (text, images, videos, JSON, etc.) in one place.\n",
    "\n",
    "- 📝 **No Fixed Schema**  \n",
    "  You don’t need to define schema or transformations before storing data.\n",
    "\n",
    "- 📚 **Schema-on-Read Pattern**  \n",
    "  - Schema is applied **when the data is read**, not when it's written.  \n",
    "  - This gives flexibility to handle evolving data formats.\n",
    "\n",
    "---\n",
    "\n",
    "## 🏗 2. Data Lake 1.0 — The First Generation\n",
    "\n",
    "Data Lake 1.0 (2000s–early 2010s) combined **storage** + **processing** technologies to create cheap, large-scale data repositories.\n",
    "\n",
    "| Component         | Technologies Used |\n",
    "|--------------------|----------------------|\n",
    "| 🗄 **Storage**     | HDFS → later Amazon S3 for virtually unlimited, cheap storage |\n",
    "| ⚙ **Processing** | Hadoop MapReduce, Apache Pig, Spark, Presto, Hive, etc. |\n",
    "\n",
    "👉 The goal was to **store everything first**, then decide later how to process it.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧰 3. Why Data Lakes Became Popular\n",
    "\n",
    "Imagine you’re a **data engineer** at an e-commerce company:\n",
    "- You have structured data from 🧾 **sales orders**.\n",
    "- Semi-structured data from 💬 **CRM systems**.\n",
    "- Unstructured data like 📝 **text reviews**, 📸 **images**, 🎧 **audio**, 📹 **video**.\n",
    "\n",
    "A data warehouse forces you to **predefine schema** → not flexible for fast, diverse data ingestion.\n",
    "\n",
    "✅ **Data Lakes solved this** by letting you dump everything into one central storage (e.g., S3) without worrying about structure first.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ 4. Shortcomings of Data Lake 1.0\n",
    "\n",
    "![Shortcomings of Data Lake 1.0](./images/shortcomings.png)\n",
    "\n",
    "Despite their promise, early data lakes often became **Data Swamps** 🐊\n",
    "\n",
    "### 🗑 **Data Swamp Problems**\n",
    "- ❌ No proper data management  \n",
    "- ❌ No data cataloging or discovery tools  \n",
    "- ❌ No quality guarantees — outdated or inaccurate data\n",
    "\n",
    "---\n",
    "\n",
    "### 📝 **Write-Only Storage**\n",
    "- Basic DML operations (e.g., DELETE, UPDATE) were hard to implement.  \n",
    "- Often required **creating new tables** for every change.  \n",
    "- ⚖️ Difficult to comply with data regulations (e.g., GDPR's “Right to be Forgotten”).\n",
    "\n",
    "---\n",
    "\n",
    "### 📉 **No Schema Management / Modeling**\n",
    "- Processing stored data was hard.  \n",
    "- Data wasn’t optimized for queries (e.g., joins were painful in MapReduce).  \n",
    "- Required specialized engineering skills to make sense of the data.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧭 5. Summary Table — Data Lake 1.0\n",
    "\n",
    "| Feature | Data Lake 1.0 |\n",
    "|--------|---------------|\n",
    "| **Schema** | Schema-on-read |\n",
    "| **Data Types** | Structured, semi-structured, unstructured |\n",
    "| **Storage** | Cheap, scalable (e.g., S3, HDFS) |\n",
    "| **Processing** | Hadoop, Spark, Pig, Presto, Hive |\n",
    "| **Pros** | Flexible ingestion, cheap storage, central repository |\n",
    "| **Cons** | No governance, poor data quality, hard DML, difficult querying |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec319a2a",
   "metadata": {},
   "source": [
    "# Organizing and Managing Data in Data Lakes\n",
    "\n",
    "To overcome the shortcomings of **Data Lake 1.0** (like lack of structure, discoverability, and performance issues), modern data lake designs use the following techniques:\n",
    "\n",
    "- 📌 **Data Zones**  \n",
    "- 🧩 **Data Partitioning**  \n",
    "- 📚 **Data Catalog**  \n",
    "- 🔁 **Integration with Data Warehouses** for analytics\n",
    "\n",
    "---\n",
    "\n",
    "## 🌐 1. Data Zones\n",
    "\n",
    "Data Zones are used to organize data in a data lake based on its **processing stage**.  \n",
    "Each zone contains data that has been processed to a different degree:\n",
    "\n",
    "- **Landing / Raw Zone** — raw ingested data (CSV, JSON, images, audio, etc.)  \n",
    "- **Cleaned / Transformed Zone** — after validation, standardization, and PII removal  \n",
    "- **Curated / Enriched Zone** — modeled, business-ready data for analytics & ML\n",
    "\n",
    "This structure helps maintain **data governance**, apply **access controls**, and ensure **data quality** at each stage.\n",
    "\n",
    "![Data Zones](./images/data_zones.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 2. Data Partitioning\n",
    "\n",
    "Partitioning is the process of splitting a large dataset into **smaller, more manageable parts** based on criteria like date, region, or category.  \n",
    "This improves query performance because the engine scans **only relevant partitions**.\n",
    "\n",
    "For example, partitioning sales data by year and month allows queries for “July 2023” to read just that partition instead of the entire dataset.\n",
    "\n",
    "![Data Partitioning](./images/data_partitioning.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 3. Data Catalog\n",
    "\n",
    "A **data catalog** is a centralized metadata repository that describes:\n",
    "- Dataset owner and source  \n",
    "- Schema and partitions  \n",
    "- Business definitions for columns  \n",
    "- Change history over time\n",
    "\n",
    "The catalog enables:\n",
    "- 🔍 Easy data discovery  \n",
    "- 🧠 Consistent understanding of datasets  \n",
    "- 📈 Schema evolution tracking\n",
    "\n",
    "This makes it far easier for analysts, engineers, and data scientists to find and use the right data.\n",
    "\n",
    "![Data Catalog](./images/data_catalog.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 🏢 4. Data Lakes + Data Warehouses\n",
    "\n",
    "Historically, many organizations used **both**:\n",
    "- **Data Lake** for cheap, large-scale storage (e.g., ML training data, logs, raw files)\n",
    "- **Data Warehouse** for fast analytical queries (e.g., dashboards, reports)\n",
    "\n",
    "They ingest all data into the lake, then **ETL a subset** into the warehouse for performance.\n",
    "\n",
    "However, this approach can be **costly and error-prone**, since:\n",
    "- ETL pipelines can fail or introduce duplication\n",
    "- There’s ongoing storage + compute overhead\n",
    "- Synchronization between lake and warehouse is tricky\n",
    "\n",
    "![Separate DL and DW](./images/separte_dl_dw.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 📝 Summary Table\n",
    "\n",
    "| Feature               | Data Zones 🧭                   | Partitioning 🧩                     | Data Catalog 📚                            | Data Lake + DW 🏢                                  |\n",
    "|------------------------|----------------------------------|--------------------------------------|---------------------------------------------|---------------------------------------------------|\n",
    "| 📌 **Purpose**        | Organize data by processing stage | Speed up queries on large datasets   | Improve discoverability and metadata mgmt   | Combine cheap storage with fast analytics         |\n",
    "| 🧠 **Key Benefit**   | Better governance & quality      | Query efficiency & scalability      | Easier search, schema consistency          | Leverage strengths of both lake and warehouse     |\n",
    "| 🧰 **Typical Use**   | Raw → Cleaned → Curated pipeline | Time/date/location-based partitioning | Company-wide data catalog or data hub     | ETL from lake → warehouse for dashboards          |\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Why This Matters\n",
    "\n",
    "These techniques **transform a chaotic data swamp** into a **well-governed, performant data platform**.  \n",
    "They’re essential building blocks of **modern data architectures** like the **Data Lakehouse**, which blends the best of both lakes and warehouses into a single system."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
